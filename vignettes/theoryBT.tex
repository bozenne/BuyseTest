% Created 2018-12-12 on 18:07
% Intended LaTeX compiler: pdflatex
\documentclass[12pt]{article}

%%%% settings when exporting code %%%% 

\usepackage{listings}
\lstset{
backgroundcolor=\color{white},
basewidth={0.5em,0.4em},
basicstyle=\ttfamily\small,
breakatwhitespace=false,
breaklines=true,
columns=fullflexible,
commentstyle=\color[rgb]{0.5,0,0.5},
frame=single,
keepspaces=true,
keywordstyle=\color{black},
literate={~}{$\sim$}{1},
numbers=left,
numbersep=10pt,
numberstyle=\ttfamily\tiny\color{gray},
showspaces=false,
showstringspaces=false,
stepnumber=1,
stringstyle=\color[rgb]{0,.5,0},
tabsize=4,
xleftmargin=.23in,
emph={anova,apply,class,coef,colnames,colNames,colSums,dim,dcast,for,ggplot,head,if,ifelse,is.na,lapply,list.files,library,logLik,melt,plot,require,rowSums,sapply,setcolorder,setkey,str,summary,tapply},
emphstyle=\color{blue}
}

%%%% packages %%%%%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{changes}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage[normalem]{ulem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{textcomp}
\usepackage{array}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{natbib}
%\VignetteIndexEntry{theory}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
\RequirePackage{fancyvrb}
\DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}
\geometry{a4paper, left=15mm, right=15mm}
\RequirePackage{colortbl} % arrayrulecolor to mix colors
\RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
\usepackage{authblk} % enable several affiliations (clash with beamer)
\renewcommand{\baselinestretch}{1.1}
\geometry{top=1cm}
\definecolor{darkgreen}{RGB}{0,125,0}
\definecolor{darkred}{RGB}{125,0,0}
\definecolor{darkblue}{RGB}{0,0,125}
\usepackage{enumitem}
\RequirePackage{xspace} %
\newcommand\Rlogo{\textbf{\textsf{R}}\xspace} %
\newcommand\Xobs{\tilde{X}}
\newcommand\Yobs{\tilde{Y}}
\newcommand\xobs{\tilde{x}}
\newcommand\yobs{\tilde{y}}
\newcommand\CensT{\varepsilon_X}
\newcommand\CensC{\varepsilon_Y}
\newcommand\censT{e_X}
\newcommand\censC{e_Y}
\RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
\RequirePackage{amsmath}
\RequirePackage{algorithm}
\RequirePackage[noend]{algpseudocode}
\RequirePackage{ifthen}
\RequirePackage{xspace} % space for newcommand macro
\RequirePackage{xifthen}
\RequirePackage{xargs}
\RequirePackage{dsfont}
\RequirePackage{amsmath,stmaryrd,graphicx}
\RequirePackage{prodint} % product integral symbol (\PRODI)
\RequirePackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\RequirePackage[makeroom]{cancel} % cancel terms
\newcommand\Ccancelto[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
\newcommand\defOperator[7]{%
\ifthenelse{\isempty{#2}}{
\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
}{
\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
}
}
\newcommand\defUOperator[5]{%
\ifthenelse{\isempty{#1}}{
#5\left#3 #2 \right#4
}{
\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
}
}
\newcommand{\defBoldVar}[2]{
\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
}
\newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
\newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
\newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}
\newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
\newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
\newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}
\newcommandx\SurvT[1][1=]{\defOperator{#1}{}{S_T}{}{(}{)}{}}
\newcommandx\SurvC[1][1=]{\defOperator{#1}{}{S_C}{}{(}{)}{}}
\newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
\newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
\newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}
\newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
\newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
\newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
\newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
\newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}
\newcommandx\Hypothesis[2][1=,2=]{
\ifthenelse{\isempty{#1}}{
\mathcal{H}
}{
\ifthenelse{\isempty{#2}}{
\mathcal{H}_{#1}
}{
\mathcal{H}^{(#2)}_{#1}
}
}
}
\newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
\ifthenelse{\isempty{#3}}{
\frac{#4 #1}{#4 #2}
}{
\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
}
}
\newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}
\newcommandx\ddpartial[3][1=,2=,3=]{
\ifthenelse{\isempty{#3}}{
\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
}{
\frac{\partial^2 #1}{\partial #2\partial #3}
}
}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Natural{\mathbb{N}}
\newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
\newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
\newcommand\half{\frac{1}{2}}
\newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
\newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
\author{Brice Ozenne}
\date{\today}
\title{Theory supporting the net benefit and Peron's scoring rules}
\hypersetup{
 colorlinks=true,
 citecolor=[rgb]{0,0.5,0},
 urlcolor=[rgb]{0,0,0.5},
 linkcolor=[rgb]{0,0,0.5},
 pdfauthor={Brice Ozenne},
 pdftitle={Theory supporting the net benefit and Peron's scoring rules},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.1 (Org mode 9.0.4)},
 pdflang={English}
 }
\begin{document}

\maketitle
This document describe the relationship between the net benefit and
traditional parameter of interest (e.g. hazard ratio). It also present
how Peron's scoring rules for the survival and competing setting were
derived.

\bigskip

In the examples we will use a sample size of:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- 1e4
\end{lstlisting}

and use the following R packages
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
library(BuyseTest)
library(riskRegression)
library(survival)
\end{lstlisting}

\tableofcontents

\clearpage

\section{References}
\label{sec:org547c61f}
\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{bibliography}

\endgroup

\clearpage

\appendix

\section{Recall on the U-statistic theory}
\label{sec:orgdeecf24}

This recall is based on chapter 1 of \cite{lee1990u}.

\subsection{Motivating example}
\label{sec:org73d8d3b}

We will illustrate basic results on U-statistics with the following
motivating question: "what is the asymptotic distribution of the
empirical variance estimator?". For a more concrete example, imagine
that we want to provide an estimate with its 95\% confidence interval
for the variability in cholesterol measurements. We assume that we are
able to collect a sample of \(n\) independent and identically
distributed (iid) realisations \((x_1,\ldots,x_n)\) of the random
variable cholesterol, denoted \(X\). We ignore any measurement error.

\subsection{Estimate, estimator, and functionnal}
\label{sec:orgfc15c92}

We can compute an \textbf{estimate} \(s\) of the variance using the following
\textbf{estimators} \(f_m\) and \(f_s\):
\begin{align}
m &= f_m(x_1,\ldots,x_n) = \frac{1}{n} \sum_{i=1}^n x_i \label{eq:m(F)} \\
s &= f_s(x_1,\ldots,x_n) = \frac{1}{n-1} \sum_{i=1}^n (x_i-m)^2 \label{eq:s(F)}
\end{align}
Given a dataset \(s\) is a deterministic (not random) quantity,
e.g. if we observe:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
x <- c(1,3,5,2,1,3)
\end{lstlisting}

then \(s\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
m <- mean(x)
s <- var(x)
s
\end{lstlisting}

\begin{verbatim}
[1] 2.3
\end{verbatim}

But in general the value of \(s\) depends on the dataset. \(s\) is
like a function \(f_n\) that takes as argument some data and output a
quantity of interest. This is often refer to as a \textbf{functionnal},
e.g. \(S=f_n(x_1,\ldots,x_n)\). Here we use the upper case \(S\) to
emphasise that it is a random quantity: for each new realisation
\((x_1,\ldots,x_n)\) of \(X\) corresponds a realisation for \(S\)
i.e. a possibly different value for the variance. If mechanism
generating the data has cumulative distribution function \(F\) then we
can also define the true value as \(\mu=f(F)=lim\). Because \(S\) and \(f\) are very close
quantities we will not distinguish them in the notation, i.e. write
\(S=S(F)\). This corresponds to formula (1) in \cite{lee1990u}. To be
more precise we can explicit what is \(S(F)\):
\begin{align}
M(F) &= \int_{-\infty}^{+\infty} x dF(x) \label{eq:M(F)}\\
S(F) &= \int_{-\infty}^{+\infty} (x - M(F))^2 dF(x) \label{eq:S(F)}
\end{align}

When we observe a sample, we use it to plug-in formula \eqref{eq:M(F)}
and \eqref{eq:S(F)} an approximation \(\hat{F}\) of \(F\). Usually our
best guess for \(F\) is \(\hat{F}(x)= \frac{1}{n}\sum_{i=1}^n
\Ind[x \leq x_i]\) where \(\Ind[.]\) is the indicator function taking value
1 if \(.\) is true and 0 otherwise. One can check that when plug-in
\(\hat{F}\) formula \eqref{eq:M(F)} and \eqref{eq:S(F)} becomes formula
\eqref{eq:m(F)} and \eqref{eq:s(F)}.

\bigskip

To summarize:
\begin{itemize}
\item an estimator is a random variable whose realisation depends on the
data. It realization is called estimate.
\item an estimate is a deterministic value that we obtain using the
observed data (e.g. variability is 2.3)
\item a functionnal (of an estimator) is the rule by which an estimator
transform the data into an estimate.
\end{itemize}

\subsection{Aim}
\label{sec:orgdf3c80a}

Using formula \eqref{eq:m(F)} and \eqref{eq:s(F)} we can easily estimate
the variance based on the observed realisations of \(X\) (i.e. the
data). However how can we get an confidence interval? What we want is
to quantify the incertainty associated with the estimator, i.e. how
the value output by the functionnal is sensitive to a change in the
dataset. To do so, since the estimator \(S\) is a random variable, we
can try to characterize its distribution. This is in general
difficult. It is much easier to look at the distribution of the
estimator \(S\) if we would have an infinite sample size. This is what
we will do, and rely on similations to see how things go in finite
sample size. As we will see, the asymptotic distribution of the
variance is a Gaussian distribution with a variance that we can estimate:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- length(x)
k <- mean((x-m)^4)
var_s <- k/n - ((n-3)*s^2)/(n*(n-1))
var_s
\end{lstlisting}

\begin{verbatim}
[1] 0.8425278
\end{verbatim}

So we obtain a 95\% confidence intervals for the variance doing:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
c(estimate = s, 
  lower = s + qnorm(0.025) * sqrt(var_s),
  upper = s + qnorm(0.975) * sqrt(var_s))
\end{lstlisting}

\begin{verbatim}
 estimate     lower     upper 
2.3000000 0.5009625 4.0990375
\end{verbatim}

We can see that it is not a very good confidence interval since it
symmetric - we know that the variance is positive so it should extend
more on the right side. But this only problematic in small sample
sizes. In large enough sample sizes the confidence interval will be
correct and we focus on this case.

\clearpage

In summary, we would like:
\begin{itemize}
\item to show that our estimator \(S\) is asymptotically normally distributed.
\item to have a formula for computing the asymptotic variance
\end{itemize}

\bigskip

Note: we can already guess that the estimator \(S\) (as most
estimators) will be asymptotically distributed because it can be
expressed as a average (see formula \eqref{eq:s(F)}). If we would know
the mean of \(X\), then the terms \(x_i-m\) are iid so the
asymptotically normality of \(S\) follows from the central limit
theorem. It does not give us a formula for the asymptotic variance
though. To get that we will use results from the theory of the U-statistics.

\subsection{Definition of a U-statistic and examples}
\label{sec:org0e0f822}

A U-statistic with kernel \(h\) of order \(k\) is an estimator of the
form:
\begin{align*}
U_n = \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_r) \in \beta} h \left(X_{\beta_1},\ldots,X_{\beta_k} \right)
\end{align*}
where \(beta\) is the set of all possible permutations between \(r\)
integers choosen from \(\{1,\ldots,n\}\). The simplest example of a
U-statistic is the estimator of mean for which \(k=1\) and \(h\) is the identity
function:
\begin{align*}
U_n = \frac{1}{{n \choose 1}} \sum_{(\beta_1) \in \{1,\ldots,n\}} X_{\beta_1} = \frac{1}{n} \sum_{i=1}^{n} X_{i}
\end{align*}
Our estimator of the variance is also a U-statistic, but this requires
a little bit more work to see that:
\begin{align*}
s &= \frac{1}{n-1} \sum_{i=1}^n (x_i-m)^2 = \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i m + m^2\right) \\
&=  \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \frac{1}{n} \sum_{j=1}^n x_j + m^2\right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( x_i^2 - 2 x_i  x_j + m^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( (x_i - x_j)^2 - x_j^2 + m^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \frac{1}{n-1} \sum_{j=1}^n \left(x_j^2 - m^2\right)
=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - s \\
s &=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \frac{(x_i - x_j)^2}{2} 
=  \frac{2}{n(n-1)} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2}
=  \frac{1}{{n \choose 2}} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2} 
\end{align*}
So the variance estimator is a U-statistic of order 2 with kernel \(k(x)=(x-m)^2\).

\subsection{Why using the U-statistic theory?}
\label{sec:org69e55df}

Because it provides very useful results to characterize the asymptotic
distribution of an estimator. For instance:

\bigskip

\textbf{Theorem 3} (\cite{lee1990u}) Let \(U_n\) be a U-statistic with a kernel \(h\) of degre \(r\). Then:
\begin{align*}
\Var[U_n] &= {n \choose k}^{-1} \sum_{c=1}^k {k \choose c} {n-k \choose k-c} \sigma^2_c \\
\text{where } \sigma^2_c &= \Cov[h(x_1,\ldots,x_r),h(x'_1,\ldots,x'_r)]
\end{align*}

\bigskip


\clearpage
\end{document}