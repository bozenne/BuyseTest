% Created 2019-01-07 ma 17:36
% Intended LaTeX compiler: pdflatex
\documentclass[12pt]{article}

%%%% settings when exporting code %%%% 

\usepackage{listings}
\lstset{
backgroundcolor=\color{white},
basewidth={0.5em,0.4em},
basicstyle=\ttfamily\small,
breakatwhitespace=false,
breaklines=true,
columns=fullflexible,
commentstyle=\color[rgb]{0.5,0,0.5},
frame=single,
keepspaces=true,
keywordstyle=\color{black},
literate={~}{$\sim$}{1},
numbers=left,
numbersep=10pt,
numberstyle=\ttfamily\tiny\color{gray},
showspaces=false,
showstringspaces=false,
stepnumber=1,
stringstyle=\color[rgb]{0,.5,0},
tabsize=4,
xleftmargin=.23in,
emph={anova,apply,class,coef,colnames,colNames,colSums,dim,dcast,for,ggplot,head,if,ifelse,is.na,lapply,list.files,library,logLik,melt,plot,require,rowSums,sapply,setcolorder,setkey,str,summary,tapply},
emphstyle=\color{blue}
}

%%%% packages %%%%%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{changes}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage[normalem]{ulem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{textcomp}
\usepackage{array}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{natbib}
%\VignetteIndexEntry{theory}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
\RequirePackage{fancyvrb}
\DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}
\geometry{a4paper, left=15mm, right=15mm}
\RequirePackage{colortbl} % arrayrulecolor to mix colors
\RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
\usepackage{authblk} % enable several affiliations (clash with beamer)
\renewcommand{\baselinestretch}{1.1}
\geometry{top=1cm}
\definecolor{darkgreen}{RGB}{0,125,0}
\definecolor{darkred}{RGB}{125,0,0}
\definecolor{darkblue}{RGB}{0,0,125}
\usepackage{enumitem}
\RequirePackage{xspace} %
\newcommand\Rlogo{\textbf{\textsf{R}}\xspace} %
\newcommand\Xobs{\tilde{X}}
\newcommand\Yobs{\tilde{Y}}
\newcommand\xobs{\tilde{x}}
\newcommand\yobs{\tilde{y}}
\newcommand\CensT{\varepsilon_X}
\newcommand\CensC{\varepsilon_Y}
\newcommand\censT{e_X}
\newcommand\censC{e_Y}
\RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
\RequirePackage{amsmath}
\RequirePackage{algorithm}
\RequirePackage[noend]{algpseudocode}
\RequirePackage{ifthen}
\RequirePackage{xspace} % space for newcommand macro
\RequirePackage{xifthen}
\RequirePackage{xargs}
\RequirePackage{dsfont}
\RequirePackage{amsmath,stmaryrd,graphicx}
\RequirePackage{prodint} % product integral symbol (\PRODI)
\RequirePackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\RequirePackage[makeroom]{cancel} % cancel terms
\newcommand\Ccancelto[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
\newcommand\defOperator[7]{%
\ifthenelse{\isempty{#2}}{
\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
}{
\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
}
}
\newcommand\defUOperator[5]{%
\ifthenelse{\isempty{#1}}{
#5\left#3 #2 \right#4
}{
\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
}
}
\newcommand{\defBoldVar}[2]{
\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
}
\newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
\newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
\newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}
\newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
\newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
\newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}
\newcommandx\SurvT[1][1=]{\defOperator{#1}{}{S_T}{}{(}{)}{}}
\newcommandx\SurvC[1][1=]{\defOperator{#1}{}{S_C}{}{(}{)}{}}
\newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
\newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
\newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}
\newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
\newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
\newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
\newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
\newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}
\newcommandx\Hypothesis[2][1=,2=]{
\ifthenelse{\isempty{#1}}{
\mathcal{H}
}{
\ifthenelse{\isempty{#2}}{
\mathcal{H}_{#1}
}{
\mathcal{H}^{(#2)}_{#1}
}
}
}
\newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
\ifthenelse{\isempty{#3}}{
\frac{#4 #1}{#4 #2}
}{
\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
}
}
\newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}
\newcommandx\ddpartial[3][1=,2=,3=]{
\ifthenelse{\isempty{#3}}{
\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
}{
\frac{\partial^2 #1}{\partial #2\partial #3}
}
}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Natural{\mathbb{N}}
\newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
\newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
\newcommand\half{\frac{1}{2}}
\newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
\newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
\author{Brice Ozenne}
\date{\today}
\title{Theory supporting the net benefit and Peron's scoring rules}
\hypersetup{
 colorlinks=true,
 citecolor=[rgb]{0,0.5,0},
 urlcolor=[rgb]{0,0,0.5},
 linkcolor=[rgb]{0,0,0.5},
 pdfauthor={Brice Ozenne},
 pdftitle={Theory supporting the net benefit and Peron's scoring rules},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.1 (Org mode 9.0.4)},
 pdflang={English}
 }
\begin{document}

\maketitle
This document describe the relationship between the net benefit and
traditional parameter of interest (e.g. hazard ratio). It also present
how Peron's scoring rules for the survival and competing setting were
derived.

\bigskip

In the examples we will use a sample size of:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- 1e4
\end{lstlisting}

and use the following R packages
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
library(BuyseTest)
library(riskRegression)
library(survival)
\end{lstlisting}

\tableofcontents

\clearpage

\section{Parameter of interest}
\label{sec:org442d19a}

Let consider two independent real valued (univariate) random variables
\(X\) and \(Y\). Informally \(X\) refer to the outcome in the
experimental group while \(Y\) refer to the outcome in the control
group. For a given threshold \(\tau \in \Real^{+*}\), the net benefit
can be expressed as:
\begin{align*}
\Delta_\tau = \Prob[X \geq Y+\tau] - \Prob[Y \geq Y+\tau]
\end{align*}
To relate the net benefit to known quantities we will also consider
the case of an infinitesimal threshold \(\tau\):
\begin{align*}
\Delta_+ = \Prob[X > Y] - \Prob[Y > X]
\end{align*}
In any case, \(X\) and \(Y\) play a symetric role in the sense that
given a formula for \(\Prob[X \geq Y+\tau]\) (or \(\Prob[X > Y]\)), we
can substitute \(X\) to \(Y\) and \(Y\) to \(X\) to obtain the formula
for \(\Prob[Y \geq X+\tau]\) (or \(\Prob[Y > X]\)).

\clearpage

\section{Asymptotic distribution}
\label{sec:orge90e676}

\subsection{Gehan scoring rule}
\label{sec:org2e81075}

In this section we consider two independent samples
\(x_1,x_2,\ldots,x_m\) and \(y_1,y_2,\ldots,y_n\) where the first one
contains iid realisations of a random variable \(X\) and the second
one contains iid realisations of a second variable \(Y\). For each
realisation we observe \(p\) endpoint. 

\bigskip


The estimator of the net benefit can be written as the difference
between two estimators:
\begin{align*}
\hat{\Delta}_\tau = \widehat{\Prob}[X \geq Y+\tau] - \widehat{\Prob}[Y \geq Y+\tau]
\end{align*}
We denote by \(\phi_k\) the scoring rule relative to \(\Prob[X \geq
Y+\tau]\) for the endpoint \(k\),
e.g. \(\phi_k(x_1,y_1)=\Ind[x_1>y_1]\) for a binary endpoint. The
scoring rule may depend of additional arguments, e.g. a threshold
\(\tau\) but this will be ignored for the moment. Finally, we denote
by \(k_{ij}\) the endpoint at which the pair \((i,j)\) is classified
as favorable or unfavorable. If this does not happen then
\(k_{ij}=p\). With this notations, the estimator \(\widehat{\Prob}[X
\geq Y+\tau]\) can be written as a U-statistic:
\begin{align*}
\widehat{\Prob}[X \geq Y+\tau] = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \phi_{k_{ij}}(x_1,y_1)
\end{align*}
This is a two sample U-statistic of order (1,1) with kernel
\(\phi_{k_{ij}}(x_1,y_1)\) (trivially symmetric in \(x\) and \(y\)
separately). From the U-statistic theory (e.g. see appendix
\ref{SM:Ustat}), it follows that \(\widehat{\Prob}[X \geq Y+\tau]\) is
unbiased, normally distributed, and its iid decomposition is the
H\(\'{a}\)jek projection:
\begin{align*}
H^{(1)}(\widehat{\Prob}[X \geq Y+\tau]) &= \frac{1}{m} \sum_{i=1}^m \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| x_i] - \widehat{\Prob}[X \geq Y+\tau] \right) + \frac{1}{n} \sum_{j=1}^n \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| y_j] - \widehat{\Prob}[X \geq Y+\tau]\right) \\
&= \sum_{l=1}^{m+n} H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])
\end{align*}
where \(H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])\) are the
individual terms of the iid decomposition. For instance in the binary
case, the term relative to the \(i-th\) observation of the
experimental group is:
\begin{align*}
H^{(1)}_i(\widehat{\Prob}[X \geq Y + \tau]) & = \frac{\Esp[\phi_{k_{ij}}(x_i,y) \bigg| x_i]-\widehat{\Prob}[X \geq Y+\tau]}{m} = \left\{ \begin{array}{cc} 
 \frac{1-p_y-\widehat{\Prob}[X \geq Y+\tau]}{m} \text{ if } x = 1 \\
\frac{-\widehat{\Prob}[X \geq Y+\tau]}{m} \text{ if } x = 0 \\
\end{array} \right. 
\end{align*}
where \(p_y\) is the proportion of 1 in the control group.


\clearpage

\subsection{Example}
\label{sec:orgb36cd30}

Let's consider a case with 2 observations per group:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d <- data.table(id = 1:4, group = c("C","C","T","T"), toxicity = c(1,0,1,0))
d
\end{lstlisting}

\begin{verbatim}
   id group toxicity
1:  1     C        1
2:  2     C        0
3:  3     T        1
4:  4     T        0
\end{verbatim}

We can form 4 pairs:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d2 <- data.table(pair = c("3-1","4-1","3-2","4-2"), 
				 type = c("1-1","0-1","1-0","0-0"),
				 favorable = c(0,0,1,0),
				 unfavorable = c(0,1,0,0))
d2
\end{lstlisting}

\begin{verbatim}
   pair type favorable unfavorable
1:  3-1  1-1         0           0
2:  4-1  0-1         0           1
3:  3-2  1-0         1           0
4:  4-2  0-0         0           0
\end{verbatim}

So \(U=\Prob[X>Y]\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
U <- 1/4
\end{lstlisting}

and the iid terms are:
\begin{align*}
H^{(1)}_1(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x>y_1]\big|y_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_2>y_1]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8 \\
H^{(1)}_2(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x>y_2]\big|y_2\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_2]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_3(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_1>y]\big|x_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_1>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_4(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_2>y]\big|x_2\right]-U \right)&= \frac{ \frac{\Ind[x_2>y_1]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8
\end{align*}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "asymptotic", trace = 0)
iid(e.BT)
\end{lstlisting}

\begin{verbatim}
Warning message:
In inferenceUstatistic(tablePairScore = outPoint$tablePairScore,  :
  Non positive definite covariance matrix
     favorable unfavorable
[1,]    -0.125       0.125
[2,]     0.125      -0.125
[3,]     0.125      -0.125
[4,]    -0.125       0.125
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tt <- e.BT@tablePairScore[[1]][,.(index.C,index.T,favorable,unfavorable)]
U <- as.double(e.BT@count.favorable/NROW(tt))
tt[,list(favorable,favorable-U),by = "index.T"]
\end{lstlisting}




\begin{verbatim}
   index.T favorable    V2
1:       3         0 -0.25
2:       3         1  0.75
3:       4         0 -0.25
4:       4         0 -0.25
\end{verbatim}

\clearpage

\section{References}
\label{sec:org6f9923b}
\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{bibliography}

\endgroup

\clearpage

\appendix

\section{Recall on the U-statistic theory}
\label{SM:Ustat}
This recall is based on chapter 1 of \cite{lee1990u}.

\subsection{Motivating example}
\label{sec:org14fb09c}

We will illustrate basic results on U-statistics with the following
motivating question: "what is the asymptotic distribution of the
empirical variance estimator?". For a more concrete example, imagine
that we want to provide an estimate with its 95\% confidence interval
of the variability in cholesterol measurements. We assume that we are
able to collect a sample of \(n\) independent and identically
distributed (iid) realisations \((x_1,\ldots,x_n)\) of the random
variable cholesterol, denoted \(X\). We ignore any measurement error.

\subsection{Estimate, estimator, and functionnal}
\label{sec:org8e66ae7}

We can compute an \textbf{estimate} of the variance using the following
\textbf{estimators} \(\hat{\mu}\) and \(\hat{\sigma}^2\):
\begin{align}
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^n x_i \label{eq:m(F)} \\
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 \label{eq:s(F)}
\end{align}
Given a dataset the estimator \(\hat{\sigma}^2\) outputs a
deterministic (i.e. not random) quantity, called the estimate of the
variance. For instance if we observe:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
x <- c(1,3,5,2,1,3)
\end{lstlisting}

then \(s\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
mu <- mean(x)
sigma2 <- var(x)
sigma2
\end{lstlisting}

\begin{verbatim}
[1] 2.3
\end{verbatim}

In general the value of the estimate depends on the dataset. The
estimator acts like a function \(f_n\) that takes as argument some
data and output a quantity of interest. This is often refer to as a
\textbf{functionnal}, e.g. \(\hat{\sigma}^2=f_n(x_1,\ldots,x_n)\). Here we
use the hat notation to emphasise that \(\hat{\sigma}^2\) is a random
quantity: for each new realisation \((x_1,\ldots,x_n)\) of \(X\)
corresponds a realisation for \(\hat{\sigma}^2\) i.e. a possibly
different value for the variance. If mechanism generating the data has
cumulative distribution function \(F\) then we can also define the
true value as \(\sigma^2=f_{\sigma^2}(F)\) (which is a deterministic
value) where:
\begin{align}
\mu(F) &= f_\mu(F) = \int_{-\infty}^{+\infty} x dF(x) \label{eq:M(F)}\\
\sigma^2(F) &= f_{\sigma^2}(F) = \int_{-\infty}^{+\infty} (x - f_\mu(F))^2 dF(x) \label{eq:S(F)}
\end{align}
This can be understood as the limit \(f(F)=\lim_{n \rightarrow \infty}
f_n(x_1,\ldots,x_n)\). Because \(\sigma^2\) and \(f_{\sigma^2}\) are
very close quantities we will not distinguish them in the notation,
i.e. write \(\sigma^2=\sigma^2(F)\). This corresponds to formula (1)
in \cite{lee1990u}. 

\bigskip

When we observe a sample, we use it to plug-in formula \eqref{eq:M(F)}
and \eqref{eq:S(F)} an approximation \(\hat{F}\) of \(F\). Usually our
best guess for \(F\) is \(\hat{F}(x)= \frac{1}{n}\sum_{i=1}^n
\Ind[x \leq x_i]\) where \(\Ind[.]\) is the indicator function taking value
1 if \(.\) is true and 0 otherwise. One can check that when plug-in
\(\hat{F}\) formula \eqref{eq:M(F)} and \eqref{eq:S(F)} becomes formula
\eqref{eq:m(F)} and \eqref{eq:s(F)}.

\bigskip

To summarize:
\begin{itemize}
\item an estimator is a random variable whose realisation depends on the
data. Its realization is called estimate.
\item an estimate is a deterministic value that we obtain using the
observed data (e.g. observed variability is 2.3)
\item a functionnal (of an estimator) is the rule by which an estimator
transforms the data into an estimate.
\end{itemize}

\subsection{Aim}
\label{sec:orgffb1561}

Using formula \eqref{eq:m(F)} and \eqref{eq:s(F)} we can easily estimate
the variance based on the observed realisations of \(X\) (i.e. the
data). However how can we get an confidence interval? What we want is
to quantify the incertainty associated with the estimator, i.e. how
the value output by the functionnal is sensitive to a change in the
dataset. To do so, since the estimator \(\hat{\sigma}^2\) is a random variable, we
can try to characterize its distribution. This is in general
difficult. It is much easier to look at the distribution of the
estimator \(\hat{\sigma}^2\) if we would have an infinite sample size. This is what
we will do, and rely on similations to see how things go in finite
sample size. As we will see, the asymptotic distribution of the
variance is a Gaussian distribution with a variance that we can estimate:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- length(x)
k <- mean((x-mu)^4)
var_sigma2 <- (k-sigma2^2)/n
var_sigma2
\end{lstlisting}

\begin{verbatim}
[1] 0.4898611
\end{verbatim}

So we obtain a 95\% confidence intervals for the variance doing:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
c(estimate = sigma2, 
  lower = sigma2 + qnorm(0.025) * sqrt(var_sigma2),
  upper = sigma2 + qnorm(0.975) * sqrt(var_sigma2))
\end{lstlisting}

\begin{verbatim}
 estimate     lower     upper 
2.3000000 0.9282197 3.6717803
\end{verbatim}

We can see that it is not a very good confidence interval since it
symmetric - we know that the variance is positive so it should extend
more on the right side. But this only problematic in small sample
sizes. In large enough sample sizes the confidence interval will be
correct and we focus on this case.

\clearpage

In summary, we would like:
\begin{itemize}
\item to show that our estimator \(\hat{\sigma}^2\) is asymptotically normally distributed.
\item to have a formula for computing the asymptotic variance.
\end{itemize}
To do so we will use results from the theory on U-statistics.

\bigskip

\textsc{Note:} we can already guess that the estimator \(\hat{\sigma}^2\) (as
most estimators) will be asymptotically distributed because it can be
expressed as a average (see formula \eqref{eq:s(F)}). If we would know
the mean of \(X\), then the terms \(x_i-\mu\) are iid so the
asymptotically normality of \(\hat{\sigma}^2\) follows from the
central limit theorem. It does not give us a formula for the
asymptotic variance though. 

\subsection{Definition of a U-statistic and examples}
\label{sec:orgcc02422}

A U-statistic with kernel \(h\) of order \(k\) is an estimator of the
form:
\begin{align*}
\hat{U} = \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)
\end{align*}
where \(\beta\) is the set of all possible permutations between \(k\)
integers choosen from \(\{1,\ldots,n\}\). We will also assume that the
kernel is symmetric, i.e. the order of the arguments in \(h\) has no
importance. Note that because the observations are iid, \(\hat{U}\) is
an unbiased estimator of \(U\).

\bigskip

\textsc{Example 1}: the simplest example of a U-statistic is the
estimator of mean for which \(k=1\) and \(h\) is the identity
function:
\begin{align*}
\hat{\mu} = \frac{1}{{n \choose 1}} \sum_{(\beta_1) \in \{1,\ldots,n\}} x_{\beta_1} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{align*}

\bigskip

\textsc{Example 2}: our estimator of the variance is also a
U-statistic, but this requires a little bit more work to see that:
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 = \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \hat{\mu} + \hat{\mu}^2\right) \\
&=  \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \frac{1}{n} \sum_{j=1}^n x_j + \hat{\mu}^2\right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( x_i^2 - 2 x_i  x_j + \hat{\mu}^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( (x_i - x_j)^2 - x_j^2 + \hat{\mu}^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \frac{1}{n-1} \sum_{j=1}^n \left(x_j^2 - \hat{\mu}^2\right)
=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \hat{\sigma}^2 \\
\hat{\sigma}^2 &=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \frac{(x_i - x_j)^2}{2} 
=  \frac{2}{n(n-1)} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2}
\hat{\sigma}^2 =  \frac{1}{{n \choose 2}} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2} 
\end{align*}
So the variance estimator is a U-statistic of order 2 with kernel
\(h(x_1,x_2)=\frac{(x_1 - x_2)^2}{2}\).

\bigskip

\textsc{Example 3}: another classical example of U-statistic is the
signed rank statistic which enable to test non-parametrically whether
the center of a distribution is 0. This corresponds to:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
wilcox.test(x)
\end{lstlisting}

\begin{verbatim}

	Wilcoxon signed rank test with continuity correction

data:  x
V = 21, p-value = 0.03501
alternative hypothesis: true location is not equal to 0

Warning message:
In wilcox.test.default(x) : cannot compute exact p-value with ties
\end{verbatim}

Let's take two random realisation of \(X\) and denote thoses \(X_1\)
and \(X_2\) (they are random variables). The parameter of interest (or
true value) is \(U = \Prob[X_1+X_2>0]\) and the corresponding
estimator is:
\begin{align*}
\hat{U} = \frac{1}{{n \choose 2}} \sum_{i=1}^{n} \sum_{i<j} \Ind[x_i+x_j>0]
\end{align*}

\subsection{A major result from the U-statistic theory}
\label{sec:org3fff585}

So far we have seen that our estimator for the variance was a
U-statistic. We will now use the U-statistic theory to obtain its
asymptotic distribution.

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 76) \\
 Let \(\hat{U}\) be a U-statistic of order \(k\) with non-zero first
 component in its H-decomposition. Then \(n^{\frac{1}{2}}
 (\hat{U}-U)\) is asymptotically normal with mean zero and asymptotic
 variance \(\sigma^2_1\) where \(\sigma^2_1\) is the variance of the
 first component in the H-decomposition of \(\hat{U}\).

\bigskip

So under the assumption that the first term of the H-decomposition of
the variance is non 0 then we know that the asymptotic distribution of
our variance estimator is normal and if we are able to compute the
variance of the first term of the H-decomposition then we would also
know the variance parameter of the asymptotic distribution. So it
remains to see what is this H-decomposition and how can we
characterize it.

\subsection{The first term of the H-decomposition}
\label{sec:org6e3d1d7}

The H-decomposition (short for Hoeffling decomposition) enables us to
decompose the estimator of a U-statistic of rank \(k\) into a sum of
\(k\) uncorrelated U-statistics of increasing order (from \(1\) to
\(k\)) with variances of decreasing order in \(n\). As a consequence
the variance of the U-statistic will be asymptotically equal to the
variance of the first non-0 term in the decomposition.

\bigskip

Before going further we introduce:
\begin{itemize}
\item \(X_1\), \ldots, \(X_n\) the random variables associated with each
sample.
\item \(\mathcal{L}_2\) the space of all random variables with zero mean
and finite variance. \\ It is equiped with the inner
product \(\Cov[X,Y]\).
\item the subspaces \(\left(\mathcal{L}_2^{(j)}\right)_{j \in
  \{1,\ldots,k\}}\) where for a given \(j\in \{1,\ldots,k\}\),
\(\mathcal{L}_2^{(j)}\) is the subspace of \(\mathcal{L}_2\)
containing all random variables of the form
\(\sum_{(\beta_1,\ldots,\beta_j) \in \beta}
  \psi(X_{\beta_1},\ldots,X_{\beta_j})\) where \(\beta\) is the set of
all possible permutations between \(j\) integers choosen from
\(\{1,\ldots,n\}\). For instance \(\mathcal{L}_2^{(1)}\) contains
the mean, \(\mathcal{L}_2^{(2)}\) contains the variance, and
\(\mathcal{L}_2^{(j)}\) contains all U-statistics of order \(j\)
with square integrable kernels.
\end{itemize}

We can now define the H-decomposition as the projection of
\(\hat{U}-U\) on the subspaces \(\mathcal{L}_2^{(1)}\),
\(\mathcal{L}_2^{(2)} \cap \left( \mathcal{L}_2^{(1)} \right)^{\perp}\), \ldots, \(\mathcal{L}_2^{(k)} \cap \left( \mathcal{L}_2^{(k-1)}
\right)^{\perp}\). Here \(A^{\perp}\) indicates the space orthogonal
to \(A\). So the first term of the H-decomposition, denoted
\(H^{(1)}\), is the projection of \(\hat{U}-U\) on
\(\mathcal{L}_2^{(1)}\); this is also called the H\(\'{a}\)jek
projection. Clearly all terms of the projection are mutually
orthogonal (or uncorrelated), they are unique (it is a projection) and
they correspond to U-statistics of increasing degree (from \(1\) to
\(k\)). It remains to get a more explicit expression for these term
and show that their variance are of decreasing order in \(n\).

\bigskip

We now focus on the first term and show that \(H^{(1)} = \sum_{i=1}^n
\Esp[\hat{U}-U|X_i]\). Clearly this term belongs to
\(\mathcal{L}_2^{(1)}\). It remains to show that \(\hat{U}-U -
H^{(1)}\) is orthogonal to \(\mathcal{L}_2^{(1)}\). Let consider an element \(V \in \mathcal{L}_2^{(1)}\):
\begin{align*}
\Cov[\hat{U}-U - H^{(1)}, V ] &= \Esp[(\hat{U}-U - H^{(1)} ) V ] \\
&= \sum_{i'=1}^{n} \Esp[(\hat{U}-U - H^{(1)}) \psi(X_{i'}) ] \\ 
&= \sum_{i'=1}^{n} \Esp[\Esp[\hat{U}-U - H^{(1)} \big| X_{i'}] \psi(X_{i'}) ]
\end{align*}
So it remains to show that \(\Esp[\hat{U}-U \big| X_{i'}] = \Esp[H^{(1)}
\big| X_{i'}]\). This follows from:
\begin{align*}
\Esp[H^{(1)} \big| X_{i'}] &= \Esp[\sum_{i=1}^n \Esp[\hat{U}-U|X_i] \big| X_{i'}] = \sum_{i=1}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Ccancelto[red]{0}{\Esp[\Esp[\hat{U}-U|X_i]]}
\end{align*}
where we have used that \(X_i\) and \(X_{i'}\) are independent and \(\Esp[\Esp[\hat{U}-U|X_i]]=\Esp[\hat{U}-U]=0\).

\bigskip

We can now re-express the first term of the H-decomposition more
explicitely:
\begin{align*}
H^{(1)} &= \sum_{i=1}^n \Esp[\hat{U}-U \big| X_i]  \\
&=  \sum_{i=1}^n \Esp[ \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) - U \big| X_i ] \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Esp[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) \big| X_i ] - U \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Ind[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] + \Ind[i \notin \beta] * 0 - U \\
&=  \frac{1}{{n \choose k}} \sum_{i=1}^n \Prob[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
&=  \frac{{n - 1 \choose k - 1}}{{n \choose k}} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
H^{(1)} &=  \frac{k}{n} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U 
\end{align*}
Let's now compute the variance of \(\hat{U}\):
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \Var[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)] \\
&= {n \choose k}^{-2} \Cov[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right),\sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
&= {n \choose k}^{-2} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} \Cov[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right), h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
 \end{align*}
Using the symmetry of the kernel we see that the terms in the double
sum only depends on the number of common observations. To determine a
term with \(j\) common observations, a choose:
\begin{itemize}
\item \(k\) observations among the \(n\) for the first kernel: \({n \choose k}\) possibilities
\item \(c\) common index for the two kernels among the \(k\): \({k \choose c}\) possibilities
\item \(k-c\) observations among the remaining \(n-k\) observations for
the second kernel: \({n - k \choose k - c}\) possibilities
\end{itemize}
So denoting \(\sigma^2_c=\Cov[ h \left(x_{1},\ldots,x_{k} \right), h \left(x_{1},\ldots,x_{c},x'_{c+1},\ldots,x'_{k} \right)]\) this gives:
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \sum_{c=0}^{n} {n \choose k} {k \choose c} {n - k \choose k - c} \sigma^2_c \\
&=  \sum_{c=0}^{k} \frac{k!(n-k)!}{n!}  \frac{k!}{c!(k-c)!} \frac{(n-k)!}{(k-2k+c)!(n-c)!} \sigma^2_c \\
&=  \sum_{c=0}^{k}  \frac{k!^2}{c!(k-c)!^2}  \frac{(n-k)!^2}{(n-2k+c)!n!} \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k)!^2}{(n-2k+c)!n!}\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k) \ldots (n-2k+c+1)}{n \ldots (n-k+1) }\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{n^{- k + 2k - c}}{n^{k}}\right) = \sum_{c=0}^{k} \mathcal{O}\left(n^{-c}\right) \sigma^2_c \\
\end{align*}
So if \(\sigma^2_1 \neq 0\) then the asymptotic variance only depends on the variance of the first term, i.e.:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}] = \frac{k^2}{n^2}  \Var[ \sum_{i=1}^n \Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} \sum_{i=1}^n \Var[\Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} n \Var[\Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ] \\
\Var[\hat{U}] &= \frac{k^2}{n}  \Var[ \Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ]
\end{align*}

\bigskip

In summary we have obtained a formula for the asymptotic variance of
the U-statistic.

\bigskip

\textsc{Example 1}: Sample mean \\
We first compute the H\(\'{a}\)jek projection of the mean:
\begin{align*}
H^{(1)}_{\hat{\mu}} = \frac{1}{n} \sum_{i=1}^n \Esp[x_i|x_i]-\mu = \frac{1}{n}  \sum_{i=1}^n x_i-\mu
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\mu}] =  \Var[H^{(1)}_{\hat{\mu}}] = \frac{1}{n^2}  \sum_{i=1}^n \Var[x_i-\mu] = \frac{1}{n^2}  \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n}
\end{align*}

\clearpage

\textsc{Example 2}: Sample variance \\
We first compute the H\(\'{a}\)jek projection of the variance:
\begin{align*}
H^{(1)}_{\hat{\sigma}^2} &= \frac{2}{n} \sum_{i=1}^n  \Esp[\frac{(x_i-X_2)^2}{2} \bigg|x_i]  - \sigma^2 = \frac{1}{n} \sum_{i=1}^n \Esp[x_i^2 - 2 x_i X_2 + X_2^2 \big|x_i]  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( x_i^2 - 2 x_i \mu + \sigma^2 + \mu^2 \right)  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( (x_i - \mu)^2 - \sigma^2 \right) 
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\sigma}^2] &=   \Var[H^{(1)}_{\hat{\sigma}^2}] = \frac{1}{n^2} \sum_{i=1}^n  \Var[ (x_i - \mu)^2 - \sigma^2]\\
&= \frac{1}{n^2} \sum_{i=1}^n \Esp[(x - \mu)^4]-\Esp[(x - \mu)^2]^2 \\
&=\frac{\mu_4-\left(\sigma^2\right)^2}{n}  
\end{align*}
where \(\mu_4=\Esp[(x - \mu)^4]\) is the fourth moment of the
distribution. For a better approximation in small sample size we could
account for the variance of the second term of the H-decomposition. We
would obtain (\cite{lee1990u}, page 13):
\begin{align*}
\Var[\hat{\sigma}^2] = \frac{\mu_4}{n}-\frac{(n-3)\left(\sigma^2\right)^2}{n(n-1)}  
\end{align*}
When \(\frac{n-3}{n-1}\) is close to 1 then the first order
approximation is sufficient.

\bigskip

\textsc{Example 3}: Signed rank statistic \\
We first compute the H\(\'{a}\)jek projection of the signed rank statistic:
\begin{align*}
 H^{(1)}_{\hat{U}} &=   \frac{2}{n} \sum_{i=1}^n \Esp\left[ \Ind[x_i+X_2>0] \big|x_i \right] - U = \frac{2}{n} \sum_{i=1}^n \Prob[ X_{2} > -x_i \big|x_i] - \Prob[X_{2}> - X_{1}] \\
 &= \frac{2}{n} \sum_{i=1}^n (1 - F(-x_i)) - \Esp[x][(1 - F(-x))] \\
\end{align*}
Since under the null, the distribution is symmetric \(F(-x)=1-F(x)\):
\begin{align*}
 H^{(1)}_{\hat{U}} &= \frac{2}{n} \sum_{i=1}^n F(x_i) - \Esp[x][F(x)]
\end{align*}
We will use that for continuous distribution \(F(x)\) is uniformly
distribution and therefore has variance \(\frac{1}{12}\). So we can
compute the asymptotic variance as:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}_{U}] = \frac{4}{n^2} \sum_{i=1}^n \Var\left[ F(x_i) - \Esp[x][F(x)] \right] = \frac{4}{n^2} n \frac{1}{12} = \frac{1}{3}
\end{align*}

\subsection{Two sample U-statistics}
\label{sec:org3ec33bc}

So far we have assumed that all our observations were iid. But in the
case of GPC, we study two populations (experimental arm and control
arm) so we can only assume to have two independent samples
\(x_1,x_2,\ldots,x_m\) and \(y_1,y_2,\ldots,y_n\) where the first one
contains iid realisations of a random variable \(X\) and the second
one contains iid realisations of a second variable \(Y\). We can now
define a two-sample U-statistic as of order \(k_x\) and \(k_y\) as:
\begin{align*}
\hat{U} = \frac{1}{{m \choose k_x}{n \choose k_y}} \sum_{(\alpha_1,\ldots,\alpha_{k_x})\in \alpha} \sum_{(\beta_1,\ldots,\beta_{k_y})\in \alpha} h(x_{\alpha_{k_x}},\ldots,x_{\alpha_j},y_{\beta_1},\ldots,y_{\beta_{k_y}})
\end{align*}
where \(\alpha\) (resp. \(\beta\)) is the set of all possible
 permutations between \(k_x\) (resp. \(k_y\)) intergers chosen from
 \(\{1,\ldots,m\}\) (resp.  \(\{1,\ldots,n\}\)) and the kernel
 \(h=h(x_1,\ldots,x_{k_x},y_1,\ldots,y_{k_y})\) is permutation symmetric in
 its first \(k_x\) arguments and its last \(k_y\) arguments
 separately. Once more it follows from the independence and iid
 assumptions that \(\hat{U}\) is an unbiased estimator of \(U =
 \Esp[h(X_1,\ldots,X_{k_x},Y_1,\ldots,Y_{k_y})]\) where \(X_1,\ldots,X_{k_x}\)
 (resp. \(Y_1,\ldots,Y_{k_y}\)) are the random variables associated to
 distinct random samples from \(X\) (resp. \(Y\)). The two-sample case
 is a specific case of the Generalized U-statistics introduced in
 section 2.2 in \cite{lee1990u}.

\bigskip

Many results for U-statistics extends to two sample U-statistics. For
instance the H\(\'{a}\)jek projection of \(\hat{U}-U\) becomes:
\begin{align*}
H^{(1)} = \frac{k_x}{m} \sum_{i=1}^{m} \left( \Esp[h(x_1,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x_i] - U \right) + \frac{k_y}{n} \sum_{j=1}^{n} \left( \Esp[h(x_1,\ldots,x_{k_x},y_1,y_2,\ldots,y_{k_y}) \big| y_j] - U \right)
\end{align*}
Before stating any asymptotic results, we need to define what we now
mean by asymptotic (since we have two sample sizes \(m\) and
\(n\)). We now mean by asymptotic that we create an increasing
sequence of \(m\) and \(n\) indexed by \(v\) such that:
\begin{itemize}
\item \(m_v \cvD[][v \rightarrow \infty] \infty\)
\item \(n_v \cvD[][v \rightarrow \infty] \infty\)
\item there exist a \(p \in ]0;1[\) satisfying \(\frac{m}{n+m} \cvD[][v
  \rightarrow \infty] p\) and \(\frac{n}{n+m} \cvD[][v \rightarrow
  \infty] 1-p\).
\end{itemize}

Informally speaking, this means that \(m\) and \(n\) goes to infinity
  at the same speed. Let's denotes:
\begin{align*}
\Var[\Esp[h(x,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x]] &= \sigma^2_{1,0} \\
\Var[\Esp[h(x_1,\ldots,x_{k_x},y,y_2,\ldots,y_{k_y}) \big| y]] &= \sigma^2_{0,1} 
\end{align*}
We then have the following result:

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 141)
 \\ Let \(\hat{U}\) be a U-statistic of order \(k_x\) and
 \(k_y\) with non-zero first component (i.e. \(\sigma^2_{1,0}>0\) and
 \(\sigma^2_{0,1}>0\)) in its H-decomposition. Then
 \((m+n)^{\frac{1}{2}} (\hat{U}-U)\) is asymptotically normal with
 mean zero and asymptotic variance \(p^{-1} k_x^2
 \sigma^2_{1,0}+(1-p)^{-1} k_y^2 \sigma^2_{0,1}\) which is the
 variance of the first component in the H-decomposition of
 \(\hat{U}\).

\bigskip

\textsc{Example 4}: Mann-Whitney statistic \\
If our parameter of interest is \(\Prob[X \leq Y]\) then the estimator:
\begin{align*}
\hat{U} = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j]
\end{align*}
is a U-statistic of order \(k_x=1\) and \(k_y=1\) with kernel \(h(x,y)=\Ind[x \leq y]\)
We first compute the H\(\'{a}\)jek projection of the signed rank statistic:
\begin{align*}
H_{\hat{U}}^{(1)} &= \frac{1}{m} \sum_{i=1}^m \left( \Esp\left[\Ind[x_i \leq y] \big| x_i\right] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Esp\left[\Ind[x \leq y_j] \big| y_j\right] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( \Prob[Y \geq x_i] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Prob[X \leq y_j] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( 1 - F_{-,y}(x_i) - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - U \right) \\
&= - \frac{1}{m} \sum_{i=1}^m \left( F_{-,y}(x_i) - \Esp_x[ F_{-,x}(x) ] \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - \Esp_y[ F_y(y) ] \right) 
\end{align*}
where \(F_{-}\) is the left limit of \(F\), \(F_x\)(resp. \(F_y\))
denoting the cumulative distribution function of \(X\)
(resp. \(Y\)). For continuous distributions \(F_{-}=F\) and under the
null hypothesis that \(F_x=F_y\), we get that:
\begin{align*}
\Var[\hat{U}] = \Var[H_{\hat{U}}^{(1)}] = \frac{1}{m} \frac{1}{12} + \frac{1}{n} \frac{1}{12} = \frac{nm}{12(m+n)}
\end{align*}
If we are not under the null we end up with the formula:
\begin{align*}
\Var[\hat{U}] = \frac{1}{m^2} \sum_{i=1}^m \Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] + \frac{1}{n^2} \sum_{j=1}^n \Var\left[ \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right]
\end{align*}
Noticing that:
\begin{align*}
\Esp\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] = \Esp\left[ \Ind[x_i \leq y]\right] - U = 0 
\end{align*}
We can compute the variance as:
\begin{align*}
\Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] &= \Esp\left[ \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right)^2\right]  \\
&= \int_{x} \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right)  \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right) dF_{X}(x)\\
&= \int_{x} \left(\int_{y_1} \left(\Ind[x \leq y_1] - U \right) dF_Y(y_1)\right)  \left(\int_{y_2} \left(\Ind[x \leq y_2] - U \right) dF_Y(y_2)\right) dF_{X}(x)\\
&= \int_{x} \int_{y_1} \int_{y_2} \left(\Ind[x \leq y_1] - U \right) \left(\Ind[x \leq y_2] - U \right) dF_Y(y_1) dF_Y(y_2) dF_{X}(x)\\
&= \Esp\left[\left( \Ind[x \leq y_1] - U\right)\left(\Ind[x \leq y_2] - U\right)\right] \\
&= \Esp\left[\Ind[x \leq x_1]  \Ind[x \leq y_2] \right] - \Esp\left[\Ind[x \leq y_1]\right] U  - \Esp\left[\Ind[x \leq y_2]\right] U + U^2 \\
&= \Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2
\end{align*}

So the variance is:
\begin{align*}
\Var[\hat{U}] &= \frac{1}{m} \left(\Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2 \right) + \frac{1}{n} \left(\Prob[x_1 \leq y, x_2 \leq y] - \Prob[x \leq y]^2 \right) \\
&= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n}
\end{align*}
In fact we could have a more precise formula by accounting for the
second term in the H-decomposition. \cite{lee1990u} (Theorem 2 page 38, formula 2)
give the general formal for the variance that becomes in the case of a two sample U statistic of degree 1:
\begin{align*}
\Var[\hat{U}] &= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n} + \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm} \\
&= \frac{1}{nm} \left((n-1)\sigma^2_{1,0} + (m-1)\sigma^2_{0,1} + \sigma^2_{1,1} \right) 
\end{align*}
where \(\sigma^2_{1,1} = \Prob[x<y](1-\Prob[x<y])\). Indeed the second
term of the H-decomposition would be the projection of \(\Ind[X \leq
Y]\) on \(X,Y\) where we substract components of the H\(\'{a}\)jek
projection to get the orthogonality between \(H_{\hat{U}}^{(1)}\) and
\(H_{\hat{U}}^{(2)}\) (see theorem 3 page 4 of \cite{lee1990u} for a
generic formula):
\begin{align*}
H_{\hat{U}}^{(2)} &= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \left(\Esp\left[ \Ind[x_i \leq y_j] \big| x_i,y_j\right] - U\right) - \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right) - \left(\Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right) \\
&= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j] - \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] + U
\end{align*}
and:
\begin{align*}
\Var[H_{\hat{U}}^{(2)}] 
&= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Var\left[\Ind[x_i \leq y_j]\right] - \Var\left[\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] - \Var\left[\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
& \qquad \qquad \qquad - \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] - \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
&= \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm} - \frac{1}{m} \sum_{i=1}^m \sum_{j=1}^n \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] + \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right]  \\
&= \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm}
\end{align*}
Since:
\begin{align*}
\Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] &= \Cov\left[\Ind[x_i \leq y_j],\Esp\left[1- \Ind[x_i > y] \big| x_i\right]\right] \\
&= \Cov\left[\Ind[x_i \leq y_j],\Esp\left[- \Ind[y < x_i] \big| x_i\right]\right]
\end{align*}
which under the null hypothesis that \(X\) and \(Y\) have the same
distribution equals \(-\Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x
\leq y_j]\right] \big| y_j\right]\). It remains to show that this is
also true under the alternative hypothesis.



\clearpage 
\end{document}