% Created 2019-01-09 on 18:18
% Intended LaTeX compiler: pdflatex
\documentclass[12pt]{article}

%%%% settings when exporting code %%%% 

\usepackage{listings}
\lstset{
backgroundcolor=\color{white},
basewidth={0.5em,0.4em},
basicstyle=\ttfamily\small,
breakatwhitespace=false,
breaklines=true,
columns=fullflexible,
commentstyle=\color[rgb]{0.5,0,0.5},
frame=single,
keepspaces=true,
keywordstyle=\color{black},
literate={~}{$\sim$}{1},
numbers=left,
numbersep=10pt,
numberstyle=\ttfamily\tiny\color{gray},
showspaces=false,
showstringspaces=false,
stepnumber=1,
stringstyle=\color[rgb]{0,.5,0},
tabsize=4,
xleftmargin=.23in,
emph={anova,apply,class,coef,colnames,colNames,colSums,dim,dcast,for,ggplot,head,if,ifelse,is.na,lapply,list.files,library,logLik,melt,plot,require,rowSums,sapply,setcolorder,setkey,str,summary,tapply},
emphstyle=\color{blue}
}

%%%% packages %%%%%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{changes}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage[normalem]{ulem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{textcomp}
\usepackage{array}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{natbib}
%\VignetteIndexEntry{theory}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
\RequirePackage{fancyvrb}
\DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}
\geometry{a4paper, left=15mm, right=15mm}
\RequirePackage{colortbl} % arrayrulecolor to mix colors
\RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
\usepackage{authblk} % enable several affiliations (clash with beamer)
\renewcommand{\baselinestretch}{1.1}
\geometry{top=1cm}
\definecolor{darkgreen}{RGB}{0,125,0}
\definecolor{darkred}{RGB}{125,0,0}
\definecolor{darkblue}{RGB}{0,0,125}
\usepackage{enumitem}
\RequirePackage{xspace} %
\newcommand\Rlogo{\textbf{\textsf{R}}\xspace} %
\newcommand\Xobs{\tilde{X}}
\newcommand\Yobs{\tilde{Y}}
\newcommand\xobs{\tilde{x}}
\newcommand\yobs{\tilde{y}}
\newcommand\CensT{\varepsilon_X}
\newcommand\CensC{\varepsilon_Y}
\newcommand\censT{e_X}
\newcommand\censC{e_Y}
\RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
\RequirePackage{amsmath}
\RequirePackage{algorithm}
\RequirePackage[noend]{algpseudocode}
\RequirePackage{ifthen}
\RequirePackage{xspace} % space for newcommand macro
\RequirePackage{xifthen}
\RequirePackage{xargs}
\RequirePackage{dsfont}
\RequirePackage{amsmath,stmaryrd,graphicx}
\RequirePackage{prodint} % product integral symbol (\PRODI)
\RequirePackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\RequirePackage[makeroom]{cancel} % cancel terms
\newcommand\Ccancelto[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
\newcommand\defOperator[7]{%
\ifthenelse{\isempty{#2}}{
\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
}{
\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
}
}
\newcommand\defUOperator[5]{%
\ifthenelse{\isempty{#1}}{
#5\left#3 #2 \right#4
}{
\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
}
}
\newcommand{\defBoldVar}[2]{
\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
}
\newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
\newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
\newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}
\newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
\newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
\newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}
\newcommandx\SurvT[1][1=]{\defOperator{#1}{}{S_T}{}{(}{)}{}}
\newcommandx\SurvC[1][1=]{\defOperator{#1}{}{S_C}{}{(}{)}{}}
\newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
\newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
\newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}
\newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
\newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
\newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
\newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
\newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}
\newcommandx\Hypothesis[2][1=,2=]{
\ifthenelse{\isempty{#1}}{
\mathcal{H}
}{
\ifthenelse{\isempty{#2}}{
\mathcal{H}_{#1}
}{
\mathcal{H}^{(#2)}_{#1}
}
}
}
\newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
\ifthenelse{\isempty{#3}}{
\frac{#4 #1}{#4 #2}
}{
\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
}
}
\newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}
\newcommandx\ddpartial[3][1=,2=,3=]{
\ifthenelse{\isempty{#3}}{
\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
}{
\frac{\partial^2 #1}{\partial #2\partial #3}
}
}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Natural{\mathbb{N}}
\newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
\newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
\newcommand\half{\frac{1}{2}}
\newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
\newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
\author{Brice Ozenne}
\date{\today}
\title{Theory supporting the net benefit and Peron's scoring rules}
\hypersetup{
 colorlinks=true,
 citecolor=[rgb]{0,0.5,0},
 urlcolor=[rgb]{0,0,0.5},
 linkcolor=[rgb]{0,0,0.5},
 pdfauthor={Brice Ozenne},
 pdftitle={Theory supporting the net benefit and Peron's scoring rules},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.1 (Org mode 9.0.4)},
 pdflang={English}
 }
\begin{document}

\maketitle
This document describe the relationship between the net benefit and
traditional parameter of interest (e.g. hazard ratio). It also present
how Peron's scoring rules for the survival and competing setting were
derived.

\bigskip

In the examples we will use a sample size of:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- 1e4
\end{lstlisting}

and use the following R packages
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
library(BuyseTest)
library(riskRegression)
library(survival)
\end{lstlisting}

\tableofcontents

\clearpage

\section{Parameter of interest}
\label{sec:org093516e}

Let consider two independent real valued (univariate) random variables
\(X\) and \(Y\). Informally \(X\) refer to the outcome in the
experimental group while \(Y\) refer to the outcome in the control
group. For a given threshold \(\tau \in \Real^{+*}\), the net benefit
can be expressed as:
\begin{align*}
\Delta_\tau = \Prob[X \geq Y+\tau] - \Prob[Y \geq Y+\tau]
\end{align*}
To relate the net benefit to known quantities we will also consider
the case of an infinitesimal threshold \(\tau\):
\begin{align*}
\Delta_+ = \Prob[X > Y] - \Prob[Y > X]
\end{align*}
In any case, \(X\) and \(Y\) play a symetric role in the sense that
given a formula for \(\Prob[X \geq Y+\tau]\) (or \(\Prob[X > Y]\)), we
can substitute \(X\) to \(Y\) and \(Y\) to \(X\) to obtain the formula
for \(\Prob[Y \geq X+\tau]\) (or \(\Prob[Y > X]\)).

\clearpage

\section{Binary variable}
\label{sec:orgd9ef560}
\subsection{Relationship between \(\Delta_+\) and the prevalence}
\label{sec:org73393ce}
\begin{align*}
\Prob[X>Y] = \Prob[X=1,Y=0]
\end{align*}
Using the independence between \(X\) and \(Y\):
\begin{align*}
\Prob[X>Y] = \Prob[X=1]\Prob[Y=0] = \Prob[X=1](1-\Prob[Y=1]) = \Prob[X=1] - \Prob[X=1]\Prob[Y=1]
\end{align*}
By symmetry:
\begin{align*}
\Prob[Y>X] = \Prob[Y=1] - \Prob[Y=1]\Prob[X=1]
\end{align*}
So 
\begin{align*}
\Delta_+ = \Prob[X=1] - \Prob[Y=1]
\end{align*}

\subsection{In R}
\label{sec:orgc35cecf}
Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
prob1 <- 0.4
prob2 <- 0.2
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(tox = rbinom(n, prob = prob1, size = 1), group = "C"),
			data.frame(tox = rbinom(n, prob = prob2, size = 1), group = "T"))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ bin(tox), data = df, method.inference = "none", trace = 0)
\end{lstlisting}
\begin{verbatim}
endpoint threshold   delta   Delta
     tox       0.5 -0.1981 -0.1981
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
prob2 - prob1
\end{lstlisting}

\begin{verbatim}
[1] -0.2
\end{verbatim}

\clearpage

\section{Continuous variable}
\label{sec:orgce1716f}
\subsection{Relationship between \(\Delta\) and Cohen's d}
\label{sec:orgbbe01ec}
Let's consider two independent normally distributed variables with common variance:
\begin{itemize}
\item \(X \sim \Gaus[\mu_X,\sigma^2]\)
\item \(Y \sim \Gaus[\mu_Y,\sigma^2]\)
\end{itemize}
Considering \(Z \sim \Gaus[d,2]\) with \(d = \frac{\mu_X-\mu_Y}{\sigma}\), we express:
\begin{align*}
\Prob[X>Y] &= \Prob[\sigma (Y-X) >0] = \Prob[Z>0] = \Phi(\frac{d}{\sqrt{2}})
\end{align*}
By symmetry
\begin{align*}
\Prob[Y>X] &= \Prob[Z<0] = 1-\Phi(\frac{d}{\sqrt{2}})
\end{align*}
So
\begin{align*}
\Delta = 2*\Phi(\frac{d}{\sqrt{2}})-1
\end{align*}

\subsection{In R}
\label{sec:orgcb6ceb2}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
meanX <- 0
meanY <- 2
sdXY <- 1
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(tox = rnorm(n, mean = meanX, sd = sdXY), group = "C"),
			data.frame(tox = rnorm(n, mean = meanY, sd = sdXY), group = "T"))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ cont(tox), data = df, method.inference = "none", trace = 0)
\end{lstlisting}

\begin{verbatim}
endpoint threshold  delta  Delta
     tox     1e-12 0.8359 0.8359
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d <- (meanY-meanX)/sdXY
2*pnorm(d/sqrt(2))-1
\end{lstlisting}

\begin{verbatim}
[1] 0.8427008
\end{verbatim}

\clearpage

\section{Survival}
\label{sec:orgc81b2a0}
\subsection{Relationship between \(\Delta\) and the hazard ratio}
\label{sec:org34562b4}
For a given cumulative density function \(F(x)\) and a corresponding
probability density function \(f(x)\) we define the hazard by:
\begin{align*}
\lambda(t) &=  \left. \frac{\Prob[t\leq T \leq t+h \big| T\geq t]}{h}\right|_{h \rightarrow 0^+} \\
&= \left. \frac{\Prob[t\leq T \leq t+h]}{\Prob[T\geq t]h}\right|_{h \rightarrow 0^+} \\
&= \frac{f(t)}{1-F(t)}
\end{align*}

\bigskip

Let now consider two times to events following an exponential distribution:
\begin{itemize}
\item \(X \sim Exp(\alpha_X)\). The corresponding hazard function is \(\lambda(t)=\alpha_X\).
\item \(Y \sim Exp(\alpha_Y)\). The corresponding hazard function is \(\lambda(t)=\alpha_Y\).
\end{itemize}
So the hazad ratio is \(HR = \frac{\alpha_X}{\alpha_Y}\). Note that if we use a Cox model we will have:
\begin{align*}
\lambda(t) = \lambda_0(t) \exp(\beta \Ind[group])
\end{align*}
where \(\exp(\beta)\) is the hazard ratio.

\bigskip

\begin{align*}
\Prob[X>Y] &= \int_{0}^{\infty} \Prob[x>Y] d\Prob[x>X] \\
 &= \int_{0}^{\infty} \left(\int_0^{x} \alpha_Y \exp(-\alpha_Y y) dy\right) \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &= \int_{0}^{\infty} \left[-\exp(-\alpha_Y y) \right]_0^{x} \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &= \int_{0}^{\infty} \left(1-\exp(-\alpha_Y x) \right) \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &=  \int_{0}^{\infty} \alpha_X \left(\exp(-\alpha_X x)-\exp(-(\alpha_X+\alpha_Y) x)\right)  dx \\
 &=  \left[\exp(-\alpha_X x)- \frac{\alpha_X}{\alpha_X+\alpha_Y} \exp(-(\alpha_X+\alpha_Y) x)\right]_{0}^{\infty} \\
 &=  1 - \frac{\alpha_X}{\alpha_X+\alpha_Y} = \frac{\alpha_Y}{\alpha_X+\alpha_Y}\\
 &=  \frac{1}{1+HR}
\end{align*}
So \(\Prob[Y>X] = \frac{\alpha_X}{\alpha_Y+\alpha_X} = 1-\frac{1}{1+HR}\) and:
\begin{align*}
\Delta_+ = 2\frac{1}{1+HR}-1 = \frac{1-HR}{1+HR}
\end{align*}

\clearpage

\subsection{Scoring rule in presence of censoring}
\label{sec:orgec425bd}
Let's consider the following random variables:  
\begin{itemize}
\item \(X\) the time to the occurrence of the event in the experimental group.
\item \(\Xobs\) the censored event time in the experimental group,
i.e. \(\Xobs = X \wedge C_X\) where \(C_X\) denotes the censoring time in the experimental group.
\item \(\CensT = \Ind[X \leq C_X]\) the event time indicator in the experimental group.
\item \(Y\) the time to the occurrence of the event in the control group.
\item \(\Yobs\) the censored event time in the control group,
i.e. \(\Yobs = X \wedge C_Y\) where \(C_Y\) denotes the censoring time in the control group.
\item \(\CensC = \Ind[Y \leq C_Y]\) the event time indicator in the control group.
\end{itemize}

We observe one realization \(\left(\xobs, \yobs, \censT, \censC
\right)\) of the random variables \(\left(\Xobs, \Yobs, \censT, \censC
\right)\). We use the short notation \(x \wedge y = \min(x,y)\) and
\(x \vee y = \max(x,y)\). We assume to know the expected survival in
each group (respectively \(\SurvC\) and \(\SurvT\)) at each timepoint.

\subsubsection{Case: \(\SurvT=0,\SurvC=1\)}
\label{sec:org8a13637}

\noindent \textcolor{darkgreen}{Probability in favor of the treatment}:
\begin{align*}
\Prob[X \geq Y + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] 
&= \Prob[X \geq \yobs + \tau \big| X>\xobs]  \\
&= \frac{\Prob[X \geq \yobs + \tau, X>\xobs]}{\Prob[X>\xobs]}  \\
&= \left\{ \begin{array}{ll}
           1 \text{ if } \xobs \geq \yobs + \tau\\
           \frac{\SurvT[(\yobs + \tau)_-]}{\SurvT[\xobs]}  \text{ if } \xobs < \yobs + \tau \\
           \end{array} \right.
\end{align*}

\noindent \textcolor{darkred}{Probability in favor of the control}:
\begin{align*}
\Prob[Y \geq X + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] 
&= \Prob[\yobs \geq X + \tau \big| X>\xobs]  \\
&= 1-\Prob[\yobs < X + \tau \big| X>\xobs]  \\
&= 1-\frac{\Prob[X>\max\left(\xobs,\yobs - \tau\right)]}{\Prob[X>\xobs]}  \\
&= \left\{ \begin{array}{ll}
           0 \text{ if } \xobs \geq \yobs - \tau\\
           1-\frac{\SurvT[\yobs - \tau]}{\SurvT[\xobs]} \text{ if } \xobs < \yobs - \tau \\
           \end{array} \right.
\end{align*}


\subsubsection{Case: \(\SurvT=1,\SurvC=0\)}
\label{sec:org982ad1f}
By symmetry we have: \\
\noindent \textcolor{darkgreen}{Probability in favor of the treatment}:
\begin{align*}
\Prob[X \geq Y + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] 
&= \left\{ \begin{array}{ll}
           0 \text{ if } \yobs \geq \xobs - \tau\\
           1-\frac{\SurvC[\xobs - \tau]}{\SurvC[\yobs]} \text{ if } \yobs < \xobs - \tau\\
           \end{array} \right.
\end{align*}

\noindent \textcolor{darkred}{Probability in favor of the control}:
\begin{align*}
\Prob[Y \geq X + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] 
&= \Prob[\yobs \geq X + \tau \big| X>\xobs]  \\
&= \left\{ \begin{array}{ll}
           1 \text{ if } \yobs \geq \xobs + \tau\\
           \frac{\SurvC[(\xobs + \tau)_-]}{\SurvC[\yobs]} \text{ if } \yobs < \xobs - \tau\\
           \end{array} \right.
\end{align*}




\subsubsection{Case: \(\SurvT=0,\SurvC=0\)}
\label{sec:orgc273756}

\noindent \textcolor{darkgreen}{Probability in favor of the treatment}:
\begin{align*}
&\Prob[X \geq Y + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] \\
&= \Prob[X \geq Y + \tau \big| X>\xobs,  Y>\yobs]  \\
&= \Prob[ \left( X \geq Y + \tau  \right) \cap \left( \xobs \geq Y + \tau  \right) \big| X>\xobs,  Y>\yobs]  
+ \Prob[ \left( X \geq Y + \tau  \right) \cap \left( \xobs < Y + \tau  \right) \big| X>\xobs,  Y>\yobs]  \\
&= \Prob[ \xobs \geq Y + \tau  \big| Y>\yobs]  
+ \frac{
\Prob \left[ \left( X \geq Y + \tau  \right) \cap \left( \xobs < Y + \tau  \right) \cap \Ccancel[red]{\left( X>\xobs \right)} \cap  \left(Y>\yobs \right) \right]
}{
\Prob[\left( X>\xobs \right) \cap  \left(Y>\yobs \right)]
}  \\
&= \Prob[ \xobs \geq Y + \tau  \big| Y>\yobs]  
+ \frac{
\Prob[ \left( X \geq Y + \tau  \right) \cap  \left(Y> \max(\yobs, \xobs-\tau) \right)]
}{
\Prob[\left( X>\xobs \right) \cap  \left(Y>\yobs \right)]
}  
\end{align*}
where we have used that:
\begin{align*}
\left( X \geq Y + \tau \right) \cap \left( \xobs < Y + \tau \right) \implies X > \xobs
\end{align*}
Since:
\begin{align*}
\Prob[A>B] &= \int_{-\infty}^{+\infty}\Prob[A>t] d \Prob[B \leq t] \\
\Prob[(A>B+\tau) \cap (B>b)] &= \int_{b^+}^{+\infty}\Prob[A>t+\tau] d \Prob[B \leq t] \\
&= - \int_{b^+}^{+\infty}\Prob[A>t+\tau] d \Prob[B > t]
\end{align*}
we obtain for \(A=X\), \(B=Y\),\(b=\max(\yobs, \xobs-\tau)\):
\begin{align*}
&\Prob[X \geq Y + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] \\
&= \Prob[ \xobs \geq Y + \tau  \big| Y>\yobs]  
- \frac{
\int_{\max(\yobs, \xobs-\tau)^+}^{\infty} \Prob[\left( X \geq t + \tau  \right)] d \Prob[Y > t]
}{
\SurvT[\xobs]\SurvC[\yobs]
}  \\
&= \Prob[ \xobs \geq Y + \tau  \big| Y>\yobs]  
- \frac{
\int_{\max(\yobs, \xobs-\tau)^+}^{\infty} \SurvT((t+\tau)_{-}) d \SurvC(t)
}{
\SurvT[\xobs]\SurvC[\yobs]
}  \\
\end{align*}
So using the results of the case \(\SurvT=1,\SurvC=0\) we obtain:
\begin{align*}
&\Prob[X \geq Y + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] \\
&= \left\{ \begin{array}{ll}
           - \frac{
\int_{\yobs^+}^{\infty} \SurvT((t+\tau)_-) d \SurvC(t)
}{
\SurvT[\xobs]\SurvC[\yobs]
}  \text{ if } \yobs \geq \xobs - \tau\\
           1-\frac{\SurvC[\xobs - \tau]}{\SurvC[\yobs]} - \frac{
\int_{(\xobs-\tau)^+}^{\infty} \SurvT((t+\tau)_-) d \SurvC(t)
}{
\SurvT[\xobs]\SurvC[\yobs]
} \text{ if } \yobs < \xobs - \tau\\ \\
           \end{array} \right.
\end{align*}

\noindent \textcolor{darkred}{Probability in favor of the control}:
By symmetry we have: \\
\begin{align*}
&\Prob[Y \geq X + \tau \big| \xobs, \yobs, \censT, \censC, \SurvT, \SurvC] \\
&= \left\{ \begin{array}{ll}
           - \frac{
\int_{\xobs^+}^{\infty} \SurvC((t+\tau)_-) d \SurvT(t)
}{
\SurvT[\xobs]\SurvC[\yobs]
}  \text{ if } \xobs \geq \yobs - \tau\\
           1-\frac{\SurvT[\yobs - \tau]}{\SurvT[\xobs]} - \frac{
\int_{(\yobs-\tau)^+}^{\infty} \SurvC((t+\tau)_-) d \SurvT(t)
}{
\SurvT[\xobs]\SurvC[\yobs]
} \text{ if } \xobs < \yobs - \tau\\ \\
           \end{array} \right.
\end{align*}

\clearpage


\subsubsection{Synthesis}
\label{sec:org3449db1}
\textcolor{darkgreen}{Probability in favor of the treatment}: \(\Prob[X \geq Y+\tau \big| \xobs,\yobs,\censT,\censC,\SurvT,\SurvC]\)
\begin{table}[!h]
	\centering
	\setlength{\extrarowheight}{6mm}
	\begin{tabular}{l@{}l@{}l|lll}
		(&$\censT$, & $\censC$) & $\xobs \leq \yobs - \tau$ & $ |\xobs - \yobs| < \tau$ & $\xobs \geq \yobs + \tau$ \\ \hline 
		(&1,&1) & \(0\) & \(0\) & \(1\) \\
		(&1,&0) & \(0\) & \(0\) & $1-\frac{\SurvC[\xobs - \tau]}{\SurvC[\yobs]}$ \\
		(&0,&1) & $\frac{\SurvT[(\yobs+\tau)_{-}]}{\SurvT[\xobs]}$ & $\frac{\SurvT[(\yobs+\tau)_{-}]}{\SurvT[\xobs]}$ & \(1\) \\
		(&0,&0) & \(-\frac{\int_{t>\yobs}^{\infty} \SurvT[(t+\tau)_{-}] d\SurvC[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
              & \(-\frac{\int_{t>\yobs}^{\infty} \SurvT[(t+\tau)_{-}] d\SurvC[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
              & \( 1 - \frac{\SurvC[\xobs-\tau]}{\SurvC[\yobs]} - \frac{\int_{t>\xobs-\tau}^{\infty} \SurvT[(t+\tau)_{-}] d\SurvC[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
\\ \hline
	\end{tabular}
\end{table}

\textcolor{darkred}{Probability in favor of the control}: \(\Prob[Y \geq X+\tau \big| \xobs,\yobs,\censT,\censC,\SurvT,\SurvC]\)

\begin{table}[!h]
	\centering
	\setlength{\extrarowheight}{6mm}
	\begin{tabular}{l@{}l@{}l|lll}
		(&$\censT$, & $\censC$) & $\xobs \leq \yobs - \tau$ & $ |\xobs - \yobs| < \tau$ & $\xobs \geq \yobs + \tau$ \\ \hline 
		(&1,&1) & \(1\) & \(0\) & \(0\) \\
		(&1,&0) & \(1\) & $\frac{\SurvC[(\xobs + \tau)_{-}]}{\SurvC[\yobs]}$ & $\frac{\SurvC[(\xobs + \tau)_{-}]}{\SurvC[\yobs]}$ \\
		(&0,&1) & $1 - \frac{\SurvT[\yobs-\tau]}{\SurvT[\xobs]}$ & \(0\) & \(0\) \\
		(&0,&0) & \( 1 - \frac{\SurvT[\yobs-\tau]}{\SurvT[\xobs]} - \frac{\int_{t>\yobs-\tau}^{\infty} \SurvC[(t+\tau)_{-}] d\SurvT[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
              & \(-\frac{\int_{t>\xobs}^{\infty} \SurvC[(t+\tau)_{-}] d\SurvT[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
              & \(-\frac{\int_{t>\xobs}^{\infty} \SurvC[(t+\tau)_{-}] d\SurvT[t]}{\SurvT[\xobs]\SurvC[\yobs]}\)
\\ \hline
	\end{tabular}
\end{table}

\hfill 

\textcolor{darkblue}{Probability neutral to the treatment}:  \(\Prob[|X-Y| < \tau  \big| \xobs,\yobs,\theta,\eta,S_T,S_C]\)

\begin{align*}
 = 1-\Prob[X \geq Y+\tau  \big|\xobs,\yobs,\theta,\eta,S_T,S_C]-\Prob[Y \geq X+\tau  \big| \xobs,\yobs,\theta,\eta,S_T,S_C]
\end{align*}

\clearpage

\subsection{Partially known survival curve}
\label{sec:org0cb43e3}
In the case where \(x^* < y^* - \tau\), we need an estimate of
\(S_X(y^* - \tau)\) to compute the probability in favor of the
control. If we can only have an estimate of \(S_X\) up to
\(x_{max} < y^* - \tau\) then we can use the following inequality:
\begin{align*}
S_X(x_{max}) &\geq S_X(y^* - \tau) \\
\Prob[x \geq y - \tau | x \geq x^*, y = y^*] &\geq 1 - \frac{ S_X(x_{max})}{S_X(x^*)} \\
\end{align*}

\textbf{Probability of being neutral}:

\begin{align*}
\Prob[|x-y| \leq \tau | x \geq x^*, y = y^*] 
&= 1-\Prob[x \geq y + \tau | x \geq x^*, y = y^*]-\Prob[y \geq x + \tau | x \geq x^*, y = y^*]  \\
&= \frac{ S_X(y^* - \tau \vee x^*) - S_X(y^* + \tau \vee x^*)}{S_X(x^*)}
\end{align*}

Consider the case \(x^*\)
If \(x_{max} > y^* - \tau\) then 
\begin{align*}
\Prob[|x-y| \leq \tau | x \geq x^*, y = y^*] \geq \frac{ S_X(y^* - \tau) - S_X(x_{max})}{S_X(x^*)}
\end{align*}
otherwise
\begin{align*}
\Prob[|x-y| \leq \tau | x \geq x^*, y = y^*] \geq 0
\end{align*}

\textbf{Probability of being uninformative}: It is computed as the complement
to 1 of the sum of the probability of being in favor of the treatment,
in favor of the control, and neutral.

\bigskip

\textsc{Example}:

\begin{itemize}
\item when \(x^* > y^* + \tau\), the probability of being favorable is 1
so the probability of being uninformative is 0.

\item when \(\left|x^* - y^*\right| < \tau\), the probability of being in
favor of the control is 0. If we know the survival in the experimental
group up to time \(y^*\), then we can only say that the probability
of being favorable is bounded below by 0. The probability of being
neutral bounded below by \(1-S_T(y^*)/S_T(x^*)\). The probability of
being uninformative is then \(S_T(y^*)/S_T(x^*)\). Clearly this
probability becomes small when \(S_T(y^*)\) is small. The
approximation by the lower bound becomes exact when \(S_T(y^*)\)
tends to 0.
\end{itemize}

\subsection{In R}
\label{sec:org7ab208d}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaX <- 2
alphaY <- 1
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(time = rexp(n, rate = alphaX), group = "C", event = 1),
			data.frame(time = rexp(n, rate = alphaY), group = "T", event = 1))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ tte(time, censoring = event), data = df,
		  method.inference = "none", trace = 0, method.tte = "Gehan")
\end{lstlisting}
\begin{verbatim}
endpoint threshold  delta  Delta
    time     1e-12 0.3403 0.3403
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.coxph <- coxph(Surv(time,event)~group,data = df)
HR <- as.double(exp(coef(e.coxph)))
c("HR" = alphaY/alphaX, "Delta" = 2*alphaX/(alphaY+alphaX)-1)
c("HR.cox" = HR, "Delta" = (1-HR)/(1+HR))
\end{lstlisting}

\begin{verbatim}
       HR     Delta 
0.5000000 0.3333333
   HR.cox     Delta 
0.4918256 0.3406392
\end{verbatim}

\clearpage

\section{Competing risks}
\label{sec:org9bc38ed}
\subsection{Theory}
\label{sec:orgeee2d13}

\subsubsection{General case (no censoring)}
\label{sec:org7b44c18}
Let consider: 
\begin{itemize}
\item \(X^*_{E}\) the time to the occurrence of the event of interest in the experimental group.
\item \(Y^*_{E}\) the time to the occurrence of the event of interest in the control group.
\item \(X^*_{CR}\) the time to the occurrence of the competing event of interest in the experimental group.
\item \(Y^*_{CR}\) the time to the occurrence of the competing event of interest in the control group.
\end{itemize}
Let denote \(\varepsilon_X = 1 +\Ind[X^*_{E} > X^*_{CR}]\) the event type
indicator in the experimental group and \(\varepsilon_Y = 1 + \Ind[Y^*_{E} >
Y^*_{CR}]\) the event type indicator in control group (\(=1\) when the
cause of interest is realised first and 2 when the competing risk is
realised first).

\bigskip

For each subject either the event of interest or the competing event
is realized. We now define:
\begin{align*}
X = \left\{
              \begin{array}{ll}
                 X^*_{E} \text{ if }\varepsilon_X = 1  \\
                 +\infty \text{ if }\varepsilon_X = 2 
                \end{array}
              \right.
\text{ and }
Y = \left\{
              \begin{array}{ll}
                 Y^*_{E} \text{ if }\varepsilon_Y = 1  \\
                 +\infty \text{ if }\varepsilon_Y = 2 
                \end{array}
              \right.
\end{align*}
i.e. when the event of interest is not realized we say that the time to event is infinite.

\bigskip

We thus have:
\begin{align*}
\Prob[X > Y] 
= & \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
&+ \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=2]\Prob[\varepsilon_X=1,\varepsilon_Y=2] \\
&+ \Prob[X > Y|\varepsilon_X=2,\varepsilon_Y=1]\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&+ \Prob[X > Y|\varepsilon_X=2,\varepsilon_Y=2]\Prob[\varepsilon_X=2,\varepsilon_Y=2] \\
= & \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
&+ 0*\Prob[\varepsilon_X=1,\varepsilon_Y=2] \\
&+ 1*\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&+ 0*\Prob[\varepsilon_X=2,\varepsilon_Y=2] \\
\end{align*}

So \(\Prob[X > Y] = \Prob[X >
Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] +
\Prob[\varepsilon_X=2,\varepsilon_Y=1]\) and:
\begin{align*}
\Delta = &
 \big(\Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1] - \Prob[X < Y|\varepsilon_X=1,\varepsilon_Y=1] \big) \Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
& + \Prob[\varepsilon_X=2,\varepsilon_Y=1] - \Prob[\varepsilon_X=1,\varepsilon_Y=2]
\end{align*}

\clearpage

\subsubsection{Exponential distribution (no censoring)}
\label{sec:org3527cc5}

Now let's assume that:
\begin{itemize}
\item \(X_{E} \sim Exp(\alpha_{E,X})\).
\item \(Y_{E} \sim Exp(\alpha_{E,Y})\).
\item \(X_{CR} \sim Exp(\alpha_{CR,X})\).
\item \(Y_{CR} \sim Exp(\alpha_{CR,Y})\).
\end{itemize}

Then:
\begin{align*}
 \Prob[X_{E} > Y_{E}] &= \Prob[X_{E} >
Y_{E}|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] +
\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&= \frac{1}{(\alpha_{E,X}+\alpha_{CR,X})(\alpha_{E,Y}+\alpha_{CR,Y})} \left(
 \alpha_{E,X}\alpha_{E,Y} \frac{\alpha_{E,X}}{\alpha_{E,X}+\alpha_{E,Y}}
+ \alpha_{CR,X}\alpha_{E,Y} \right) \\
\end{align*}


Just for comparison let's compare to the cumulative incidence. First
we only consider one group and two competing events whose times to
event follow an exponential distribution:
\begin{itemize}
\item \(T_E \sim Exp(\alpha_E)\). The corresponding hazard function is \(\lambda(t)=\alpha_E\).
\item \(T_{CR} \sim Exp(\alpha_{CR})\). The corresponding hazard function is \(\lambda(t)=\alpha_{CR}\).
\end{itemize}
The cumulative incidence function can be written:
\begin{align*}
CIF_1(t) &= \int_0^t \lambda_1(s) S(s_-) ds \\
&= \int_0^t \alpha_E \exp(- (\alpha_E + \alpha_{CR}) * s_-) ds \\
&= \frac{\alpha_E}{\alpha_E + \alpha_{CR}} \left[ \exp(- (\alpha_E + \alpha_{CR}) * s_-)\right]_t^0 \\
&= \frac{\alpha_E}{\alpha_E + \alpha_{CR}} \left(1 - \exp(- (\alpha_E + \alpha_{CR}) * t_-)\right) 
\end{align*}
where \(S(t)\) denote the event free survival and \(s_-\) denotes the right sided limit.

\bigskip

Then applying this formula in the case of two groups gives:
\begin{align*}
CIF_1(t|group = X) &= \frac{\alpha_{E,X}}{\alpha_{E,X} + \alpha_{CR,X}} \left(1 - \exp(- (\alpha_{E,X} + \alpha_{CR,X}) * t_-)\right) \\
CIF_1(t|group = Y) &= \frac{\alpha_{E,Y}}{\alpha_{E,Y} + \alpha_{CR,Y}} \left(1 - \exp(- (\alpha_{E,Y} + \alpha_{CR,Y}) * t_-)\right) 
\end{align*}

\clearpage

\subsection{In R}
\label{sec:org7d22886}

\subsubsection{BuyseTest (no censoring)}
\label{sec:orgde8c1dc}

Setting:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaE.X <- 2
alphaCR.X <- 1
alphaE.Y <- 3
alphaCR.Y <- 2
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(time1 = rexp(n, rate = alphaE.X), time2 = rexp(n, rate = alphaCR.X), group = "1"),
			data.frame(time1 = rexp(n, rate = alphaE.Y), time2 = rexp(n, rate = alphaCR.Y), group = "2"))
df$time <- pmin(df$time1,df$time2) ## first event
df$event <- (df$time2<df$time1)+1 ## type of event
\end{lstlisting}

BuyseTest:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT <- BuyseTest(group ~ tte(time, censoring = event), data = df,
				  method.inference = "none", method.tte = "Gehan",
				  trace = 0)
summary(e.BT, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
       Generalized pairwise comparison with 1 prioritized endpoint

> statistic       : net chance of a better outcome (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs

> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100      41.6       45.12   13.28     0 -0.0352 -0.0352
\end{verbatim}

Note that without censoring one can get the same results by treating
time as a continuous variable that take value \(\infty\) when the
competing risk is observed:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
df$timeXX <- df$time
df$timeXX[df$event==2] <- max(df$time)+1
e.BT.bis <- BuyseTest(group ~ cont(timeXX), data = df,
				  method.inference = "none", trace = 0)
summary(e.BT.bis, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
       Generalized pairwise comparison with 1 prioritized endpoint

> statistic       : net chance of a better outcome (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
  timeXX     1e-12   100      41.6       45.12   13.28     0 -0.0352 -0.0352
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
weight <- (alphaE.X+alphaCR.X)*(alphaE.Y+alphaCR.Y)
exp <- list()
exp$favorable <- 1/weight*(alphaE.X*alphaE.Y*alphaE.X/(alphaE.X+alphaE.Y)+(alphaE.X*alphaCR.Y))
exp$unfavorable <- 1/weight*(alphaE.X*alphaE.Y*alphaE.Y/(alphaE.X+alphaE.Y)+(alphaE.Y*alphaCR.X))
exp$neutral <- alphaCR.X*alphaCR.Y/weight

100*unlist(exp)
\end{lstlisting}

\begin{verbatim}
favorable unfavorable     neutral 
 42.66667    44.00000    13.33333
\end{verbatim}

\subsubsection{BuyseTest (with censoring)}
\label{sec:orga14aa37}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
df$eventC <- df$event
df$eventC[rbinom(n, size = 1, prob = 0.2)==1] <- 0
\end{lstlisting}

BuyseTest (biased):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BTC <- BuyseTest(group ~ tte(time, censoring = eventC), data = df,
				   method.inference = "none", method.tte = "Gehan",
				   trace = 0)
summary(e.BTC, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
       Generalized pairwise comparison with 1 prioritized endpoint

> statistic       : net chance of a better outcome (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs

> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100      31.1       35.15    8.65  25.1 -0.0406 -0.0406
\end{verbatim}

BuyseTest (unbiased):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BTCC <- BuyseTest(group ~ tte(time, censoring = eventC), data = df,
				   method.inference = "none", method.tte = "Gehan corrected",
				   trace = 0)
summary(e.BTCC, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
       Generalized pairwise comparison with 1 prioritized endpoint

> statistic       : net chance of a better outcome (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs
                    IPW for uninformative pairs

> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100     41.52       46.94   11.54     0 -0.0542 -0.0542
\end{verbatim}

\subsubsection{Cumulative incidence}
\label{sec:org33dceae}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaE <- 2
alphaCR <- 1
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- data.frame(time1 = rexp(n, rate = alphaE), time2 = rexp(n, rate = alphaCR), group = "1", event = 1)
df$time <- pmin(df$time1,df$time2)
df$event <- (df$time2<df$time1)+1
\end{lstlisting}

Cumulative incidence (via risk regression):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.CSC <- CSC(Hist(time, event) ~ 1, data = df)
vec.times <- unique(round(exp(seq(log(min(df$time)),log(max(df$time)),length.out = 12)),2))
e.CSCpred <- predict(e.CSC, newdata = data.frame(X = 1), time = vec.times , cause = 1)
\end{lstlisting}

Expected vs. calculated:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
cbind(time = vec.times,
	  CSC = e.CSCpred$absRisk[1,],
	  manual = alphaE/(alphaE+alphaCR)*(1-exp(-(alphaE+alphaCR)*(vec.times)))
	  )
\end{lstlisting}

\begin{verbatim}
     time    CSC     manual
[1,] 0.00 0.0000 0.00000000
[2,] 0.01 0.0186 0.01970298
[3,] 0.02 0.0377 0.03882364
[4,] 0.05 0.0924 0.09286135
[5,] 0.14 0.2248 0.22863545
[6,] 0.42 0.4690 0.47756398
[7,] 1.24 0.6534 0.65051069
[8,] 3.70 0.6703 0.66665659
\end{verbatim}

Could also be obtained treating the outcome as binary:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
mean((df$time<=1)*(df$event==1))
\end{lstlisting}

\begin{verbatim}
[1] 0.6375
\end{verbatim}




\clearpage

\section{Inverse probability weighting}
\label{sec:orge2c78fd}

In case of censoring we can use an inverse probability weighting
approach. Let denote \(\delta_{c,X}\) (resp. \(\delta_{c,Y}\)) the
indicator of no censoring relative to \(\tilde{X}\) (resp \(\tilde{Y}\)), \(\tilde{X}_E\) and \(\tilde{Y}_E\) the
censored event time. We can use inverse probability weighting to
compute the net benefit:
\begin{align*}
\Delta^{IPW} &= \frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[\tilde{Y}>\tilde{X}]-\Ind[\tilde{Y}<\tilde{X}])\\
&= \left\{
                \begin{array}{ll}
                  \frac{1}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[Y>X]-\Ind[Y<X])\text{, if no censoring}\\
                  0\text{, if censoring}
                \end{array}
              \right.
\end{align*}

This is equivalent to weight the informative pairs (i.e. favorable,
unfavorable and neutral) by the inverse of the complement of the
probability of being uninformative. This is what is done by the
argument \texttt{correction.tte} of \texttt{BuyseTest}. This works whenever the
censoring mechanism is independent of the event times and we have a
consistent estimate of \(\Prob[\delta_c]\) since:
\begin{align*}
\Esp[\Delta^{IPW}] &= \Esp\left[ \Esp\left[ \frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[\tilde{Y}>\tilde{X}]-\Ind[\tilde{Y}<\tilde{X}]) \Bigg| \tilde{X}, \tilde{Y} \right] \right]\\
&= \Esp\left[\Esp\left[\frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Bigg| \tilde{X}, \tilde{Y} \right]\right] \Esp\left[\Ind[Y>X]-\Ind[Y<X]\right]\\
&= \frac{\Esp\left[\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}} \right]}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Delta
= \frac{\Esp[\delta_{c,\tilde{X}}]\Esp[\delta_{c,\tilde{Y}}]}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Delta\\
&= \Delta
\end{align*}
where we used the law of total expectation (first line) and the independence between the censoring mecanisms.

\clearpage

\section{Asymptotic distribution}
\label{sec:org365371b}

In this section we restrict ourself to the GPC as defined in
\citep{buyse2010generalized}, i.e. we do not consider Peron scoring rule
nor any correction (like inverse probability weighting).

\subsection{Theory}
\label{sec:org9b9493b}

We consider two independent samples \(x_1,x_2,\ldots,x_m\) and
\(y_1,y_2,\ldots,y_n\) where the first one contains iid realisations
of a random variable \(X\) and the second one contains iid
realisations of a second variable \(Y\). For each realisation we
observe \(p\) endpoints.

\bigskip


The estimator of the net benefit can be written as the difference
between two estimators:
\begin{align*}
\hat{\Delta}_\tau = \widehat{\Prob}[X \geq Y+\tau] - \widehat{\Prob}[Y \geq Y+\tau]
\end{align*}
We denote by \(\phi_k\) the scoring rule relative to \(\Prob[X \geq
Y+\tau]\) for the endpoint \(k\),
e.g. \(\phi_k(x_1,y_1)=\Ind[x_1>y_1]\) for a binary endpoint. The
scoring rule may depend of additional arguments, e.g. a threshold
\(\tau\) but this will be ignored for the moment. Finally, we denote
by \(k_{ij}\) the endpoint at which the pair \((i,j)\) is classified
as favorable or unfavorable. If this does not happen then
\(k_{ij}=p\). With this notations, the estimator \(\widehat{\Prob}[X
\geq Y+\tau]\) can be written as a U-statistic:
\begin{align*}
\widehat{\Prob}[X \geq Y+\tau] = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \phi_{k_{ij}}(x_1,y_1)
\end{align*}
This is a two sample U-statistic of order (1,1) with kernel
\(\phi_{k_{ij}}(x_1,y_1)\) (trivially symmetric in \(x\) and \(y\)
separately). From the U-statistic theory (e.g. see appendix
\ref{SM:Ustat}), it follows that \(\widehat{\Prob}[X \geq Y+\tau]\) is
unbiased, normally distributed, and its iid decomposition is the
H\(\'{a}\)jek projection:
\begin{align*}
H^{(1)}(\widehat{\Prob}[X \geq Y+\tau]) &= \frac{1}{m} \sum_{i=1}^m \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| x_i] - \widehat{\Prob}[X \geq Y+\tau] \right) \\
& \qquad + \frac{1}{n} \sum_{j=1}^n \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| y_j] - \widehat{\Prob}[X \geq Y+\tau]\right) \\
&= \sum_{l=1}^{m+n} H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])
\end{align*}
where \(H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])\) are the
individual terms of the iid decomposition. For instance in the binary
case, the term relative to the \(i-th\) observation of the
experimental group is:
\begin{align*}
H^{(1)}_i(\widehat{\Prob}[X \geq Y + \tau]) & = \frac{\Esp[\phi_{k_{ij}}(x_i,y) \bigg| x_i]-\widehat{\Prob}[X \geq Y+\tau]}{m} = \left\{ \begin{array}{cc} 
 \frac{1-p_y-\widehat{\Prob}[X \geq Y+\tau]}{m} \text{ if } x = 1 \\
\frac{-\widehat{\Prob}[X \geq Y+\tau]}{m} \text{ if } x = 0 \\
\end{array} \right. 
\end{align*}
where \(p_y\) is the proportion of 1 in the control group.


\clearpage

\subsection{Example}
\label{sec:org0c112e8}

Let's consider a case with 2 observations per group:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d <- data.table(id = 1:4, group = c("C","C","T","T"), toxicity = c(1,0,1,0))
d
\end{lstlisting}

\begin{verbatim}
   id group toxicity
1:  1     C        1
2:  2     C        0
3:  3     T        1
4:  4     T        0
\end{verbatim}

We can form 4 pairs:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d2 <- data.table(pair = c("3-1","4-1","3-2","4-2"), 
				 type = c("1-1","0-1","1-0","0-0"),
				 favorable = c(0,0,1,0),
				 unfavorable = c(0,1,0,0))
d2
\end{lstlisting}

\begin{verbatim}
   pair type favorable unfavorable
1:  3-1  1-1         0           0
2:  4-1  0-1         0           1
3:  3-2  1-0         1           0
4:  4-2  0-0         0           0
\end{verbatim}

So \(U=\Prob[X>Y]\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
U <- 1/4
\end{lstlisting}

and the iid terms are:
\begin{align*}
H^{(1)}_1(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{n} \left( \Esp\left[\Ind[x>y_1]\big|y_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_2>y_1]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8 \\
H^{(1)}_2(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{n} \left( \Esp\left[\Ind[x>y_2]\big|y_2\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_2]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_3(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_1>y]\big|x_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_1>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_4(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_2>y]\big|x_2\right]-U \right)&= \frac{ \frac{\Ind[x_2>y_1]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8
\end{align*}

We can use the method \texttt{iid} to extract the iid decomposition in the
BuyseTest package:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "asymptotic", trace = 0)
iid(e.BT)
\end{lstlisting}

\begin{verbatim}
     favorable unfavorable
[1,]    -0.125       0.125
[2,]     0.125      -0.125
[3,]     0.125      -0.125
[4,]    -0.125       0.125
\end{verbatim}

This leads to the following estimates for the variance covariance:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
crossprod(iid(e.BT))
\end{lstlisting}

\begin{verbatim}
            favorable unfavorable
favorable      0.0625     -0.0625
unfavorable   -0.0625      0.0625
\end{verbatim}

Which is precisely what is stored in \texttt{e.BT}:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT@covariance
\end{lstlisting}

\begin{verbatim}
         favorable unfavorable covariance
toxicity    0.0625      0.0625    -0.0625
\end{verbatim}

Note that we could also estimate the variance via the formula given in
\citep{bebu2015large}, e.g.:
\begin{align*}
\sigma^2_{favorable} &= \Prob[X \geq Y_1,X \geq Y_2] - \Prob[X \geq Y]^2 \\
&= 1/8 - 1/16 = 0.0625
\end{align*}
Indeed to compute \(\Prob[X \geq Y_1,X \geq Y_2]\) we distinguish
2*2*2=8 cases (\(X \in \{x_1,x_2\}\), \(Y_1 \in \{y_1,y_2\}\), and
\(Y_2 \in \{y_1,y_2\}\)) and only one satisfyies \(X \geq Y_1,X \geq
Y_2\) (when \(X=x_1\) and \(Y_1=Y_2=y_2\)). This is what is performed when calling:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e2.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "asymptotic-bebu", trace = 0)
e2.BT@covariance
\end{lstlisting}

\begin{verbatim}
         favorable unfavorable covariance
toxicity    0.0625      0.0625    -0.0625
\end{verbatim}

\clearpage

\subsection{Type 1 error in finite sample}
\label{sec:orgab4b0a3}

\subsubsection{Binary endpoint}
\label{sec:org6f6c790}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsBin <- system.time(
	eBin.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								 formula = Treatment ~ bin(toxicity),
								 sample.size = c(10,25,50,100,250),                                   
								 method.inference = "asymptotic", trace = 0,
								 transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsBin
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.47    0.14  211.06
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eBin.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
       Simulation study with Generalized pairwise comparison

> statistic   : net benefit
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        0.0023      0.2235     1  0.2116             0.085            0.113
                                      2  0.2116             0.085            0.113
 25  25        -6e-04      0.1482     1  0.1385             0.084            0.089
                                      2  0.1385             0.084            0.089
 50  50       -0.0015      0.1003     1  0.0990             0.059            0.059
                                      2  0.0990             0.059            0.059
100 100       -0.0018      0.0694     1  0.0704             0.044            0.044
                                      2  0.0704             0.044            0.044
250 250       -0.0011      0.0423     1  0.0446             0.045            0.045
                                      2  0.0446             0.045            0.045

> statistic   : win ratio
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        1.6606      2.2207     1  1.6772            0.1301           0.0381
                                      2  1.6540            0.1301           0.0381
 25  25        1.2083      0.8376     1  0.7044            0.1120           0.0620
                                      2  0.7035            0.1120           0.0620
 50  50        1.0795      0.4534     1  0.4366            0.0730           0.0590
                                      2  0.4365            0.0730           0.0590
100 100        1.0327      0.2948     1  0.2937            0.0520           0.0440
                                      2  0.2936            0.0520           0.0440
250 250        1.0099      0.1715     1  0.1810            0.0490           0.0450
                                      2  0.1810            0.0490           0.0450
\end{verbatim}

\clearpage

\subsubsection{Continuous endpoint}
\label{sec:org4e2f1b1}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsCont <- system.time(
	eCont.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								  formula = Treatment ~ cont(score), 
								  sample.size = c(10,25,50,100,250), 
								  method.inference = "asymptotic", trace = 0,
								  transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsCont
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.86    0.16  195.00
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eCont.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
       Simulation study with Generalized pairwise comparison

> statistic   : net benefit
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        0.0056      0.2642     1  0.2562             0.076            0.130
                                      2  0.2615             0.073            0.130
 25  25        0.0048      0.1593     1  0.1632             0.061            0.088
                                      2  0.1647             0.054            0.087
 50  50        0.0063      0.1156     1  0.1154             0.064            0.080
                                      2  0.1160             0.060            0.080
100 100        0.0015      0.0825     1  0.0816             0.054            0.062
                                      2  0.0818             0.054            0.062
250 250         6e-04       0.052     1  0.0516             0.050            0.053
                                      2  0.0517             0.050            0.053

> statistic   : win ratio
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        1.1973        0.79     1  0.6710             0.090            0.041
                                      2  0.6856             0.082            0.033
 25  25        1.0652      0.3568     1  0.3574             0.063            0.036
                                      2  0.3607             0.062            0.033
 50  50         1.041      0.2466     1  0.2438             0.053            0.054
                                      2  0.2449             0.052            0.053
100 100        1.0167      0.1676     1  0.1672             0.054            0.049
                                      2  0.1676             0.053            0.048
250 250        1.0067      0.1047     1  0.1042             0.050            0.049
                                      2  0.1044             0.050            0.049
\end{verbatim}

\clearpage

\subsubsection{Time to event endpoint (Gehan method)}
\label{sec:org455f214}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsGehan <- system.time(
	eGehan.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								   formula = Treatment ~ tte(eventtime, 
															 censoring = status), 
								   method.tte = "Gehan",
								   sample.size = c(10,25,50,100,250), 
								   method.inference = "asymptotic", trace = 0
								   transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsGehan
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.38    0.25  177.58
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eGehan.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
        Simulation study with Generalized pairwise comparison

 > statistic   : net benefit
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000     -0.003120     0.14812 0.14413          0.087
2:  50  50         1000   1000      0.001308     0.06445 0.06620          0.052
3: 100 100         1000   1000     -0.000690     0.04785 0.04690          0.049
4: 250 250         1000   1000     -0.000647     0.02929 0.02978          0.050

 > statistic   : win ratio
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10          974    974         1.873      3.2268  2.0924        0.04339
2:  50  50         1000   1000         1.092      0.4696  0.4510        0.04500
3: 100 100         1000   1000         1.038      0.3045  0.2983        0.04500
4: 250 250         1000   1000         1.012      0.1818  0.1819        0.04900
\end{verbatim}

\clearpage

\subsubsection{Time to event endpoint (Peron method)}
\label{sec:org3f23fac}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsPeron <- system.time(
	ePeron.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								   formula = Treatment ~ tte(eventtime, 
															 censoring = status), 
								   method.tte = "Peron",
								   sample.size = c(10,25,50,100,250), 
								   method.inference = "asymptotic", trace = 0
								   transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsPeron
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.16    0.13  198.24
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(ePeron.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
        Simulation study with Generalized pairwise comparison

 > statistic   : net benefit
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000     0.0048942     0.22201 0.19280          0.172
2:  50  50         1000   1000     0.0028917     0.11738 0.08918          0.167
3: 100 100         1000   1000     0.0004554     0.10206 0.06325          0.250
4: 250 250         1000   1000     0.0036080     0.08643 0.04016          0.358

 > statistic   : win ratio
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000         1.137      0.6089 0.47252          0.086
2:  50  50         1000   1000         1.037      0.2597 0.19203          0.144
3: 100 100         1000   1000         1.023      0.2186 0.13397          0.231
4: 250 250         1000   1000         1.023      0.1838 0.08482          0.354
\end{verbatim}

\clearpage

\section{References}
\label{sec:org981a8dc}
\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{bibliography}

\endgroup

\clearpage

\appendix

\section{Recall on the U-statistic theory}
\label{SM:Ustat}
This recall is based on chapter 1 of \cite{lee1990u}.

\subsection{Motivating example}
\label{sec:org3299456}

We will illustrate basic results on U-statistics with the following
motivating question: "what is the asymptotic distribution of the
empirical variance estimator?". For a more concrete example, imagine
that we want to provide an estimate with its 95\% confidence interval
of the variability in cholesterol measurements. We assume that we are
able to collect a sample of \(n\) independent and identically
distributed (iid) realisations \((x_1,\ldots,x_n)\) of the random
variable cholesterol, denoted \(X\). We ignore any measurement error.

\subsection{Estimate, estimator, and functionnal}
\label{sec:org391a341}

We can compute an \textbf{estimate} of the variance using the following
\textbf{estimators} \(\hat{\mu}\) and \(\hat{\sigma}^2\):
\begin{align}
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^n x_i \label{eq:m(F)} \\
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 \label{eq:s(F)}
\end{align}
Given a dataset the estimator \(\hat{\sigma}^2\) outputs a
deterministic (i.e. not random) quantity, called the estimate of the
variance. For instance if we observe:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
x <- c(1,3,5,2,1,3)
\end{lstlisting}

then \(s\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
mu <- mean(x)
sigma2 <- var(x)
sigma2
\end{lstlisting}

\begin{verbatim}
[1] 2.3
\end{verbatim}

In general the value of the estimate depends on the dataset. The
estimator acts like a function \(f_n\) that takes as argument some
data and output a quantity of interest. This is often refer to as a
\textbf{functionnal}, e.g. \(\hat{\sigma}^2=f_n(x_1,\ldots,x_n)\). Here we
use the hat notation to emphasise that \(\hat{\sigma}^2\) is a random
quantity: for each new realisation \((x_1,\ldots,x_n)\) of \(X\)
corresponds a realisation for \(\hat{\sigma}^2\) i.e. a possibly
different value for the variance. If mechanism generating the data has
cumulative distribution function \(F\) then we can also define the
true value as \(\sigma^2=f_{\sigma^2}(F)\) (which is a deterministic
value) where:
\begin{align}
\mu(F) &= f_\mu(F) = \int_{-\infty}^{+\infty} x dF(x) \label{eq:M(F)}\\
\sigma^2(F) &= f_{\sigma^2}(F) = \int_{-\infty}^{+\infty} (x - f_\mu(F))^2 dF(x) \label{eq:S(F)}
\end{align}
This can be understood as the limit \(f(F)=\lim_{n \rightarrow \infty}
f_n(x_1,\ldots,x_n)\). Because \(\sigma^2\) and \(f_{\sigma^2}\) are
very close quantities we will not distinguish them in the notation,
i.e. write \(\sigma^2=\sigma^2(F)\). This corresponds to formula (1)
in \cite{lee1990u}. 

\bigskip

When we observe a sample, we use it to plug-in formula \eqref{eq:M(F)}
and \eqref{eq:S(F)} an approximation \(\hat{F}\) of \(F\). Usually our
best guess for \(F\) is \(\hat{F}(x)= \frac{1}{n}\sum_{i=1}^n
\Ind[x \leq x_i]\) where \(\Ind[.]\) is the indicator function taking value
1 if \(.\) is true and 0 otherwise. One can check that when plug-in
\(\hat{F}\) formula \eqref{eq:M(F)} and \eqref{eq:S(F)} becomes formula
\eqref{eq:m(F)} and \eqref{eq:s(F)}.

\bigskip

To summarize:
\begin{itemize}
\item an estimator is a random variable whose realisation depends on the
data. Its realization is called estimate.
\item an estimate is a deterministic value that we obtain using the
observed data (e.g. observed variability is 2.3)
\item a functionnal (of an estimator) is the rule by which an estimator
transforms the data into an estimate.
\end{itemize}

\subsection{Aim}
\label{sec:org0fb7494}

Using formula \eqref{eq:m(F)} and \eqref{eq:s(F)} we can easily estimate
the variance based on the observed realisations of \(X\) (i.e. the
data). However how can we get an confidence interval? What we want is
to quantify the incertainty associated with the estimator, i.e. how
the value output by the functionnal is sensitive to a change in the
dataset. To do so, since the estimator \(\hat{\sigma}^2\) is a random variable, we
can try to characterize its distribution. This is in general
difficult. It is much easier to look at the distribution of the
estimator \(\hat{\sigma}^2\) if we would have an infinite sample size. This is what
we will do, and rely on similations to see how things go in finite
sample size. As we will see, the asymptotic distribution of the
variance is a Gaussian distribution with a variance that we can estimate:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- length(x)
k <- mean((x-mu)^4)
var_sigma2 <- (k-sigma2^2)/n
var_sigma2
\end{lstlisting}

\begin{verbatim}
[1] 0.4898611
\end{verbatim}

So we obtain a 95\% confidence intervals for the variance doing:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
c(estimate = sigma2, 
  lower = sigma2 + qnorm(0.025) * sqrt(var_sigma2),
  upper = sigma2 + qnorm(0.975) * sqrt(var_sigma2))
\end{lstlisting}

\begin{verbatim}
 estimate     lower     upper 
2.3000000 0.9282197 3.6717803
\end{verbatim}

We can see that it is not a very good confidence interval since it
symmetric - we know that the variance is positive so it should extend
more on the right side. But this only problematic in small sample
sizes. In large enough sample sizes the confidence interval will be
correct and we focus on this case.

\clearpage

In summary, we would like:
\begin{itemize}
\item to show that our estimator \(\hat{\sigma}^2\) is asymptotically normally distributed.
\item to have a formula for computing the asymptotic variance.
\end{itemize}
To do so we will use results from the theory on U-statistics.

\bigskip

\textsc{Note:} we can already guess that the estimator \(\hat{\sigma}^2\) (as
most estimators) will be asymptotically distributed because it can be
expressed as a average (see formula \eqref{eq:s(F)}). If we would know
the mean of \(X\), then the terms \(x_i-\mu\) are iid so the
asymptotically normality of \(\hat{\sigma}^2\) follows from the
central limit theorem. It does not give us a formula for the
asymptotic variance though. 

\subsection{Definition of a U-statistic and examples}
\label{sec:org9c1b613}

A U-statistic with kernel \(h\) of order \(k\) is an estimator of the
form:
\begin{align*}
\hat{U} = \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)
\end{align*}
where \(\beta\) is the set of all possible permutations between \(k\)
integers choosen from \(\{1,\ldots,n\}\). We will also assume that the
kernel is symmetric, i.e. the order of the arguments in \(h\) has no
importance. Note that because the observations are iid, \(\hat{U}\) is
an unbiased estimator of \(U\).

\bigskip

\textsc{Example 1}: the simplest example of a U-statistic is the
estimator of mean for which \(k=1\) and \(h\) is the identity
function:
\begin{align*}
\hat{\mu} = \frac{1}{{n \choose 1}} \sum_{(\beta_1) \in \{1,\ldots,n\}} x_{\beta_1} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{align*}

\bigskip

\textsc{Example 2}: our estimator of the variance is also a
U-statistic, but this requires a little bit more work to see that:
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 = \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \hat{\mu} + \hat{\mu}^2\right) \\
&=  \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \frac{1}{n} \sum_{j=1}^n x_j + \hat{\mu}^2\right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( x_i^2 - 2 x_i  x_j + \hat{\mu}^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( (x_i - x_j)^2 - x_j^2 + \hat{\mu}^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \frac{1}{n-1} \sum_{j=1}^n \left(x_j^2 - \hat{\mu}^2\right)
=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \hat{\sigma}^2 \\
\hat{\sigma}^2 &=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \frac{(x_i - x_j)^2}{2} 
=  \frac{2}{n(n-1)} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2}
\hat{\sigma}^2 =  \frac{1}{{n \choose 2}} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2} 
\end{align*}
So the variance estimator is a U-statistic of order 2 with kernel
\(h(x_1,x_2)=\frac{(x_1 - x_2)^2}{2}\).

\bigskip

\textsc{Example 3}: another classical example of U-statistic is the
signed rank statistic which enable to test non-parametrically whether
the center of a distribution is 0. This corresponds to:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
wilcox.test(x)
\end{lstlisting}

\begin{verbatim}

	Wilcoxon signed rank test with continuity correction

data:  x
V = 21, p-value = 0.03501
alternative hypothesis: true location is not equal to 0

Warning message:
In wilcox.test.default(x) : cannot compute exact p-value with ties
\end{verbatim}

Let's take two random realisation of \(X\) and denote thoses \(X_1\)
and \(X_2\) (they are random variables). The parameter of interest (or
true value) is \(U = \Prob[X_1+X_2>0]\) and the corresponding
estimator is:
\begin{align*}
\hat{U} = \frac{1}{{n \choose 2}} \sum_{i=1}^{n} \sum_{i<j} \Ind[x_i+x_j>0]
\end{align*}

\subsection{A major result from the U-statistic theory}
\label{sec:orgd3ce90b}

So far we have seen that our estimator for the variance was a
U-statistic. We will now use the U-statistic theory to obtain its
asymptotic distribution.

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 76) \\
 Let \(\hat{U}\) be a U-statistic of order \(k\) with non-zero first
 component in its H-decomposition. Then \(n^{\frac{1}{2}}
 (\hat{U}-U)\) is asymptotically normal with mean zero and asymptotic
 variance \(\sigma^2_1\) where \(\sigma^2_1\) is the variance of the
 first component in the H-decomposition of \(\hat{U}\).

\bigskip

So under the assumption that the first term of the H-decomposition of
the variance is non 0 then we know that the asymptotic distribution of
our variance estimator is normal and if we are able to compute the
variance of the first term of the H-decomposition then we would also
know the variance parameter of the asymptotic distribution. So it
remains to see what is this H-decomposition and how can we
characterize it.

\subsection{The first term of the H-decomposition}
\label{sec:org8fe7194}

The H-decomposition (short for Hoeffling decomposition) enables us to
decompose the estimator of a U-statistic of rank \(k\) into a sum of
\(k\) uncorrelated U-statistics of increasing order (from \(1\) to
\(k\)) with variances of decreasing order in \(n\). As a consequence
the variance of the U-statistic will be asymptotically equal to the
variance of the first non-0 term in the decomposition.

\bigskip

Before going further we introduce:
\begin{itemize}
\item \(X_1\), \ldots, \(X_n\) the random variables associated with each
sample.
\item \(\mathcal{L}_2\) the space of all random variables with zero mean
and finite variance. \\ It is equiped with the inner
product \(\Cov[X,Y]\).
\item the subspaces \(\left(\mathcal{L}_2^{(j)}\right)_{j \in
  \{1,\ldots,k\}}\) where for a given \(j\in \{1,\ldots,k\}\),
\(\mathcal{L}_2^{(j)}\) is the subspace of \(\mathcal{L}_2\)
containing all random variables of the form
\(\sum_{(\beta_1,\ldots,\beta_j) \in \beta}
  \psi(X_{\beta_1},\ldots,X_{\beta_j})\) where \(\beta\) is the set of
all possible permutations between \(j\) integers choosen from
\(\{1,\ldots,n\}\). For instance \(\mathcal{L}_2^{(1)}\) contains
the mean, \(\mathcal{L}_2^{(2)}\) contains the variance, and
\(\mathcal{L}_2^{(j)}\) contains all U-statistics of order \(j\)
with square integrable kernels.
\end{itemize}

We can now define the H-decomposition as the projection of
\(\hat{U}-U\) on the subspaces \(\mathcal{L}_2^{(1)}\),
\(\mathcal{L}_2^{(2)} \cap \left( \mathcal{L}_2^{(1)} \right)^{\perp}\), \ldots, \(\mathcal{L}_2^{(k)} \cap \left( \mathcal{L}_2^{(k-1)}
\right)^{\perp}\). Here \(A^{\perp}\) indicates the space orthogonal
to \(A\). So the first term of the H-decomposition, denoted
\(H^{(1)}\), is the projection of \(\hat{U}-U\) on
\(\mathcal{L}_2^{(1)}\); this is also called the H\(\'{a}\)jek
projection. Clearly all terms of the projection are mutually
orthogonal (or uncorrelated), they are unique (it is a projection) and
they correspond to U-statistics of increasing degree (from \(1\) to
\(k\)). It remains to get a more explicit expression for these term
and show that their variance are of decreasing order in \(n\).

\bigskip

We now focus on the first term and show that \(H^{(1)} = \sum_{i=1}^n
\Esp[\hat{U}-U|X_i]\). Clearly this term belongs to
\(\mathcal{L}_2^{(1)}\). It remains to show that \(\hat{U}-U -
H^{(1)}\) is orthogonal to \(\mathcal{L}_2^{(1)}\). Let consider an element \(V \in \mathcal{L}_2^{(1)}\):
\begin{align*}
\Cov[\hat{U}-U - H^{(1)}, V ] &= \Esp[(\hat{U}-U - H^{(1)} ) V ] \\
&= \sum_{i'=1}^{n} \Esp[(\hat{U}-U - H^{(1)}) \psi(X_{i'}) ] \\ 
&= \sum_{i'=1}^{n} \Esp[\Esp[\hat{U}-U - H^{(1)} \big| X_{i'}] \psi(X_{i'}) ]
\end{align*}
So it remains to show that \(\Esp[\hat{U}-U \big| X_{i'}] = \Esp[H^{(1)}
\big| X_{i'}]\). This follows from:
\begin{align*}
\Esp[H^{(1)} \big| X_{i'}] &= \Esp[\sum_{i=1}^n \Esp[\hat{U}-U|X_i] \big| X_{i'}] = \sum_{i=1}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Ccancelto[red]{0}{\Esp[\Esp[\hat{U}-U|X_i]]}
\end{align*}
where we have used that \(X_i\) and \(X_{i'}\) are independent and \(\Esp[\Esp[\hat{U}-U|X_i]]=\Esp[\hat{U}-U]=0\).

\bigskip

We can now re-express the first term of the H-decomposition more
explicitely:
\begin{align*}
H^{(1)} &= \sum_{i=1}^n \Esp[\hat{U}-U \big| X_i]  \\
&=  \sum_{i=1}^n \Esp[ \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) - U \big| X_i ] \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Esp[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) \big| X_i ] - U \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Ind[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] + \Ind[i \notin \beta] * 0 - U \\
&=  \frac{1}{{n \choose k}} \sum_{i=1}^n \Prob[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
&=  \frac{{n - 1 \choose k - 1}}{{n \choose k}} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
H^{(1)} &=  \frac{k}{n} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U 
\end{align*}
Let's now compute the variance of \(\hat{U}\):
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \Var[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)] \\
&= {n \choose k}^{-2} \Cov[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right),\sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
&= {n \choose k}^{-2} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} \Cov[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right), h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
 \end{align*}
Using the symmetry of the kernel we see that the terms in the double
sum only depends on the number of common observations. To determine a
term with \(j\) common observations, a choose:
\begin{itemize}
\item \(k\) observations among the \(n\) for the first kernel: \({n \choose k}\) possibilities
\item \(c\) common index for the two kernels among the \(k\): \({k \choose c}\) possibilities
\item \(k-c\) observations among the remaining \(n-k\) observations for
the second kernel: \({n - k \choose k - c}\) possibilities
\end{itemize}
So denoting \(\sigma^2_c=\Cov[ h \left(x_{1},\ldots,x_{k} \right), h \left(x_{1},\ldots,x_{c},x'_{c+1},\ldots,x'_{k} \right)]\) this gives:
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \sum_{c=0}^{n} {n \choose k} {k \choose c} {n - k \choose k - c} \sigma^2_c \\
&=  \sum_{c=0}^{k} \frac{k!(n-k)!}{n!}  \frac{k!}{c!(k-c)!} \frac{(n-k)!}{(k-2k+c)!(n-c)!} \sigma^2_c \\
&=  \sum_{c=0}^{k}  \frac{k!^2}{c!(k-c)!^2}  \frac{(n-k)!^2}{(n-2k+c)!n!} \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k)!^2}{(n-2k+c)!n!}\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k) \ldots (n-2k+c+1)}{n \ldots (n-k+1) }\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{n^{- k + 2k - c}}{n^{k}}\right) = \sum_{c=0}^{k} \mathcal{O}\left(n^{-c}\right) \sigma^2_c \\
\end{align*}
So if \(\sigma^2_1 \neq 0\) then the asymptotic variance only depends on the variance of the first term, i.e.:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}] = \frac{k^2}{n^2}  \Var[ \sum_{i=1}^n \Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} \sum_{i=1}^n \Var[\Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} n \Var[\Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ] \\
\Var[\hat{U}] &= \frac{k^2}{n}  \Var[ \Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ]
\end{align*}

\bigskip

In summary we have obtained a formula for the asymptotic variance of
the U-statistic.

\bigskip

\textsc{Example 1}: Sample mean \\
We first compute the H\(\'{a}\)jek projection of the mean:
\begin{align*}
H^{(1)}_{\hat{\mu}} = \frac{1}{n} \sum_{i=1}^n \Esp[x_i|x_i]-\mu = \frac{1}{n}  \sum_{i=1}^n x_i-\mu
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\mu}] =  \Var[H^{(1)}_{\hat{\mu}}] = \frac{1}{n^2}  \sum_{i=1}^n \Var[x_i-\mu] = \frac{1}{n^2}  \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n}
\end{align*}

\clearpage

\textsc{Example 2}: Sample variance \\
We first compute the H\(\'{a}\)jek projection of the variance:
\begin{align*}
H^{(1)}_{\hat{\sigma}^2} &= \frac{2}{n} \sum_{i=1}^n  \Esp[\frac{(x_i-X_2)^2}{2} \bigg|x_i]  - \sigma^2 = \frac{1}{n} \sum_{i=1}^n \Esp[x_i^2 - 2 x_i X_2 + X_2^2 \big|x_i]  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( x_i^2 - 2 x_i \mu + \sigma^2 + \mu^2 \right)  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( (x_i - \mu)^2 - \sigma^2 \right) 
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\sigma}^2] &=   \Var[H^{(1)}_{\hat{\sigma}^2}] = \frac{1}{n^2} \sum_{i=1}^n  \Var[ (x_i - \mu)^2 - \sigma^2]\\
&= \frac{1}{n^2} \sum_{i=1}^n \Esp[(x - \mu)^4]-\Esp[(x - \mu)^2]^2 \\
&=\frac{\mu_4-\left(\sigma^2\right)^2}{n}  
\end{align*}
where \(\mu_4=\Esp[(x - \mu)^4]\) is the fourth moment of the
distribution. For a better approximation in small sample size we could
account for the variance of the second term of the H-decomposition. We
would obtain (\cite{lee1990u}, page 13):
\begin{align*}
\Var[\hat{\sigma}^2] = \frac{\mu_4}{n}-\frac{(n-3)\left(\sigma^2\right)^2}{n(n-1)}  
\end{align*}
When \(\frac{n-3}{n-1}\) is close to 1 then the first order
approximation is sufficient.

\bigskip

\textsc{Example 3}: Signed rank statistic \\
We first compute the H\(\'{a}\)jek projection of the signed rank statistic:
\begin{align*}
 H^{(1)}_{\hat{U}} &=   \frac{2}{n} \sum_{i=1}^n \Esp\left[ \Ind[x_i+X_2>0] \big|x_i \right] - U = \frac{2}{n} \sum_{i=1}^n \Prob[ X_{2} > -x_i \big|x_i] - \Prob[X_{2}> - X_{1}] \\
 &= \frac{2}{n} \sum_{i=1}^n (1 - F(-x_i)) - \Esp[x][(1 - F(-x))] \\
\end{align*}
Since under the null, the distribution is symmetric \(F(-x)=1-F(x)\):
\begin{align*}
 H^{(1)}_{\hat{U}} &= \frac{2}{n} \sum_{i=1}^n F(x_i) - \Esp[x][F(x)]
\end{align*}
We will use that for continuous distribution \(F(x)\) is uniformly
distribution and therefore has variance \(\frac{1}{12}\). So we can
compute the asymptotic variance as:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}_{U}] = \frac{4}{n^2} \sum_{i=1}^n \Var\left[ F(x_i) - \Esp[x][F(x)] \right] = \frac{4}{n^2} n \frac{1}{12} = \frac{1}{3}
\end{align*}

\subsection{Two sample U-statistics}
\label{sec:orgee4ce38}

So far we have assumed that all our observations were iid. But in the
case of GPC, we study two populations (experimental arm and control
arm) so we can only assume to have two independent samples
\(x_1,x_2,\ldots,x_m\) and \(y_1,y_2,\ldots,y_n\) where the first one
contains iid realisations of a random variable \(X\) and the second
one contains iid realisations of a second variable \(Y\). We can now
define a two-sample U-statistic of order \(k_x\) and \(k_y\) as:
\begin{align*}
\hat{U} = \frac{1}{{m \choose k_x}{n \choose k_y}} \sum_{(\alpha_1,\ldots,\alpha_{k_x})\in \alpha} \sum_{(\beta_1,\ldots,\beta_{k_y})\in \beta} h(x_{\alpha_{k_x}},\ldots,x_{\alpha_j},y_{\beta_1},\ldots,y_{\beta_{k_y}})
\end{align*}
where \(\alpha\) (resp. \(\beta\)) is the set of all possible
 permutations between \(k_x\) (resp. \(k_y\)) intergers chosen from
 \(\{1,\ldots,m\}\) (resp.  \(\{1,\ldots,n\}\)) and the kernel
 \(h=h(x_1,\ldots,x_{k_x},y_1,\ldots,y_{k_y})\) is permutation symmetric in
 its first \(k_x\) arguments and its last \(k_y\) arguments
 separately. Once more it follows from the independence and iid
 assumptions that \(\hat{U}\) is an unbiased estimator of \(U =
 \Esp[h(X_1,\ldots,X_{k_x},Y_1,\ldots,Y_{k_y})]\) where \(X_1,\ldots,X_{k_x}\)
 (resp. \(Y_1,\ldots,Y_{k_y}\)) are the random variables associated to
 distinct random samples from \(X\) (resp. \(Y\)). The two-sample case
 is a specific case of the Generalized U-statistics introduced in
 section 2.2 in \cite{lee1990u}.

\bigskip

Many results for U-statistics extends to two sample U-statistics. For
instance the H\(\'{a}\)jek projection of \(\hat{U}-U\) becomes:
\begin{align*}
H^{(1)} = \frac{k_x}{m} \sum_{i=1}^{m} \left( \Esp[h(x_1,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x_i] - U \right) + \frac{k_y}{n} \sum_{j=1}^{n} \left( \Esp[h(x_1,\ldots,x_{k_x},y_1,y_2,\ldots,y_{k_y}) \big| y_j] - U \right)
\end{align*}
Before stating any asymptotic results, we need to define what we now
mean by asymptotic (since we have two sample sizes \(m\) and
\(n\)). We now mean by asymptotic that we create an increasing
sequence of \(m\) and \(n\) indexed by \(v\) such that:
\begin{itemize}
\item \(m_v \cvD[][v \rightarrow \infty] \infty\)
\item \(n_v \cvD[][v \rightarrow \infty] \infty\)
\item there exist a \(p \in ]0;1[\) satisfying \(\frac{m}{n+m} \cvD[][v
  \rightarrow \infty] p\) and \(\frac{n}{n+m} \cvD[][v \rightarrow
  \infty] 1-p\).
\end{itemize}

Informally speaking, this means that \(m\) and \(n\) goes to infinity
  at the same speed. Let's denotes:
\begin{align*}
\Var[\Esp[h(x,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x]] &= \sigma^2_{1,0} \\
\Var[\Esp[h(x_1,\ldots,x_{k_x},y,y_2,\ldots,y_{k_y}) \big| y]] &= \sigma^2_{0,1} 
\end{align*}
We then have the following result:

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 141)
 \\ Let \(\hat{U}\) be a U-statistic of order \(k_x\) and
 \(k_y\) with non-zero first component (i.e. \(\sigma^2_{1,0}>0\) and
 \(\sigma^2_{0,1}>0\)) in its H-decomposition. Then
 \((m+n)^{\frac{1}{2}} (\hat{U}-U)\) is asymptotically normal with
 mean zero and asymptotic variance \(p^{-1} k_x^2
 \sigma^2_{1,0}+(1-p)^{-1} k_y^2 \sigma^2_{0,1}\) which is the
 variance of the first component in the H-decomposition of
 \(\hat{U}\).

\bigskip

\textsc{Example 4}: Mann-Whitney statistic \\
If our parameter of interest is \(\Prob[X \leq Y]\) then the estimator:
\begin{align*}
\hat{U} = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j]
\end{align*}
is a U-statistic of order \(k_x=1\) and \(k_y=1\) with kernel \(h(x,y)=\Ind[x \leq y]\)
We first compute the H\(\'{a}\)jek projection of the signed rank statistic:
\begin{align*}
H_{\hat{U}}^{(1)} &= \frac{1}{m} \sum_{i=1}^m \left( \Esp\left[\Ind[x_i \leq y] \big| x_i\right] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Esp\left[\Ind[x \leq y_j] \big| y_j\right] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( \Prob[Y \geq x_i] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Prob[X \leq y_j] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( 1 - F_{-,y}(x_i) - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - U \right) \\
&= - \frac{1}{m} \sum_{i=1}^m \left( F_{-,y}(x_i) - \Esp_x[ F_{-,x}(x) ] \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - \Esp_y[ F_y(y) ] \right) 
\end{align*}
where \(F_{-}\) is the left limit of \(F\), \(F_x\)(resp. \(F_y\))
denoting the cumulative distribution function of \(X\)
(resp. \(Y\)). For continuous distributions \(F_{-}=F\) and under the
null hypothesis that \(F_x=F_y\), we get that:
\begin{align*}
\Var[\hat{U}] = \Var[H_{\hat{U}}^{(1)}] = \frac{1}{m} \frac{1}{12} + \frac{1}{n} \frac{1}{12} = \frac{nm}{12(m+n)}
\end{align*}
If we are not under the null we end up with the formula:
\begin{align*}
\Var[\hat{U}] = \frac{1}{m^2} \sum_{i=1}^m \Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] + \frac{1}{n^2} \sum_{j=1}^n \Var\left[ \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right]
\end{align*}
Noticing that:
\begin{align*}
\Esp\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] = \Esp\left[ \Ind[x_i \leq y]\right] - U = 0 
\end{align*}
We can compute the variance as:
\begin{align*}
\Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] &= \Esp\left[ \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right)^2\right]  \\
&= \int_{x} \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right)  \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right) dF_{X}(x)\\
&= \int_{x} \left(\int_{y_1} \left(\Ind[x \leq y_1] - U \right) dF_Y(y_1)\right)  \left(\int_{y_2} \left(\Ind[x \leq y_2] - U \right) dF_Y(y_2)\right) dF_{X}(x)\\
&= \int_{x} \int_{y_1} \int_{y_2} \left(\Ind[x \leq y_1] - U \right) \left(\Ind[x \leq y_2] - U \right) dF_Y(y_1) dF_Y(y_2) dF_{X}(x)\\
&= \Esp\left[\left( \Ind[x \leq y_1] - U\right)\left(\Ind[x \leq y_2] - U\right)\right] \\
&= \Esp\left[\Ind[x \leq x_1]  \Ind[x \leq y_2] \right] - \Esp\left[\Ind[x \leq y_1]\right] U  - \Esp\left[\Ind[x \leq y_2]\right] U + U^2 \\
&= \Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2
\end{align*}

So the variance is:
\begin{align*}
\Var[\hat{U}] &= \frac{1}{m} \left(\Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2 \right) + \frac{1}{n} \left(\Prob[x_1 \leq y, x_2 \leq y] - \Prob[x \leq y]^2 \right) \\
&= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n}
\end{align*}
In fact we could have a more precise formula by accounting for the
second term in the H-decomposition. \cite{lee1990u} (Theorem 2 page 38, formula 2)
give the general formal for the variance that becomes in the case of a two sample U statistic of degree 1:
\begin{align*}
\Var[\hat{U}] &= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n} + \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm} \\
&= \frac{1}{nm} \left((n-1)\sigma^2_{1,0} + (m-1)\sigma^2_{0,1} + \sigma^2_{1,1} \right) 
\end{align*}
where \(\sigma^2_{1,1} = \Prob[x<y](1-\Prob[x<y])\). Indeed the second
term of the H-decomposition would be the projection of \(\Ind[X \leq
Y]\) on \(X,Y\) where we substract components of the H\(\'{a}\)jek
projection to get the orthogonality between \(H_{\hat{U}}^{(1)}\) and
\(H_{\hat{U}}^{(2)}\) (see theorem 3 page 4 of \cite{lee1990u} for a
generic formula):
\begin{align*}
H_{\hat{U}}^{(2)} &= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \left(\Esp\left[ \Ind[x_i \leq y_j] \big| x_i,y_j\right] - U\right) - \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right) - \left(\Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right) \\
&= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j] - \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] + U
\end{align*}
and:
\begin{align*}
\Var[H_{\hat{U}}^{(2)}] 
&= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Var\left[\Ind[x_i \leq y_j]\right] - \Var\left[\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] - \Var\left[\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
& \qquad \qquad \qquad - \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] - \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
&= \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm} - \frac{1}{m} \sum_{i=1}^m \sum_{j=1}^n \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] + \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right]  \\
&= \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm}
\end{align*}
Since:
\begin{align*}
\Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] &= \Cov\left[\Ind[x_i \leq y_j],\Esp\left[1- \Ind[x_i > y] \big| x_i\right]\right] \\
&= \Cov\left[\Ind[x_i \leq y_j],\Esp\left[- \Ind[y < x_i] \big| x_i\right]\right]
\end{align*}
which under the null hypothesis that \(X\) and \(Y\) have the same
distribution equals \(-\Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x
\leq y_j]\right] \big| y_j\right]\). It remains to show that this is
also true under the alternative hypothesis.



\clearpage 

\section{Information about the R session used for this document}
\label{sec:org19ff907}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
sessionInfo()
\end{lstlisting}

\begin{verbatim}
R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252    LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C                   
[5] LC_TIME=Danish_Denmark.1252    

attached base packages:
 [1] stats4    parallel  tools     stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] lava_1.6.4          doParallel_1.0.14   iterators_1.0.10    foreach_1.4.4       BuyseTest_1.7       testthat_2.0.0      prodlim_2018.04.18 
 [8] spelling_1.2        roxygen2_6.1.0.9000 butils.base_1.2     Rcpp_1.0.0          data.table_1.11.8   usethis_1.4.0       devtools_2.0.1     

loaded via a namespace (and not attached):
 [1] compiler_3.5.1            prettyunits_1.0.2         base64enc_0.1-3           remotes_2.0.2             digest_0.6.17             pkgbuild_1.0.2           
 [7] pkgload_1.0.2             lattice_0.20-35           memoise_1.1.0             rlang_0.3.0.1             Matrix_1.2-14             cli_1.0.1                
[13] commonmark_1.6            RcppArmadillo_0.9.200.4.0 withr_2.1.2               stringr_1.3.1             xml2_1.2.0                desc_1.2.0               
[19] fs_1.2.6                  grid_3.5.1                rprojroot_1.3-2           glue_1.3.0                R6_2.3.0                  processx_3.2.0           
[25] survival_2.42-6           sessioninfo_1.1.1         callr_3.0.0               purrr_0.2.5               magrittr_1.5              codetools_0.2-15         
[31] backports_1.1.2           ps_1.1.0                  splines_3.5.1             assertthat_0.2.0          stringi_1.2.4             crayon_1.3.4
\end{verbatim}
\end{document}