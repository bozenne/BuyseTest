% Created 2019-09-11 on 14:20
% Intended LaTeX compiler: pdflatex
\documentclass[12pt]{article}

%%%% settings when exporting code %%%% 

\usepackage{listings}
\lstset{
backgroundcolor=\color{white},
basewidth={0.5em,0.4em},
basicstyle=\ttfamily\small,
breakatwhitespace=false,
breaklines=true,
columns=fullflexible,
commentstyle=\color[rgb]{0.5,0,0.5},
frame=single,
keepspaces=true,
keywordstyle=\color{black},
literate={~}{$\sim$}{1},
numbers=left,
numbersep=10pt,
numberstyle=\ttfamily\tiny\color{gray},
showspaces=false,
showstringspaces=false,
stepnumber=1,
stringstyle=\color[rgb]{0,.5,0},
tabsize=4,
xleftmargin=.23in,
emph={anova,apply,class,coef,colnames,colNames,colSums,dim,dcast,for,ggplot,head,if,ifelse,is.na,lapply,list.files,library,logLik,melt,plot,require,rowSums,sapply,setcolorder,setkey,str,summary,tapply},
emphstyle=\color{blue}
}

%%%% packages %%%%%

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{changes}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage[normalem]{ulem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{array}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{natbib}
%\VignetteIndexEntry{theory}
%\VignetteEngine{R.rsp::tex}
%\VignetteKeyword{R}
\RequirePackage{fancyvrb}
\DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.5,0,0}}}
\geometry{a4paper, left=15mm, right=15mm}
\RequirePackage{colortbl} % arrayrulecolor to mix colors
\RequirePackage{setspace} % to modify the space between lines - incompatible with footnote in beamer
\usepackage{authblk} % enable several affiliations (clash with beamer)
\renewcommand{\baselinestretch}{1.1}
\geometry{top=1cm}
\definecolor{darkgreen}{RGB}{0,125,0}
\definecolor{darkred}{RGB}{125,0,0}
\definecolor{darkblue}{RGB}{0,0,125}
\usepackage{enumitem}
\RequirePackage{xspace} %
\newcommand\Rlogo{\textbf{\textsf{R}}\xspace} %
\newcommand\Xobs{\tilde{X}}
\newcommand\Yobs{\tilde{Y}}
\newcommand\xobs{\tilde{x}}
\newcommand\yobs{\tilde{y}}
\newcommand\CensT{\varepsilon^X}
\newcommand\CensC{\varepsilon^Y}
\newcommand\censT{e^X}
\newcommand\censC{e^Y}
\newcommand\sample{\chi}
\newcommand\VX{\mathbf{X}}
\newcommand\VY{\mathbf{Y}}
\newcommand\VW{\mathbf{W}}
\newcommand\Vtau{\boldsymbol{\tau}}
\RequirePackage{epstopdf} % to be able to convert .eps to .pdf image files
\RequirePackage{amsmath}
\RequirePackage{algorithm}
\RequirePackage[noend]{algpseudocode}
\RequirePackage{ifthen}
\RequirePackage{xspace} % space for newcommand macro
\RequirePackage{xifthen}
\RequirePackage{xargs}
\RequirePackage{dsfont}
\RequirePackage{amsmath,stmaryrd,graphicx}
\RequirePackage{prodint} % product integral symbol (\PRODI)
\RequirePackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\RequirePackage[makeroom]{cancel} % cancel terms
\newcommand\Ccancelto[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\Ccancel[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
\newcommand\defOperator[7]{%
\ifthenelse{\isempty{#2}}{
\ifthenelse{\isempty{#1}}{#7{#3}#4}{#7{#3}#4 \left#5 #1 \right#6}
}{
\ifthenelse{\isempty{#1}}{#7{#3}#4_{#2}}{#7{#3}#4_{#1}\left#5 #2 \right#6}
}
}
\newcommand\defUOperator[5]{%
\ifthenelse{\isempty{#1}}{
#5\left#3 #2 \right#4
}{
\ifthenelse{\isempty{#2}}{\underset{#1}{\operatornamewithlimits{#5}}}{
\underset{#1}{\operatornamewithlimits{#5}}\left#3 #2 \right#4}
}
}
\newcommand{\defBoldVar}[2]{
\ifthenelse{\equal{#2}{T}}{\boldsymbol{#1}}{\mathbf{#1}}
}
\newcommandx\Cov[2][1=,2=]{\defOperator{#1}{#2}{C}{ov}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Esp[2][1=,2=]{\defOperator{#1}{#2}{E}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Prob[2][1=,2=]{\defOperator{#1}{#2}{P}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Qrob[2][1=,2=]{\defOperator{#1}{#2}{Q}{}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Var[2][1=,2=]{\defOperator{#1}{#2}{V}{ar}{\lbrack}{\rbrack}{\mathbb}}
\newcommandx\Binom[2][1=,2=]{\defOperator{#1}{#2}{B}{}{(}{)}{\mathcal}}
\newcommandx\Gaus[2][1=,2=]{\defOperator{#1}{#2}{N}{}{(}{)}{\mathcal}}
\newcommandx\Wishart[2][1=,2=]{\defOperator{#1}{#2}{W}{ishart}{(}{)}{\mathcal}}
\newcommandx\Likelihood[2][1=,2=]{\defOperator{#1}{#2}{L}{}{(}{)}{\mathcal}}
\newcommandx\Information[2][1=,2=]{\defOperator{#1}{#2}{I}{}{(}{)}{\mathcal}}
\newcommandx\Score[2][1=,2=]{\defOperator{#1}{#2}{S}{}{(}{)}{\mathcal}}
\newcommandx\SurvT[1][1=]{\defOperator{#1}{}{S_T}{}{(}{)}{}}
\newcommandx\SurvC[1][1=]{\defOperator{#1}{}{S_C}{}{(}{)}{}}
\newcommandx\SurvThat[1][1=]{\defOperator{#1}{}{\hat{S}_T}{}{(}{)}{}}
\newcommandx\SurvChat[1][1=]{\defOperator{#1}{}{\hat{S}_C}{}{(}{)}{}}
\newcommandx\Vois[2][1=,2=]{\defOperator{#1}{#2}{V}{}{(}{)}{\mathcal}}
\newcommandx\IF[2][1=,2=]{\defOperator{#1}{#2}{IF}{}{(}{)}{\mathcal}}
\newcommandx\Ind[1][1=]{\defOperator{}{#1}{1}{}{(}{)}{\mathds}}
\newcommandx\Max[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{min}}
\newcommandx\Min[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{max}}
\newcommandx\argMax[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmax}}
\newcommandx\argMin[2][1=,2=]{\defUOperator{#1}{#2}{(}{)}{argmin}}
\newcommandx\cvD[2][1=D,2=n \rightarrow \infty]{\xrightarrow[#2]{#1}}
\newcommandx\Hypothesis[2][1=,2=]{
\ifthenelse{\isempty{#1}}{
\mathcal{H}
}{
\ifthenelse{\isempty{#2}}{
\mathcal{H}_{#1}
}{
\mathcal{H}^{(#2)}_{#1}
}
}
}
\newcommandx\dpartial[4][1=,2=,3=,4=\partial]{
\ifthenelse{\isempty{#3}}{
\frac{#4 #1}{#4 #2}
}{
\left.\frac{#4 #1}{#4 #2}\right\rvert_{#3}
}
}
\newcommandx\dTpartial[3][1=,2=,3=]{\dpartial[#1][#2][#3][d]}
\newcommandx\ddpartial[3][1=,2=,3=]{
\ifthenelse{\isempty{#3}}{
\frac{\partial^{2} #1}{\left( \partial #2\right)^2}
}{
\frac{\partial^2 #1}{\partial #2\partial #3}
}
}
\newcommand\Real{\mathbb{R}}
\newcommand\Rational{\mathbb{Q}}
\newcommand\Natural{\mathbb{N}}
\newcommand\trans[1]{{#1}^\intercal}%\newcommand\trans[1]{{\vphantom{#1}}^\top{#1}}
\newcommand{\independent}{\mathrel{\text{\scalebox{1.5}{$\perp\mkern-10mu\perp$}}}}
\newcommand\half{\frac{1}{2}}
\newcommand\normMax[1]{\left|\left|#1\right|\right|_{max}}
\newcommand\normTwo[1]{\left|\left|#1\right|\right|_{2}}
\newcommandx\Hajek{H\'{a}jek\xspace}
\author{Brice Ozenne}
\date{\today}
\title{Theory supporting the net benefit and Peron's scoring rules}
\hypersetup{
 colorlinks=true,
 citecolor=[rgb]{0,0.5,0},
 urlcolor=[rgb]{0,0,0.5},
 linkcolor=[rgb]{0,0,0.5},
 pdfauthor={Brice Ozenne},
 pdftitle={Theory supporting the net benefit and Peron's scoring rules},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.2.1 (Org mode 9.0.4)},
 pdflang={English}
 }
\begin{document}

\maketitle
This document describes the theoretical background of the methods
implemented in the \texttt{BuyseTest} package. Some are illutrated on
specific examples where we will use the following R packages:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
library(BuyseTest)
library(data.table)
library(survival)
library(riskRegression)
\end{lstlisting}

\bigskip

\tableofcontents

\clearpage

\section{Parameter of interest}
\label{sec:org1d8cd75}

\subsection{Univariate case}
\label{sec:org5ebb5d2}
Consider two independent real valued (univariate) random variables
\(X\) and \(Y\). Informally \(X\) refer to the outcome in the
experimental group while \(Y\) refer to the outcome in the control
group. For a given threshold \(\tau \in \Real^{+*}\), the net benefit
can be expressed as:
\begin{align*}
\Delta_\tau = \Prob[X \geq Y + \tau] - \Prob[Y \geq X + \tau]
\end{align*}
To relate the net benefit to known quantities we will also consider
the case of an infinitesimal threshold \(\tau\):
\begin{align*}
\Delta_+ = \Prob[X > Y] - \Prob[Y > X]
\end{align*}
In any case, \(X\) and \(Y\) play a symetric role in the sense that
given a formula for \(\Prob[X \geq Y+\tau]\) (or \(\Prob[X > Y]\)), we
can substitute \(X\) to \(Y\) and \(Y\) to \(X\) to obtain the formula
for \(\Prob[Y \geq X+\tau]\) (or \(\Prob[Y > X]\)).

\subsection{Multivariate case}
\label{sec:org320bdac}

In the multivariate case we now have a vector of outcome \(\VX =
(X_1,\ldots,X_K)\) and \(\VY = (Y_1,\ldots,Y_K)\) for each group, and
a vector of clinical thresholds \(\Vtau =
(\tau_1,\ldots,\tau_K)\). When the same outcome is used at several
priorities, we will also denote by \(h(\tau_l)\) the previously used
threshold relative to outcome \(l\) (if it is the first time that
outcome \(l\) is analyzed then \(h(\tau_l)=+\infty\)). The values of
each outcomes are assumed to be ordered such that we are able to
define a partial ordering for any two realizations of the \(l\)-th
outcome in each group \(x_k \succ y_k = \Ind[x_k \geq y_k + \tau_k]\) where \(\Ind[.]\) denotes the indicator function. 


\bigskip

We also introduce the weights \(\VW = (W_1,\ldots,W_K)\) indicating
whether the pair could be classified at the previous priorities with
distinct outcome.
\begin{itemize}
\item \textbf{General case}: for \(k \in \{1,\ldots,K\}\):
\end{itemize}
\begin{align*}
W_k = \left\{ \begin{array}{c} 
1\text{, if } k = 1 \\ 
\prod_{k' \in S_k } \left(\left|X_{k'} - Y_{k'}\right| < \tau_{k'}\right) \text{, otherwise} 
\end{array} \right.
\end{align*}
where \(S_k=\left\{k \text{ such that }  k \in \{1,\ldots,k-1\} \text{ and } \left(X_k,Y_k\right) \neq
\left(X_{k'},Y_{k'}\right)\right\}\), i.e. the indexes of the previous
outcomes that are distinct from the \(k\)-th outcome.

\begin{itemize}
\item \textbf{Distinct outcomes}: for \(k \in \{1,\ldots,K\}\):
\end{itemize}
\begin{align*}
W_k = \left\{ \begin{array}{c} 
1\text{, if } k = 1 \\ 
\prod_{k'=1}^{k-1} \left(\left|X_{k'} - Y_{k'}\right| < \tau_{k'}\right) \text{, otherwise} 
\end{array} \right.
\end{align*}

\clearpage

The net benefit is then defined as \(\Delta_{\Vtau} = \Prob[\VX \succ \VY + \Vtau] - \Prob[\VY \succ \VX + \Vtau]\)

\begin{itemize}
\item \textbf{General case}:
\end{itemize}
\begin{align*}
\Prob[\VX \succ \VY + \Vtau] = \sum_{k=1}^K \Prob\left[\left(W_k=1\right) \cap
\left(X_k \in [Y_k + \tau_k;Y_k + \tau_k + h(\tau_k)[ \right) \right] 
\end{align*}
\begin{itemize}
\item \textbf{Distinct outcomes}:
\end{itemize}
\begin{align*}
\Prob[\VX \succ \VY + \Vtau] = \sum_{k=1}^K
  \Prob\left[\left(W_k=1\right) \cap \left(X_k \geq Y_k + \tau_k\right) \right]
\end{align*}

\bigskip

Note: In the following we focus, when possible, on the univariate case to
simplify the notations and the exposition.

\clearpage

\section{Relationship between the net benefit and classical summary statistics}
\label{sec:org262740e}
\subsection{Binary variable}
\label{sec:orgdb3fd55}
\subsubsection{Relationship between \(\Delta_+\) and the prevalence}
\label{sec:org12019aa}
\begin{align*}
\Prob[X>Y] = \Prob[X=1,Y=0]
\end{align*}
Using the independence between \(X\) and \(Y\):
\begin{align*}
\Prob[X>Y] = \Prob[X=1]\Prob[Y=0] = \Prob[X=1](1-\Prob[Y=1]) = \Prob[X=1] - \Prob[X=1]\Prob[Y=1]
\end{align*}
By symmetry:
\begin{align*}
\Prob[Y>X] = \Prob[Y=1] - \Prob[Y=1]\Prob[X=1]
\end{align*}
So 
\begin{align*}
\Delta_+ = \Prob[X=1] - \Prob[Y=1]
\end{align*}

\subsubsection{In R}
\label{sec:orgd5966f2}
Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
prob1 <- 0.4
prob2 <- 0.2
n <- 1e4
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(tox = rbinom(n, prob = prob1, size = 1), group = "C"),
			data.frame(tox = rbinom(n, prob = prob2, size = 1), group = "T"))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ bin(tox), data = df, method.inference = "none", trace = 0)
\end{lstlisting}
\begin{verbatim}
endpoint threshold   delta   Delta
     tox       0.5 -0.1981 -0.1981
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
prob2 - prob1
\end{lstlisting}

\begin{verbatim}
[1] -0.2
\end{verbatim}

\clearpage

\subsection{Continuous variable}
\label{sec:org3dae1ae}
\subsubsection{Relationship between \(\Delta_+\) and Cohen's d}
\label{sec:org728a81b}
Let's consider two independent normally distributed variables with common variance:
\begin{itemize}
\item \(X \sim \Gaus[\mu_X,\sigma^2]\)
\item \(Y \sim \Gaus[\mu_Y,\sigma^2]\)
\end{itemize}
Considering \(Z \sim \Gaus[d,2]\) with \(d = \frac{\mu_X-\mu_Y}{\sigma}\), we express:
\begin{align*}
\Prob[X>Y] &= \Prob[\sigma (Y-X) >0] = \Prob[Z>0] = \Phi(\frac{d}{\sqrt{2}})
\end{align*}
By symmetry
\begin{align*}
\Prob[Y>X] &= \Prob[Z<0] = 1-\Phi(\frac{d}{\sqrt{2}})
\end{align*}
So
\begin{align*}
\Delta = 2*\Phi(\frac{d}{\sqrt{2}})-1
\end{align*}

\subsubsection{In R}
\label{sec:orgce21389}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
meanX <- 0
meanY <- 2
sdXY <- 1
n <- 1e4
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(tox = rnorm(n, mean = meanX, sd = sdXY), group = "C"),
			data.frame(tox = rnorm(n, mean = meanY, sd = sdXY), group = "T"))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ cont(tox), data = df, method.inference = "none", trace = 0)
\end{lstlisting}

\begin{verbatim}
endpoint threshold  delta  Delta
     tox     1e-12 0.8359 0.8359
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d <- (meanY-meanX)/sdXY
2*pnorm(d/sqrt(2))-1
\end{lstlisting}

\begin{verbatim}
[1] 0.8427008
\end{verbatim}

\clearpage

\subsection{Time to event variable (survival)}
\label{sec:orgf8f5095}
\subsubsection{Relationship between \(\Delta_+\) and the hazard ratio}
\label{sec:org6bf00c5}
For a given cumulative density function \(F(x)\) and a corresponding
probability density function \(f(x)\) we define the hazard by:
\begin{align*}
\lambda(t) &=  \left. \frac{\Prob[t\leq T \leq t+h \big| T\geq t]}{h}\right|_{h \rightarrow 0^+} \\
&= \left. \frac{\Prob[t\leq T \leq t+h]}{\Prob[T\geq t]h}\right|_{h \rightarrow 0^+} \\
&= \frac{f(t)}{1-F(t)}
\end{align*}

\bigskip

Let now consider two times to events following an exponential distribution:
\begin{itemize}
\item \(X \sim Exp(\alpha_X)\). The corresponding hazard function is \(\lambda(t)=\alpha_X\).
\item \(Y \sim Exp(\alpha_Y)\). The corresponding hazard function is \(\lambda(t)=\alpha_Y\).
\end{itemize}
So the hazad ratio is \(HR = \frac{\alpha_X}{\alpha_Y}\). Note that if we use a Cox model we will have:
\begin{align*}
\lambda(t) = \lambda_0(t) \exp(\beta \Ind[group])
\end{align*}
where \(\exp(\beta)\) is the hazard ratio.

\bigskip

\begin{align*}
\Prob[X>Y] &= \int_{0}^{\infty} \Prob[x>Y] d\Prob[x>X] \\
 &= \int_{0}^{\infty} \left(\int_0^{x} \alpha_Y \exp(-\alpha_Y y) dy\right) \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &= \int_{0}^{\infty} \left[-\exp(-\alpha_Y y) \right]_0^{x} \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &= \int_{0}^{\infty} \left(1-\exp(-\alpha_Y x) \right) \left( \alpha_X \exp(-\alpha_X x) dx \right) \\
 &=  \int_{0}^{\infty} \alpha_X \left(\exp(-\alpha_X x)-\exp(-(\alpha_X+\alpha_Y) x)\right)  dx \\
 &=  \left[\exp(-\alpha_X x)- \frac{\alpha_X}{\alpha_X+\alpha_Y} \exp(-(\alpha_X+\alpha_Y) x)\right]_{0}^{\infty} \\
 &=  1 - \frac{\alpha_X}{\alpha_X+\alpha_Y} = \frac{\alpha_Y}{\alpha_X+\alpha_Y}\\
 &=  \frac{1}{1+HR}
\end{align*}
So \(\Prob[Y>X] = \frac{\alpha_X}{\alpha_Y+\alpha_X} = 1-\frac{1}{1+HR}\) and:
\begin{align*}
\Delta_+ = 2\frac{1}{1+HR}-1 = \frac{1-HR}{1+HR}
\end{align*}

\clearpage

\subsubsection{In R}
\label{sec:orgc4a678d}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaX <- 2
alphaY <- 1
n <- 1e4
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(time = rexp(n, rate = alphaX), group = "C", event = 1),
			data.frame(time = rexp(n, rate = alphaY), group = "T", event = 1))
\end{lstlisting}

Buyse test:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest(group ~ tte(time, censoring = event), data = df,
		  method.inference = "none", trace = 0, scoring.rule = "Gehan")
\end{lstlisting}
\begin{verbatim}
endpoint threshold  delta  Delta
    time     1e-12 0.3403 0.3403
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.coxph <- coxph(Surv(time,event)~group,data = df)
HR <- as.double(exp(coef(e.coxph)))
c("HR" = alphaY/alphaX, "Delta" = 2*alphaX/(alphaY+alphaX)-1)
c("HR.cox" = HR, "Delta" = (1-HR)/(1+HR))
\end{lstlisting}

\begin{verbatim}
       HR     Delta 
0.5000000 0.3333333
   HR.cox     Delta 
0.4918256 0.3406392
\end{verbatim}

\clearpage

\subsection{Time to event variable (competing risks)}
\label{sec:orgb342dc9}
\subsubsection{Relationship between \(\Delta_+\) and the hazard function}
\label{sec:org435e992}
Let consider: 
\begin{itemize}
\item \(X^*_{E}\) the time to the occurrence of the event of interest in the experimental group.
\item \(Y^*_{E}\) the time to the occurrence of the event of interest in the control group.
\item \(X^*_{CR}\) the time to the occurrence of the competing event of interest in the experimental group.
\item \(Y^*_{CR}\) the time to the occurrence of the competing event of interest in the control group.
\end{itemize}
Let denote \(\varepsilon_X = 1 +\Ind[X^*_{E} > X^*_{CR}]\) the event type
indicator in the experimental group and \(\varepsilon_Y = 1 + \Ind[Y^*_{E} >
Y^*_{CR}]\) the event type indicator in control group (\(=1\) when the
cause of interest is realised first and 2 when the competing risk is
realised first).

\bigskip

For each subject either the event of interest or the competing event
is realized. We now define:
\begin{align*}
X = \left\{
              \begin{array}{ll}
                 X^*_{E} \text{ if }\varepsilon_X = 1  \\
                 +\infty \text{ if }\varepsilon_X = 2 
                \end{array}
              \right.
\text{ and }
Y = \left\{
              \begin{array}{ll}
                 Y^*_{E} \text{ if }\varepsilon_Y = 1  \\
                 +\infty \text{ if }\varepsilon_Y = 2 
                \end{array}
              \right.
\end{align*}
i.e. when the event of interest is not realized we say that the time to event is infinite.

\bigskip

We thus have:
\begin{align*}
\Prob[X > Y] 
= & \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
&+ \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=2]\Prob[\varepsilon_X=1,\varepsilon_Y=2] \\
&+ \Prob[X > Y|\varepsilon_X=2,\varepsilon_Y=1]\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&+ \Prob[X > Y|\varepsilon_X=2,\varepsilon_Y=2]\Prob[\varepsilon_X=2,\varepsilon_Y=2] \\
= & \Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
&+ 0*\Prob[\varepsilon_X=1,\varepsilon_Y=2] \\
&+ 1*\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&+ 0*\Prob[\varepsilon_X=2,\varepsilon_Y=2] \\
\end{align*}

So \(\Prob[X > Y] = \Prob[X >
Y|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] +
\Prob[\varepsilon_X=2,\varepsilon_Y=1]\) and:
\begin{align*}
\Delta = &
 \big(\Prob[X > Y|\varepsilon_X=1,\varepsilon_Y=1] - \Prob[X < Y|\varepsilon_X=1,\varepsilon_Y=1] \big) \Prob[\varepsilon_X=1,\varepsilon_Y=1] \\
& + \Prob[\varepsilon_X=2,\varepsilon_Y=1] - \Prob[\varepsilon_X=1,\varepsilon_Y=2]
\end{align*}

Now let's assume that:
\begin{itemize}
\item \(X_{E} \sim Exp(\alpha_{E,X})\).
\item \(Y_{E} \sim Exp(\alpha_{E,Y})\).
\item \(X_{CR} \sim Exp(\alpha_{CR,X})\).
\item \(Y_{CR} \sim Exp(\alpha_{CR,Y})\).
\end{itemize}

Then:
\begin{align*}
 \Prob[X_{E} > Y_{E}] &= \Prob[X_{E} >
Y_{E}|\varepsilon_X=1,\varepsilon_Y=1]\Prob[\varepsilon_X=1,\varepsilon_Y=1] +
\Prob[\varepsilon_X=2,\varepsilon_Y=1] \\
&= \frac{1}{(\alpha_{E,X}+\alpha_{CR,X})(\alpha_{E,Y}+\alpha_{CR,Y})} \left(
 \alpha_{E,X}\alpha_{E,Y} \frac{\alpha_{E,X}}{\alpha_{E,X}+\alpha_{E,Y}}
+ \alpha_{CR,X}\alpha_{E,Y} \right) \\
\end{align*}


Just for comparison let's compare to the cumulative incidence. First
we only consider one group and two competing events whose times to
event follow an exponential distribution:
\begin{itemize}
\item \(T_E \sim Exp(\alpha_E)\). The corresponding hazard function is \(\lambda(t)=\alpha_E\).
\item \(T_{CR} \sim Exp(\alpha_{CR})\). The corresponding hazard function is \(\lambda(t)=\alpha_{CR}\).
\end{itemize}
The cumulative incidence function can be written:
\begin{align*}
CIF_1(t) &= \int_0^t \lambda_1(s) S(s_-) ds \\
&= \int_0^t \alpha_E \exp(- (\alpha_E + \alpha_{CR}) * s_-) ds \\
&= \frac{\alpha_E}{\alpha_E + \alpha_{CR}} \left[ \exp(- (\alpha_E + \alpha_{CR}) * s_-)\right]_t^0 \\
&= \frac{\alpha_E}{\alpha_E + \alpha_{CR}} \left(1 - \exp(- (\alpha_E + \alpha_{CR}) * t_-)\right) 
\end{align*}
where \(S(t)\) denote the event free survival and \(s_-\) denotes the right sided limit.

\bigskip

Then applying this formula in the case of two groups gives:
\begin{align*}
CIF_1(t|group = X) &= \frac{\alpha_{E,X}}{\alpha_{E,X} + \alpha_{CR,X}} \left(1 - \exp(- (\alpha_{E,X} + \alpha_{CR,X}) * t_-)\right) \\
CIF_1(t|group = Y) &= \frac{\alpha_{E,Y}}{\alpha_{E,Y} + \alpha_{CR,Y}} \left(1 - \exp(- (\alpha_{E,Y} + \alpha_{CR,Y}) * t_-)\right) 
\end{align*}

\clearpage

\subsubsection{In R}
\label{sec:org319bc9b}
\begin{enumerate}
\item No censoring
\label{sec:org3dc0736}
Setting:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaE.X <- 2
alphaCR.X <- 1
alphaE.Y <- 3
alphaCR.Y <- 2
n <- 1e3
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- rbind(data.frame(time1 = rexp(n, rate = alphaE.X), time2 = rexp(n, rate = alphaCR.X), group = "1"),
			data.frame(time1 = rexp(n, rate = alphaE.Y), time2 = rexp(n, rate = alphaCR.Y), group = "2"))
df$time <- pmin(df$time1,df$time2) ## first event
df$event <- (df$time2<df$time1)+1 ## type of event
\end{lstlisting}

BuyseTest:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT <- BuyseTest(group ~ tte(time, censoring = event), data = df,
				  method.inference = "none", scoring.rule = "Gehan",
				  trace = 0)
summary(e.BT, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
      Generalized pairwise comparisons with 1 endpoint

> statistic       : net benefit (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs
> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100      41.6       45.12   13.28     0 -0.0352 -0.0352
\end{verbatim}

Note that without censoring one can get the same results by treating
time as a continuous variable that take value \(\infty\) when the
competing risk is observed:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
df$timeXX <- df$time
df$timeXX[df$event==2] <- max(df$time)+1
e.BT.bis <- BuyseTest(group ~ cont(timeXX), data = df,
				  method.inference = "none", trace = 0)
summary(e.BT.bis, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
      Generalized pairwise comparisons with 1 endpoint

> statistic       : net benefit (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
  timeXX     1e-12   100      41.6       45.12   13.28     0 -0.0352 -0.0352
\end{verbatim}

Expected:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
weight <- (alphaE.X+alphaCR.X)*(alphaE.Y+alphaCR.Y)
exp <- list()
exp$favorable <- 1/weight*(alphaE.X*alphaE.Y*alphaE.X/(alphaE.X+alphaE.Y)+(alphaE.X*alphaCR.Y))
exp$unfavorable <- 1/weight*(alphaE.X*alphaE.Y*alphaE.Y/(alphaE.X+alphaE.Y)+(alphaE.Y*alphaCR.X))
exp$neutral <- alphaCR.X*alphaCR.Y/weight

100*unlist(exp)
\end{lstlisting}

\begin{verbatim}
favorable unfavorable     neutral 
 42.66667    44.00000    13.33333
\end{verbatim}

\item Censoring
\label{sec:orgced04b7}
Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
df$eventC <- df$event
df$eventC[rbinom(n, size = 1, prob = 0.2)==1] <- 0
\end{lstlisting}

BuyseTest (biased):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BTC <- BuyseTest(group ~ tte(time, censoring = eventC), data = df,
				   method.inference = "none", scoring.rule = "Gehan",
				   trace = 0)
summary(e.BTC, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
      Generalized pairwise comparisons with 1 endpoint

> statistic       : net benefit (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs
> uninformative pairs: no contribution at the current endpoint, analyzed at later endpoints (if any)
> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100      31.1       35.15    8.65  25.1 -0.0406 -0.0406
\end{verbatim}

BuyseTest (unbiased):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BTCC <- BuyseTest(group ~ tte(time, censoring = eventC), data = df,
					method.inference = "none", scoring.rule = "Gehan",
					correction.uninf = 2,
					trace = 0)
summary(e.BTCC, percentage = TRUE)
\end{lstlisting}

\begin{verbatim}
      Generalized pairwise comparisons with 1 endpoint

> statistic       : net benefit (delta: endpoint specific, Delta: global) 
> null hypothesis : Delta == 0 
> treatment groups: 1 (control) vs. 2 (treatment) 
> censored pairs  : uninformative pairs
> uninformative pairs: no contribution, their weight is passed to the informative pairs using IPCW
> results
endpoint threshold total favorable unfavorable neutral uninf   delta   Delta
    time     1e-12   100     41.52       46.94   11.54     0 -0.0542 -0.0542
\end{verbatim}

\item Cumulative incidence
\label{sec:org0f869a2}

Settings:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
alphaE <- 2
alphaCR <- 1
n <- 1e3
\end{lstlisting}

Simulate data:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
set.seed(10)
df <- data.frame(time1 = rexp(n, rate = alphaE), time2 = rexp(n, rate = alphaCR), group = "1", event = 1)
df$time <- pmin(df$time1,df$time2)
df$event <- (df$time2<df$time1)+1
\end{lstlisting}

Cumulative incidence (via risk regression):
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.CSC <- CSC(Hist(time, event) ~ 1, data = df)
vec.times <- unique(round(exp(seq(log(min(df$time)),log(max(df$time)),length.out = 12)),2))
e.CSCpred <- predict(e.CSC, newdata = data.frame(X = 1), time = vec.times , cause = 1)
\end{lstlisting}

Expected vs. calculated:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
cbind(time = vec.times,
	  CSC = e.CSCpred$absRisk[1,],
	  manual = alphaE/(alphaE+alphaCR)*(1-exp(-(alphaE+alphaCR)*(vec.times)))
	  )
\end{lstlisting}

\begin{verbatim}
     time    CSC     manual
[1,] 0.00 0.0000 0.00000000
[2,] 0.01 0.0186 0.01970298
[3,] 0.02 0.0377 0.03882364
[4,] 0.05 0.0924 0.09286135
[5,] 0.14 0.2248 0.22863545
[6,] 0.42 0.4690 0.47756398
[7,] 1.24 0.6534 0.65051069
[8,] 3.70 0.6703 0.66665659
\end{verbatim}

Could also be obtained treating the outcome as binary:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
mean((df$time<=1)*(df$event==1))
\end{lstlisting}

\begin{verbatim}
[1] 0.6375
\end{verbatim}



\clearpage
\end{enumerate}

\section{Scoring rules in the survival case}
\label{sec:orgf63586d}
Let's consider the following random variables:  
\begin{itemize}
\item \(X\) the time to the occurrence of the event in the experimental group.
\item \(\Xobs\) the censored event time in the experimental group,
i.e. \(\Xobs = X \wedge C_X\) where \(C_X\) denotes the censoring time in the experimental group.
\item \(\CensT = \Ind[X \leq C_X]\) the event indicator in the experimental group.
\item \(Y\) the time to the occurrence of the event in the control group.
\item \(\Yobs\) the censored event time in the control group,
i.e. \(\Yobs = X \wedge C_Y\) where \(C_Y\) denotes the censoring time in the control group.
\item \(\CensC = \Ind[Y \leq C_Y]\) the event indicator in the control group.
\end{itemize}

We denote by \(\sample_{ij}=\left(\xobs_{i}, \yobs_{j}, \censT_i, \censC_j
\right)\) one realization of the random variables \(\left(\Xobs,
\Yobs, \censT, \censC \right)\). We use the short notation \(x \wedge
y = \min(x,y)\), \(x \vee y = \max(x,y)\), and \(\sample =
\left(\sample_{ij}\right)_{i \in \{1,\ldots,n\},j \in
\{1,\ldots,m\}}\).


\subsection{Partial ordering in presence of right-censored data}
\label{sec:org6dbef8b}
In presence of right censoring we may not be able to compute the
partial ordering between two realizations of an outcome, say \(x_l\)
and \(y_l\). Indeed we only observe a lower bound of the realized
outcomes \(\xobs_l\) and \(\yobs_l\). But we can re-define the partial
ordering as the expected ordering given our knowledge of the
distribution of \(\VX\) and \(\VY\):
\begin{align*}
x_k \succ y_k = \Prob\left[x_k \in [y_k + \tau_k, y_k + h(\tau_k)[ | \sample \right]
\end{align*}
In a similar fashion we can express the neutral score by:
\begin{align*}
\nu_k = \Prob[|x_k - y_k| < \tau_k | \sample] 
\end{align*}
and the weights are defined by:
\begin{align*}
w_k = \left\{ \begin{array}{c} 1\text{, if } k = 1 \\ \prod_{k' \in S_k} \nu_{k'} \text{, otherwise} \end{array} \right.
\end{align*}

\subsection{Gehan scoring rule}
\label{sec:orgc436b52}
TO BE DONE

\subsection{Peron scoring rule when the survival curve is known}
\label{sec:orgc986af3}

We use the following estimator for the probability to be in favor of
the treatment:
\begin{align*}
\hat{\Prob}[\VX \succ \VY + \Vtau] \approx \frac{1}{nm} \sum_{k=1}^K \Prob[w_k=1 |\sample]\Prob[X_k \geq Y_k + \tau_k|\sample]
\end{align*}
This approximation is exact for independent outcomes or when the same
outcome is used at several priorities. In presence of correlated
outcomes we are neglecting a covariance term. To be more precise let
consider the case of two distincts, not independent, outcomes. We make
the following approximation:
\begin{align*}
\Esp\left[ \Prob[w_k=1|\sample]\Prob[X_k \geq Y_k + \tau_k|\sample] \right] \approx \Esp[]
\end{align*}

TO BE CONTINUED

\subsection{Peron scoring rule when the survival curve is estimated}
\label{sec:orga5719e2}

TO BE DONE

\section{Scoring rules in the competing risk case}
\label{sec:org203e292}
TO BE DONE

\clearpage

\section{Corrections for uninformative pairs}
\label{sec:org6505fcb}
\subsection{Inverse probability weighting}
\label{sec:org8bcacc2}

In case of censoring we can use an inverse probability weighting
approach. Let denote \(\delta_{c,X}\) (resp. \(\delta_{c,Y}\)) the
indicator of no censoring relative to \(\tilde{X}\) (resp \(\tilde{Y}\)), \(\tilde{X}_E\) and \(\tilde{Y}_E\) the
censored event time. We can use inverse probability weighting to
compute the net benefit:
\begin{align*}
\Delta^{IPW} &= \frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[\tilde{Y}>\tilde{X}]-\Ind[\tilde{Y}<\tilde{X}])\\
&= \left\{
                \begin{array}{ll}
                  \frac{1}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[Y>X]-\Ind[Y<X])\text{, if no censoring}\\
                  0\text{, if censoring}
                \end{array}
              \right.
\end{align*}

This is equivalent to weight the informative pairs (i.e. favorable,
unfavorable and neutral) by the inverse of the complement of the
probability of being uninformative. This is what is done by the
argument \texttt{correction.tte} of \texttt{BuyseTest}. This works whenever the
censoring mechanism is independent of the event times and we have a
consistent estimate of \(\Prob[\delta_c]\) since:
\begin{align*}
\Esp[\Delta^{IPW}] &= \Esp\left[ \Esp\left[ \frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} (\Ind[\tilde{Y}>\tilde{X}]-\Ind[\tilde{Y}<\tilde{X}]) \Bigg| \tilde{X}, \tilde{Y} \right] \right]\\
&= \Esp\left[\Esp\left[\frac{\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}}}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Bigg| \tilde{X}, \tilde{Y} \right]\right] \Esp\left[\Ind[Y>X]-\Ind[Y<X]\right]\\
&= \frac{\Esp\left[\delta_{c,\tilde{X}}\delta_{c,\tilde{Y}} \right]}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Delta
= \frac{\Esp[\delta_{c,\tilde{X}}]\Esp[\delta_{c,\tilde{Y}}]}{\Prob[\delta_{c,\tilde{X}}]\Prob[\delta_{c,\tilde{Y}}]} \Delta\\
&= \Delta
\end{align*}
where we used the law of total expectation (first line) and the independence between the censoring mecanisms.

\clearpage

\section{Asymptotic distribution}
\label{sec:orge35da5b}

We consider two independent samples \(x_1,x_2,\ldots,x_m\) and
\(y_1,y_2,\ldots,y_n\) where the first one contains iid realisations
of a random variable \(X\) and the second one contains iid
realisations of a second variable \(Y\). To simplify the notation
\(x_1\) represent all the information relative to the first
observation in the treatment group (e.g. the right-censored time to
event and event type indicator). For each realisation we observe \(p\)
endpoints. The estimator of the net benefit can be written as the
difference between two estimators:
\begin{align*}
\hat{\Delta}_\tau = \widehat{\Prob}[X \geq Y+\tau] - \widehat{\Prob}[Y \geq X+\tau]
\end{align*}
These two estimators are symmetric so it is sufficient to study one of
them. Indeed, we will see that each of them can be express as the sum
of iid terms, so they are jointly normally distributed. The iid terms
will enable us to estimate the variance-covariance matrix between the
two estimators and therefore to obtain the variance of the estimator
of the net benefit.

\subsection{Gehan scoring rule}
\label{sec:orga0c1e42}
\subsubsection{\Hajek projection}
\label{sec:orgffda209}

In this section we restrict ourself to the GPC as defined in
\citep{buyse2010generalized}, i.e. we do not consider Peron scoring rule
nor any correction (like inverse probability weighting). We denote by
\(\phi_{k}\) the scoring rule relative to \(\Prob[X \geq Y+\tau]\) for
the endpoint \(k\), e.g. \(\phi_{k}(x_{1},y_{1})=\Ind\left[x_{1k} \geq
y_{1k}+\tau_k\right]\) for a binary endpoint. The scoring rule may
depend of additional arguments, e.g. the threshold \(\tau_k\) but this
will be ignored (since their are known quantities). Finally, we denote
by \(k_{ij}\) the endpoint at which the pair \((i,j)\) is classified
as favorable or unfavorable. If this does not happen then
\(k_{ij}=p\). With this notations, the estimator \(\widehat{\Prob}[X
\geq Y+\tau]\) can be written as a U-statistic:
\begin{align*}
\widehat{\Prob}[X \geq Y+\tau] = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \phi_{k_{ij}}(x_i,y_j)
\end{align*}
This is a two sample U-statistic of order (1,1) with kernel
\(\phi_{k_{ij}}(x_1,y_1)\) (trivially symmetric in \(x\) and \(y\)
separately). From the U-statistic theory (e.g. see appendix
\ref{SM:Ustat}), it follows that \(\widehat{\Prob}[X \geq Y+\tau]\) is
unbiased, normally distributed, and its iid decomposition is the
\Hajek projection:
\begin{align*}
H^{(1)}(\widehat{\Prob}[X \geq Y+\tau]) &= \frac{1}{m} \sum_{i=1}^m \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| x_i] - \Prob[X \geq Y+\tau] \right) \\
& \qquad + \frac{1}{n} \sum_{j=1}^n \left( \Esp[\phi_{k_{ij}}(x_i,y_j) \bigg| y_j] - \Prob[X \geq Y+\tau]\right) \\
&= \sum_{l=1}^{m+n} H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])
\end{align*}
where \(H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])\) are the
individual terms of the iid decomposition. For instance in the binary
case, the term relative to the \(i-th\) observation of the
experimental group is:
\begin{align*}
H^{(1)}_i(\widehat{\Prob}[X \geq Y + \tau]) & = \frac{\Esp[\phi_{k_{ij}}(x_i,y) \bigg| x_i]-\Prob[X \geq Y+\tau]}{m} = \left\{ \begin{array}{cc} 
 \frac{1-p_y-\Prob[X \geq Y+\tau]}{m} \text{ if } x = 1 \\
\frac{-\Prob[X \geq Y+\tau]}{m} \text{ if } x = 0 \\
\end{array} \right. 
\end{align*}
where \(p_y\) is the proportion of 1 in the control group.

\subsubsection{Variance estimator based on the \Hajek projection}
\label{sec:orga5f0fbc}

The \Hajek projection can be used to estimate the variance of \(\widehat{\Prob}[X \geq Y+\tau]\).
Indeed, since:
\begin{align*}
\left(\widehat{\Prob}[X \geq Y+\tau] - \Prob[X \geq Y+\tau] \right) = H^{(1)}(\widehat{\Prob}[X \geq Y + \tau]) + o_p \left( n^{-\half} \right)
\end{align*}
We obtain that asymptotically:
\begin{align*}
\Var\left[\widehat{\Prob}[X \geq Y+\tau]\right] &= \frac{\sigma_{1,0}^2}{n} + \frac{\sigma_{0,1}^2}{m} = \sum_{l=1}^{m+n} \left(H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau]) \right)^2 \\
\text{where } \sigma_{1,0}^2 &= \frac{1}{m}\sum_{i=1}^m \left(\Esp[\phi_{k_{ij}}(x_i,y) \bigg| x_i]-\Prob[X \geq Y+\tau]\right)^2 = m\sum_{i=1}^m \left(H^{(1)}_i(\widehat{\Prob}[X \geq Y + \tau]) \right)^2 \\
\text{and   } \sigma_{0,1}^2 &= \frac{1}{n}\sum_{j=1}^n \left(\Esp[\phi_{k_{ij}}(x_i,y) \bigg| y_j]-\Prob[X \geq Y+\tau] \right)^2 = n\sum_{j=1}^n \left(H^{(1)}_j(\widehat{\Prob}[X \geq Y + \tau]) \right)^2
\end{align*}
Similarly we obtain:
\begin{align*}
\Var\left[\widehat{\Prob}[Y \geq X+\tau]\right] &= \sum_{l=1}^{m+n} \left(H^{(1)}_l(\widehat{\Prob}[Y \geq X + \tau]) \right)^2 \\
\Cov\left[\widehat{\Prob}[X \geq Y+\tau],\widehat{\Prob}[Y \geq X+\tau]\right] &= \sum_{l=1}^{m+n} \left(H^{(1)}_l(\widehat{\Prob}[X \geq Y + \tau])H^{(1)}_l(\widehat{\Prob}[Y \geq X + \tau]) \right) \\
\end{align*}

\subsubsection{Variance estimator based on a second order H-decomposition}
\label{sec:org0c1727d}

An better estimator (i.e. unbiased) of the variance of
\(\widehat{\Prob}[X \geq Y+\tau]\) can be obtained using a second
order H-decomposition. As explained in the appendix \ref{SM:Ustat}, the
formula for the variance becomes:
\begin{align*}
\Var\left[\widehat{\Prob}[X \geq Y+\tau]\right] &= \frac{1}{nm} \left((m-1)\sigma_{1,0}^2 + (n-1)\sigma_{0,1}^2 + \sigma_{1,1}^2\right) \\
\text{where } \sigma_{1,1}^2 &= \Esp[\phi_{k_{ij}}(x_i,y_j)^2] - \Prob[X \geq Y+\tau]^2
\end{align*}
Note that since we consider binary scores,
\(\phi_{k_{ij}}(x_i,y_j)^2=\phi_{k_{ij}}(x_i,y_j)\) so
\(\sigma_{1,1}^2=\Prob[X \geq Y+\tau](1-\Prob[X \geq Y+\tau])\). When
computing the covariance \(\sigma_{1,1}^2 = -\Prob[X \geq
Y+\tau]\Prob[Y \geq X+\tau]\) because \(\Ind[x_i \geq y_j +
\tau]\Ind[y_j \geq x_i + \tau]=0\).

\clearpage

\subsubsection{Example}
\label{sec:org185f30d}

Let's consider a case with 2 observations per group:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d <- data.table(id = 1:4, group = c("C","C","T","T"), toxicity = c(1,0,1,0))
d
\end{lstlisting}

\begin{verbatim}
   id group toxicity
1:  1     C        1
2:  2     C        0
3:  3     T        1
4:  4     T        0
\end{verbatim}

We can form 4 pairs:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
d2 <- data.table(pair = c("3-1","4-1","3-2","4-2"), 
				 type = c("1-1","0-1","1-0","0-0"),
				 favorable = c(0,0,1,0),
				 unfavorable = c(0,1,0,0))
d2
\end{lstlisting}

\begin{verbatim}
   pair type favorable unfavorable
1:  3-1  1-1         0           0
2:  4-1  0-1         0           1
3:  3-2  1-0         1           0
4:  4-2  0-0         0           0
\end{verbatim}

So \(U=\Prob[X>Y]\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
U <- 1/4
\end{lstlisting}

and the iid terms are:
\begin{align*}
H^{(1)}_1(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{n} \left( \Esp\left[\Ind[x>y_1]\big|y_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_2>y_1]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8 \\
H^{(1)}_2(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{n} \left( \Esp\left[\Ind[x>y_2]\big|y_2\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_2]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_3(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_1>y]\big|x_1\right]-U \right)&= \frac{ \frac{\Ind[x_1>y_1]+\Ind[x_1>y_2]}{2}- 1/4}{2} = \frac{1/2-1/4}{2} = 1/8 \\
H^{(1)}_4(\widehat{\Prob}[X \geq Y + \tau]) = \frac{1}{m} \left( \Esp\left[\Ind[x_2>y]\big|x_2\right]-U \right)&= \frac{ \frac{\Ind[x_2>y_1]+\Ind[x_2>y_2]}{2}- 1/4}{2} = \frac{0-1/4}{2} = -1/8
\end{align*}

We can use the method \texttt{iid} to extract the iid decomposition in the
BuyseTest package:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest.options(order.Hprojection = 1)
e.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "u-statistic", trace = 0)
iid(e.BT)
\end{lstlisting}

\begin{verbatim}
     favorable unfavorable
[1,]    -0.125       0.125
[2,]     0.125      -0.125
[3,]     0.125      -0.125
[4,]    -0.125       0.125
\end{verbatim}

This leads to the following estimates for the variance covariance:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
crossprod(iid(e.BT))
\end{lstlisting}

\begin{verbatim}
            favorable unfavorable
favorable      0.0625     -0.0625
unfavorable   -0.0625      0.0625
\end{verbatim}

Which is precisely what is stored in \texttt{e.BT}:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e.BT@covariance
\end{lstlisting}

\begin{verbatim}
             favorable unfavorable covariance netBenefit winRatio
toxicity_0.5    0.0625      0.0625    -0.0625       0.25        4
\end{verbatim}

Note that we could also estimate the variance via the formula given in
\citep{bebu2015large}, e.g.:
\begin{align*}
\sigma^2_{favorable} &= \Prob[X \geq Y_1,X \geq Y_2] - \Prob[X \geq Y]^2 \\
&= 1/8 - 1/16 = 0.0625
\end{align*}
Indeed to compute \(\Prob[X \geq Y_1,X \geq Y_2]\) we distinguish
2*2*2=8 cases (\(X \in \{x_1,x_2\}\), \(Y_1 \in \{y_1,y_2\}\), and
\(Y_2 \in \{y_1,y_2\}\)) and only one satisfyies \(X \geq Y_1,X \geq
Y_2\) (when \(X=x_1\) and \(Y_1=Y_2=y_2\)). This is what is performed when calling:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
e2.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "u-statistic-bebu", trace = 0)
e2.BT@covariance
\end{lstlisting}

\begin{verbatim}
             favorable unfavorable covariance netBenefit winRatio
toxicity_0.5    0.0625      0.0625    -0.0625       0.25        4
\end{verbatim}

\bigskip

Let's now consider the second order decomposition. For the variance of
\(\widehat{\Prob}[Y \geq X + \tau]\):
\begin{itemize}
\item \(\sigma^{2}_{1,1}=1/4(1-1/4)=3/16\)
\item \(\sigma^{2}_{1,0}=\frac{(-1/4)^2+(1/4)^2}{2} = 1/16\)
\item \(\sigma^{2}_{0,1}=\frac{(1/4)^2+(-1/4)^2}{2}  = 1/16\)
\end{itemize}
So \(\sigma^2_{favorable}\) becomes
\(\frac{1}{2*2}\left((2-1)\frac{1}{16}+(2-1)\frac{1}{16}+\frac{3}{16}\right)=\frac{5}{64}\)

\bigskip

For the covariance between \(\widehat{\Prob}[Y \geq X + \tau]\) and
\(\widehat{\Prob}[X \geq Y + \tau]\):
\begin{itemize}
\item \(\sigma^{2}_{1,1}=- (1/4)(1/4)  = -1/16\)
\item \(\sigma^{2}_{1,0}=- 1/16\)
\item \(\sigma^{2}_{0,1}=- 1/16\)
\end{itemize}
So the covariance betwen the estimators equals
\(\frac{1}{2*2}\left((2-1)\frac{-1}{16}+(2-1)\frac{-1}{16}-\frac{1}{16}\right)=\frac{3}{64}\). This
is exactly what BuyseTest outputs:

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
BuyseTest.options(order.Hprojection = 2)
e.BT <- BuyseTest(group ~ bin(toxicity), data = d, 
				  keep.pairScore = TRUE,
				  method.inference = "u-statistic", trace = 0)
e.BT@covariance
\end{lstlisting}

\begin{verbatim}
             favorable unfavorable covariance netBenefit winRatio
toxicity_0.5  0.078125    0.078125  -0.046875       0.25        4
\end{verbatim}



\clearpage

\subsection{Peron scoring rule}
\label{sec:orgb6e9f70}
\subsubsection{Decomposition in iid terms}
\label{sec:org98273ee}

The Peron scoring rule involve survival probabilities that are
estimated via a Kaplan Meier estimator in each group. These estimators
admit the following expansion:
\begin{align*}
\sqrt{m}\left( \SurvThat - \SurvT \right) &= \frac{1}{\sqrt{m}} \sum_{i=1}^m \psi_{\SurvT}(x_i) + o_p(1)  \\
\sqrt{n}\left( \SurvChat - \SurvC \right) &= \frac{1}{\sqrt{n}} \sum_{j=1}^n \psi_{\SurvC}(y_j) + o_p(1)
\end{align*}
Denoting by \(F\) the cumulative distribution function (cdf) relative
to \(X\) and \(G\) the one relative to \(Y\). We will denote by
\(F_n\) and \(G_n\) their empirical counterpart. Then we can re-write
\(\widehat{\Prob}[X \geq Y+\tau]\) as
\(\Delta_{\tau}(F_n,G_n,\SurvChat,\SurvThat)\) and obtain the
following decomposition:
\begin{align*}
&\sqrt{N}\left(\Delta_{\tau}(F_n,G_n,\SurvChat,\SurvThat)-\Delta_{\tau}(F,G,\SurvC,\SurvT) \right) \\
&=  \sqrt{N}\left(\Delta_{\tau}(F_n,G_n,\SurvChat,\SurvThat)-\Delta_{\tau}(F_n,G_n,\SurvC,\SurvT) \right)
+  \sqrt{N}\left(\Delta_{\tau}(F_n,G_n,\SurvC,\SurvT)-\Delta_{\tau}(F,G,\SurvC,\SurvT) \right)
\end{align*}
The second term has been treated in the previous section, so we can focus on the first term. 
\begin{align*}
\sqrt{N}\left(\Delta_{\tau}(F_n,G_n,\SurvChat,\SurvThat)-\Delta_{\tau}(F_n,G_n,\SurvC,\SurvT) \right)
&= \frac{\sqrt{N}}{nm} \sum_{i=1}^m \sum_{j=1}^n \phi(x_i,y_j,\SurvChat,\SurvThat) - \phi(x_i,y_j,\SurvC,\SurvT) \\
\end{align*}
So for uncensored observations this term is 0. For censored
observations, we need to take look term by term.
\begin{itemize}
\item if \(\phi(x_i,y_j,\SurvC,\SurvT) = 1-\frac{\SurvC(\xobs-\tau)}{\SurvC(\yobs)}\) then:
\end{itemize}
\begin{align*}
\phi(x_i,y_j,\SurvChat,\SurvThat) - \phi(x_i,y_j,\SurvC,\SurvT) &= -\left( \frac{\SurvChat(\xobs_i-\tau)}{\SurvChat(\yobs_j)} - \frac{\SurvC(\xobs_i-\tau)}{\SurvC(\yobs_j)} \right) \\
&= -\left( \frac{\SurvChat(\xobs_i-\tau)}{\SurvChat(\yobs_j)} - \frac{\SurvC(\xobs_i-\tau)}{\SurvChat(\yobs_j)} + \frac{\SurvC(\xobs_i-\tau)}{\SurvChat(\yobs_j)} - \frac{\SurvC(\xobs_i-\tau)}{\SurvC(\yobs_j)} \right)  \\
&= -\left( \frac{\SurvChat(\xobs_i-\tau)-\SurvC(\xobs_i-\tau)}{\SurvChat(\yobs_j)} - \frac{\SurvC(\xobs_i-\tau)(\SurvChat(\yobs_j)-\SurvC(\yobs_j))}{\SurvChat(\yobs_j)\SurvC(\yobs_j)} \right)  \\
&= -\left( \frac{1}{\SurvC(\yobs_j)} \frac{1}{n} \sum_{k=1}^n \psi_{\SurvC,k}(\xobs_i-\tau) - \frac{\SurvC(\xobs_i-\tau)}{\SurvC^2(\yobs_j)} \frac{1}{m} \sum_{k=1}^m \psi_{\SurvT,m}(\yobs_j)\right)  \\
\end{align*}




\subsubsection{Example}
\label{sec:org715b87d}

\subsection{Type 1 error in finite sample}
\label{sec:orga01dbf4}

\subsubsection{Binary endpoint}
\label{sec:org3230a9f}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsBin <- system.time(
	eBin.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								 formula = Treatment ~ bin(toxicity),
								 sample.size = c(10,25,50,100,250),                                   
								 method.inference = "u-statistic", trace = 0,
								 transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsBin
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.47    0.14  211.06
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eBin.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
       Simulation study with Generalized pairwise comparison

> statistic   : net benefit
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        0.0023      0.2235     1  0.2116             0.085            0.113
                                      2  0.2116             0.085            0.113
 25  25        -6e-04      0.1482     1  0.1385             0.084            0.089
                                      2  0.1385             0.084            0.089
 50  50       -0.0015      0.1003     1  0.0990             0.059            0.059
                                      2  0.0990             0.059            0.059
100 100       -0.0018      0.0694     1  0.0704             0.044            0.044
                                      2  0.0704             0.044            0.044
250 250       -0.0011      0.0423     1  0.0446             0.045            0.045
                                      2  0.0446             0.045            0.045

> statistic   : win ratio
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        1.6606      2.2207     1  1.6772            0.1301           0.0381
                                      2  1.6540            0.1301           0.0381
 25  25        1.2083      0.8376     1  0.7044            0.1120           0.0620
                                      2  0.7035            0.1120           0.0620
 50  50        1.0795      0.4534     1  0.4366            0.0730           0.0590
                                      2  0.4365            0.0730           0.0590
100 100        1.0327      0.2948     1  0.2937            0.0520           0.0440
                                      2  0.2936            0.0520           0.0440
250 250        1.0099      0.1715     1  0.1810            0.0490           0.0450
                                      2  0.1810            0.0490           0.0450
\end{verbatim}

\clearpage

\subsubsection{Continuous endpoint}
\label{sec:orgb65faa7}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsCont <- system.time(
	eCont.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								  formula = Treatment ~ cont(score), 
								  sample.size = c(10,25,50,100,250), 
								  method.inference = "u-statistic", trace = 0,
								  transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsCont
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.86    0.16  195.00
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eCont.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
       Simulation study with Generalized pairwise comparison

> statistic   : net benefit
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        0.0056      0.2642     1  0.2562             0.076            0.130
                                      2  0.2615             0.073            0.130
 25  25        0.0048      0.1593     1  0.1632             0.061            0.088
                                      2  0.1647             0.054            0.087
 50  50        0.0063      0.1156     1  0.1154             0.064            0.080
                                      2  0.1160             0.060            0.080
100 100        0.0015      0.0825     1  0.0816             0.054            0.062
                                      2  0.0818             0.054            0.062
250 250         6e-04       0.052     1  0.0516             0.050            0.053
                                      2  0.0517             0.050            0.053

> statistic   : win ratio
n.T n.C mean.estimate sd.estimate order mean.se rejection (FALSE) rejection (TRUE)
 10  10        1.1973        0.79     1  0.6710             0.090            0.041
                                      2  0.6856             0.082            0.033
 25  25        1.0652      0.3568     1  0.3574             0.063            0.036
                                      2  0.3607             0.062            0.033
 50  50         1.041      0.2466     1  0.2438             0.053            0.054
                                      2  0.2449             0.052            0.053
100 100        1.0167      0.1676     1  0.1672             0.054            0.049
                                      2  0.1676             0.053            0.048
250 250        1.0067      0.1047     1  0.1042             0.050            0.049
                                      2  0.1044             0.050            0.049
\end{verbatim}

\clearpage

\subsubsection{Time to event endpoint (Gehan method)}
\label{sec:org0fc555e}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsGehan <- system.time(
	eGehan.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								   formula = Treatment ~ tte(eventtime, 
															 censoring = status), 
								   scoring.rule = "Gehan",
								   sample.size = c(10,25,50,100,250), 
								   method.inference = "u-statistic", trace = 0
								   transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsGehan
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.38    0.25  177.58
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(eGehan.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
        Simulation study with Generalized pairwise comparison

 > statistic   : net benefit
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000     -0.003120     0.14812 0.14413          0.087
2:  50  50         1000   1000      0.001308     0.06445 0.06620          0.052
3: 100 100         1000   1000     -0.000690     0.04785 0.04690          0.049
4: 250 250         1000   1000     -0.000647     0.02929 0.02978          0.050

 > statistic   : win ratio
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10          974    974         1.873      3.2268  2.0924        0.04339
2:  50  50         1000   1000         1.092      0.4696  0.4510        0.04500
3: 100 100         1000   1000         1.038      0.3045  0.2983        0.04500
4: 250 250         1000   1000         1.012      0.1818  0.1819        0.04900
\end{verbatim}

\clearpage

\subsubsection{Time to event endpoint (Peron method)}
\label{sec:orgde3aa80}
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsPeron <- system.time(
	ePeron.power <- powerBuyseTest(sim = simBuyseTest, n.rep = 1e3, cpus = 4,
								   formula = Treatment ~ tte(eventtime, 
															 censoring = status), 
								   scoring.rule = "Peron",
								   sample.size = c(10,25,50,100,250), 
								   method.inference = "u-statistic", trace = 0
								   transform = c(TRUE,FALSE), order.Hprojection = 1:2)
)
\end{lstlisting}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
tpsPeron
\end{lstlisting}

\begin{verbatim}
user  system elapsed 
1.16    0.13  198.24
\end{verbatim}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
summary(ePeron.power, statistic = c("netBenefit","winRatio"), 
		legend = FALSE, col.rep = FALSE)
\end{lstlisting}

\begin{verbatim}
        Simulation study with Generalized pairwise comparison

 > statistic   : net benefit
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000     0.0048942     0.22201 0.19280          0.172
2:  50  50         1000   1000     0.0028917     0.11738 0.08918          0.167
3: 100 100         1000   1000     0.0004554     0.10206 0.06325          0.250
4: 250 250         1000   1000     0.0036080     0.08643 0.04016          0.358

 > statistic   : win ratio
   n.T n.C rep.estimate rep.se mean.estimate sd.estimate mean.se rejection.rate
1:  10  10         1000   1000         1.137      0.6089 0.47252          0.086
2:  50  50         1000   1000         1.037      0.2597 0.19203          0.144
3: 100 100         1000   1000         1.023      0.2186 0.13397          0.231
4: 250 250         1000   1000         1.023      0.1838 0.08482          0.354
\end{verbatim}

\clearpage

\section{References}
\label{sec:orgbf35200}
\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{bibliography}

\endgroup

\clearpage

\appendix

\section{Recall on the U-statistic theory}
\label{SM:Ustat}
This recall is based on chapter 1 of \cite{lee1990u}.

\subsection{Motivating example}
\label{sec:org95bfc16}

We will illustrate basic results on U-statistics with the following
motivating question: "what is the asymptotic distribution of the
empirical variance estimator?". For a more concrete example, imagine
that we want to provide an estimate with its 95\% confidence interval
of the variability in cholesterol measurements. We assume that we are
able to collect a sample of \(n\) independent and identically
distributed (iid) realisations \((x_1,\ldots,x_n)\) of the random
variable cholesterol, denoted \(X\). We ignore any measurement error.

\subsection{Estimate, estimator, and functionnal}
\label{sec:orgba588dd}

We can compute an \textbf{estimate} of the variance using the following
\textbf{estimators} \(\hat{\mu}\) and \(\hat{\sigma}^2\):
\begin{align}
\hat{\mu} &= \frac{1}{n} \sum_{i=1}^n x_i \label{eq:m(F)} \\
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 \label{eq:s(F)}
\end{align}
Given a dataset the estimator \(\hat{\sigma}^2\) outputs a
deterministic (i.e. not random) quantity, called the estimate of the
variance. For instance if we observe:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
x <- c(1,3,5,2,1,3)
\end{lstlisting}

then \(s\) equals:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
mu <- mean(x)
sigma2 <- var(x)
sigma2
\end{lstlisting}

\begin{verbatim}
[1] 2.3
\end{verbatim}

In general the value of the estimate depends on the dataset. The
estimator acts like a function \(f_n\) that takes as argument some
data and output a quantity of interest. This is often refer to as a
\textbf{functionnal}, e.g. \(\hat{\sigma}^2=f_n(x_1,\ldots,x_n)\). Here we
use the hat notation to emphasise that \(\hat{\sigma}^2\) is a random
quantity: for each new realisation \((x_1,\ldots,x_n)\) of \(X\)
corresponds a realisation for \(\hat{\sigma}^2\) i.e. a possibly
different value for the variance. If mechanism generating the data has
cumulative distribution function \(F\) then we can also define the
true value as \(\sigma^2=f_{\sigma^2}(F)\) (which is a deterministic
value) where:
\begin{align}
\mu(F) &= f_\mu(F) = \int_{-\infty}^{+\infty} x dF(x) \label{eq:M(F)}\\
\sigma^2(F) &= f_{\sigma^2}(F) = \int_{-\infty}^{+\infty} (x - f_\mu(F))^2 dF(x) \label{eq:S(F)}
\end{align}
This can be understood as the limit \(f(F)=\lim_{n \rightarrow \infty}
f_n(x_1,\ldots,x_n)\). Because \(\sigma^2\) and \(f_{\sigma^2}\) are
very close quantities we will not distinguish them in the notation,
i.e. write \(\sigma^2=\sigma^2(F)\). This corresponds to formula (1)
in \cite{lee1990u}. 

\bigskip

When we observe a sample, we use it to plug-in formula \eqref{eq:M(F)}
and \eqref{eq:S(F)} an approximation \(\hat{F}\) of \(F\). Usually our
best guess for \(F\) is \(\hat{F}(x)= \frac{1}{n}\sum_{i=1}^n
\Ind[x \leq x_i]\) where \(\Ind[.]\) is the indicator function taking value
1 if \(.\) is true and 0 otherwise. One can check that when plug-in
\(\hat{F}\) formula \eqref{eq:M(F)} and \eqref{eq:S(F)} becomes formula
\eqref{eq:m(F)} and \eqref{eq:s(F)}.

\bigskip

To summarize:
\begin{itemize}
\item an estimator is a random variable whose realisation depends on the
data. Its realization is called estimate.
\item an estimate is a deterministic value that we obtain using the
observed data (e.g. observed variability is 2.3)
\item a functionnal (of an estimator) is the rule by which an estimator
transforms the data into an estimate.
\end{itemize}

\subsection{Aim}
\label{sec:orgcf171a9}

Using formula \eqref{eq:m(F)} and \eqref{eq:s(F)} we can easily estimate
the variance based on the observed realisations of \(X\) (i.e. the
data). However how can we get an confidence interval? What we want is
to quantify the incertainty associated with the estimator, i.e. how
the value output by the functionnal is sensitive to a change in the
dataset. To do so, since the estimator \(\hat{\sigma}^2\) is a random variable, we
can try to characterize its distribution. This is in general
difficult. It is much easier to look at the distribution of the
estimator \(\hat{\sigma}^2\) if we would have an infinite sample size. This is what
we will do, and rely on similations to see how things go in finite
sample size. As we will see, the asymptotic distribution of the
variance is a Gaussian distribution with a variance that we can estimate:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
n <- length(x)
k <- mean((x-mu)^4)
var_sigma2 <- (k-sigma2^2)/n
var_sigma2
\end{lstlisting}

\begin{verbatim}
[1] 0.4898611
\end{verbatim}

So we obtain a 95\% confidence intervals for the variance doing:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
c(estimate = sigma2, 
  lower = sigma2 + qnorm(0.025) * sqrt(var_sigma2),
  upper = sigma2 + qnorm(0.975) * sqrt(var_sigma2))
\end{lstlisting}

\begin{verbatim}
 estimate     lower     upper 
2.3000000 0.9282197 3.6717803
\end{verbatim}

We can see that it is not a very good confidence interval since it
symmetric - we know that the variance is positive so it should extend
more on the right side. But this only problematic in small sample
sizes. In large enough sample sizes the confidence interval will be
correct and we focus on this case.

\clearpage

In summary, we would like:
\begin{itemize}
\item to show that our estimator \(\hat{\sigma}^2\) is asymptotically normally distributed.
\item to have a formula for computing the asymptotic variance.
\end{itemize}
To do so we will use results from the theory on U-statistics.

\bigskip

\textsc{Note:} we can already guess that the estimator \(\hat{\sigma}^2\) (as
most estimators) will be asymptotically distributed because it can be
expressed as a average (see formula \eqref{eq:s(F)}). If we would know
the mean of \(X\), then the terms \(x_i-\mu\) are iid so the
asymptotically normality of \(\hat{\sigma}^2\) follows from the
central limit theorem. It does not give us a formula for the
asymptotic variance though. 

\subsection{Definition of a U-statistic and examples}
\label{sec:orge9acc9d}

A U-statistic with kernel \(h\) of order \(k\) is an estimator of the
form:
\begin{align*}
\hat{U} = \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)
\end{align*}
where \(\beta\) is the set of all possible permutations between \(k\)
integers choosen from \(\{1,\ldots,n\}\). We will also assume that the
kernel is symmetric, i.e. the order of the arguments in \(h\) has no
importance. Note that because the observations are iid, \(\hat{U}\) is
an unbiased estimator of \(U\).

\bigskip

\textsc{Example 1}: the simplest example of a U-statistic is the
estimator of mean for which \(k=1\) and \(h\) is the identity
function:
\begin{align*}
\hat{\mu} = \frac{1}{{n \choose 1}} \sum_{(\beta_1) \in \{1,\ldots,n\}} x_{\beta_1} = \frac{1}{n} \sum_{i=1}^{n} x_{i}
\end{align*}

\bigskip

\textsc{Example 2}: our estimator of the variance is also a
U-statistic, but this requires a little bit more work to see that:
\begin{align*}
\hat{\sigma}^2 &= \frac{1}{n-1} \sum_{i=1}^n (x_i-\hat{\mu})^2 = \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \hat{\mu} + \hat{\mu}^2\right) \\
&=  \frac{1}{n-1} \sum_{i=1}^n \left(x_i^2 - 2 x_i \frac{1}{n} \sum_{j=1}^n x_j + \hat{\mu}^2\right) =  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( x_i^2 - 2 x_i  x_j + \hat{\mu}^2 \right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \left( (x_i - x_j)^2 - x_j^2 + \hat{\mu}^2 \right) =  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \frac{1}{n-1} \sum_{j=1}^n \left(x_j^2 - \hat{\mu}^2\right) \\
&=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n (x_i - x_j)^2 - \hat{\sigma}^2 \\
\hat{\sigma}^2 &=  \frac{1}{n(n-1)} \sum_{i=1}^n \sum_{j=1}^n \frac{(x_i - x_j)^2}{2} 
=  \frac{2}{n(n-1)} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2}
=  \frac{1}{{n \choose 2}} \sum_{i=1}^n \sum_{i<j}^n \frac{(x_i - x_j)^2}{2} 
\end{align*}
where we have used that \(\sum_{i=1}^n \left( - 2 x_i \hat{\mu} +
\hat{\mu}^2\right) = \left( - 2 n \hat{\mu}^2 + n \hat{\mu}^2\right) =
\sum_{i=1}^n - \hat{\mu}^2\). So the variance estimator is a
U-statistic of order 2 with kernel \(h(x_1,x_2)=\frac{(x_1 -
x_2)^2}{2}\).

\clearpage

We can verify that numerically:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
M.combn <- combn(length(x),2) ## create all pairs
xi_minus_xj <- apply(M.combn,2, function(iPair){(x[iPair[1]]-x[iPair[2]])})
mean(xi_minus_xj^2/2) - var(x)
\end{lstlisting}

\begin{verbatim}
[1] 2.3
\end{verbatim}


\bigskip

\textsc{Example 3}: another classical example of U-statistic is the
signed rank statistic which enable to test non-parametrically whether
the center of a distribution is 0. This corresponds to:
\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
wilcox.test(x)
\end{lstlisting}

\begin{verbatim}

	Wilcoxon signed rank test with continuity correction

data:  x
V = 21, p-value = 0.03501
alternative hypothesis: true location is not equal to 0

Warning message:
In wilcox.test.default(x) : cannot compute exact p-value with ties
\end{verbatim}

Let's take two random realisation of \(X\) and denote thoses \(X_1\)
and \(X_2\) (they are random variables). The parameter of interest (or
true value) is \(U = \Prob[X_1+X_2>0]\) and the corresponding
estimator is:
\begin{align*}
\hat{U} = \frac{1}{{n \choose 2}} \sum_{i=1}^{n} \sum_{i<j} \Ind[x_i+x_j>0]
\end{align*}

\subsection{A major result from the U-statistic theory}
\label{sec:orgd19dfd3}

So far we have seen that our estimator for the variance was a
U-statistic. We will now use the U-statistic theory to obtain its
asymptotic distribution.

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 76) \\
 Let \(\hat{U}\) be a U-statistic of order \(k\) with non-zero first
 component in its H-decomposition. Then \(n^{\frac{1}{2}}
 (\hat{U}-U)\) is asymptotically normal with mean zero and asymptotic
 variance \(\sigma^2_1\) where \(\sigma^2_1\) is the variance of the
 first component in the H-decomposition of \(\hat{U}\).

\bigskip

So under the assumption that the first term of the H-decomposition of
the variance is non 0 then we know that the asymptotic distribution of
our variance estimator is normal and if we are able to compute the
variance of the first term of the H-decomposition then we would also
know the variance parameter of the asymptotic distribution. So it
remains to see what is this H-decomposition and how can we
characterize it.

\subsection{The first term of the H-decomposition}
\label{sec:org9676ff3}

The H-decomposition (short for Hoeffling decomposition) enables us to
decompose the estimator of a U-statistic of rank \(k\) into a sum of
\(k\) uncorrelated U-statistics of increasing order (from \(1\) to
\(k\)) with variances of decreasing order in \(n\). As a consequence
the variance of the U-statistic will be asymptotically equal to the
variance of the first non-0 term in the decomposition.

\bigskip

Before going further we introduce:
\begin{itemize}
\item \(X_1\), \ldots, \(X_n\) the random variables associated with each
sample.
\item \(\mathcal{L}_2\) the space of all random variables with zero mean
and finite variance. \\ It is equiped with the inner
product \(\Cov[X,Y]\).
\item the subspaces \(\left(\mathcal{L}_2^{(j)}\right)_{j \in
  \{1,\ldots,k\}}\) where for a given \(j\in \{1,\ldots,k\}\),
\(\mathcal{L}_2^{(j)}\) is the subspace of \(\mathcal{L}_2\)
containing all random variables of the form
\(\sum_{(\beta_1,\ldots,\beta_j) \in \beta}
  \psi(X_{\beta_1},\ldots,X_{\beta_j})\) where \(\beta\) is the set of
all possible permutations between \(j\) integers choosen from
\(\{1,\ldots,n\}\). For instance \(\mathcal{L}_2^{(1)}\) contains
the mean, \(\mathcal{L}_2^{(2)}\) contains the variance, and
\(\mathcal{L}_2^{(j)}\) contains all U-statistics of order \(j\)
with square integrable kernels.
\end{itemize}

We can now define the H-decomposition as the projection of
\(\hat{U}-U\) on the subspaces \(\mathcal{L}_2^{(1)}\),
\(\mathcal{L}_2^{(2)} \cap \left( \mathcal{L}_2^{(1)} \right)^{\perp}\), \ldots, \(\mathcal{L}_2^{(k)} \cap \left( \mathcal{L}_2^{(k-1)}
\right)^{\perp}\). Here \(A^{\perp}\) indicates the space orthogonal
to \(A\). So the first term of the H-decomposition, denoted
\(H^{(1)}\), is the projection of \(\hat{U}-U\) on
\(\mathcal{L}_2^{(1)}\); this is also called the \Hajek
projection. Clearly all terms of the projection are mutually
orthogonal (or uncorrelated), they are unique (it is a projection) and
they correspond to U-statistics of increasing degree (from \(1\) to
\(k\)). It remains to get a more explicit expression for these term
and show that their variance are of decreasing order in \(n\).

\bigskip

We now focus on the first term and show that \(H^{(1)} = \sum_{i=1}^n
\Esp[\hat{U}-U|X_i]\). Clearly this term belongs to
\(\mathcal{L}_2^{(1)}\). It remains to show that \(\hat{U}-U -
H^{(1)}\) is orthogonal to \(\mathcal{L}_2^{(1)}\). Let consider an element \(V \in \mathcal{L}_2^{(1)}\):
\begin{align*}
\Cov[\hat{U}-U - H^{(1)}, V ] &= \Esp[(\hat{U}-U - H^{(1)} ) V ] \\
&= \sum_{i'=1}^{n} \Esp[(\hat{U}-U - H^{(1)}) \psi(X_{i'}) ] \\ 
&= \sum_{i'=1}^{n} \Esp[\Esp[\hat{U}-U - H^{(1)} \big| X_{i'}] \psi(X_{i'}) ]
\end{align*}
So it remains to show that \(\Esp[\hat{U}-U \big| X_{i'}] = \Esp[H^{(1)}
\big| X_{i'}]\). This follows from:
\begin{align*}
\Esp[H^{(1)} \big| X_{i'}] &= \Esp[\sum_{i=1}^n \Esp[\hat{U}-U|X_i] \big| X_{i'}] = \sum_{i=1}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Esp[\Esp[\hat{U}-U|X_i] \big| X_{i'}] \\
&= \Esp[\hat{U}-U|X_i] + \sum_{i\neq i'}^n \Ccancelto[red]{0}{\Esp[\Esp[\hat{U}-U|X_i]]}
\end{align*}
where we have used that \(X_i\) and \(X_{i'}\) are independent and \(\Esp[\Esp[\hat{U}-U|X_i]]=\Esp[\hat{U}-U]=0\).

\bigskip

We can now re-express the first term of the H-decomposition more
explicitely:
\begin{align*}
H^{(1)} &= \sum_{i=1}^n \Esp[\hat{U}-U \big| X_i]  \\
&=  \sum_{i=1}^n \Esp[ \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) - U \big| X_i ] \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Esp[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right) \big| X_i ] - U \\
&=  \frac{1}{{n \choose k}} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{i=1}^n \Ind[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] + \Ind[i \notin \beta] * 0 - U \\
&=  \frac{1}{{n \choose k}} \sum_{i=1}^n \Prob[i \in \beta] \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
&=  \frac{{n - 1 \choose k - 1}}{{n \choose k}} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U  \\
H^{(1)} &=  \frac{k}{n} \sum_{i=1}^n \Esp[ h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] - U 
\end{align*}
Let's now compute the variance of \(\hat{U}\):
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \Var[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right)] \\
&= {n \choose k}^{-2} \Cov[\sum_{(\beta_1,\ldots,\beta_k) \in \beta} h \left(x_{\beta_1},\ldots,x_{\beta_k} \right),\sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
&= {n \choose k}^{-2} \sum_{(\beta_1,\ldots,\beta_k) \in \beta} \sum_{(\beta'_1,\ldots,\beta'_k) \in \beta'} \Cov[ h \left(x_{\beta_1},\ldots,x_{\beta_k} \right), h \left(x_{\beta'_1},\ldots,x_{\beta'_k} \right)] \\
 \end{align*}
Using the symmetry of the kernel we see that the terms in the double
sum only depends on the number of common observations. To determine a
term with \(j\) common observations, a choose:
\begin{itemize}
\item \(k\) observations among the \(n\) for the first kernel: \({n \choose k}\) possibilities
\item \(c\) common index for the two kernels among the \(k\): \({k \choose c}\) possibilities
\item \(k-c\) observations among the remaining \(n-k\) observations for
the second kernel: \({n - k \choose k - c}\) possibilities
\end{itemize}
So denoting \(\sigma^2_c=\Cov[ h \left(x_{1},\ldots,x_{k} \right), h \left(x_{1},\ldots,x_{c},x'_{c+1},\ldots,x'_{k} \right)]\) this gives:
\begin{align*}
 \Var[\hat{U}] &= {n \choose k}^{-2} \sum_{c=0}^{n} {n \choose k} {k \choose c} {n - k \choose k - c} \sigma^2_c \\
&=  \sum_{c=0}^{k} \frac{k!(n-k)!}{n!}  \frac{k!}{c!(k-c)!} \frac{(n-k)!}{(k-2k+c)!(n-c)!} \sigma^2_c \\
&=  \sum_{c=0}^{k}  \frac{k!^2}{c!(k-c)!^2}  \frac{(n-k)!^2}{(n-2k+c)!n!} \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k)!^2}{(n-2k+c)!n!}\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{(n-k) \ldots (n-2k+c+1)}{n \ldots (n-k+1) }\right) \sigma^2_c \\
&= \sum_{c=0}^{k} \mathcal{O}\left(\frac{n^{- k + 2k - c}}{n^{k}}\right) = \sum_{c=0}^{k} \mathcal{O}\left(n^{-c}\right) \sigma^2_c \\
\end{align*}
So if \(\sigma^2_1 \neq 0\) then the asymptotic variance only depends on the variance of the first term, i.e.:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}] = \frac{k^2}{n^2}  \Var[ \sum_{i=1}^n \Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} \sum_{i=1}^n \Var[\Esp[h \left(x_1,\ldots,x_{i-1},x_i,x_{i+1},\ldots,x_{k} \right) \big| x_i] ] \\
&= \frac{k^2}{n^2} n \Var[\Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ] \\
\Var[\hat{U}] &= \frac{k^2}{n}  \Var[ \Esp[h \left(x,x_2,\ldots,x_{k} \right) \big| x] ]
\end{align*}

\bigskip

In summary we have obtained a formula for the asymptotic variance of
the U-statistic.

\bigskip

\textsc{Example 1}: Sample mean \\
We first compute the \Hajek projection of the mean:
\begin{align*}
H^{(1)}_{\hat{\mu}} = \frac{1}{n} \sum_{i=1}^n \Esp[x_i|x_i]-\mu = \frac{1}{n}  \sum_{i=1}^n x_i-\mu
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\mu}] =  \Var[H^{(1)}_{\hat{\mu}}] = \frac{1}{n^2}  \sum_{i=1}^n \Var[x_i-\mu] = \frac{1}{n^2}  \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n}
\end{align*}

\clearpage

\textsc{Example 2}: Sample variance \\
We first compute the \Hajek projection of the variance:
\begin{align*}
H^{(1)}_{\hat{\sigma}^2} &= \frac{2}{n} \sum_{i=1}^n  \Esp[\frac{(x_i-X_2)^2}{2} \bigg|x_i]  - \sigma^2 = \frac{1}{n} \sum_{i=1}^n \Esp[x_i^2 - 2 x_i X_2 + X_2^2 \big|x_i]  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( x_i^2 - 2 x_i \mu + \sigma^2 + \mu^2 \right)  - \sigma^2 \\
&= \frac{1}{n} \sum_{i=1}^n \left( (x_i - \mu)^2 - \sigma^2 \right) 
\end{align*}
And then compute the asymptotic variance as:
\begin{align*}
\Var[\hat{\sigma}^2] &=   \Var[H^{(1)}_{\hat{\sigma}^2}] = \frac{1}{n^2} \sum_{i=1}^n  \Var[ (x_i - \mu)^2 - \sigma^2]\\
&= \frac{1}{n^2} \sum_{i=1}^n \Esp[(x - \mu)^4]-\Esp[(x - \mu)^2]^2 \\
&=\frac{\mu_4-\left(\sigma^2\right)^2}{n}  
\end{align*}
where \(\mu_4=\Esp[(x - \mu)^4]\) is the fourth moment of the
distribution. For a better approximation in small sample size we could
account for the variance of the second term of the H-decomposition. We
would obtain (\cite{lee1990u}, page 13):
\begin{align*}
\Var[\hat{\sigma}^2] = \frac{\mu_4}{n}-\frac{(n-3)\left(\sigma^2\right)^2}{n(n-1)}  
\end{align*}
When \(\frac{n-3}{n-1}\) is close to 1 then the first order
approximation is sufficient.

\bigskip

\textsc{Example 3}: Signed rank statistic \\
We first compute the \Hajek projection of the signed rank statistic:
\begin{align*}
 H^{(1)}_{\hat{U}} &=   \frac{2}{n} \sum_{i=1}^n \Esp\left[ \Ind[x_i+X_2>0] \big|x_i \right] - U = \frac{2}{n} \sum_{i=1}^n \Prob[ X_{2} > -x_i \big|x_i] - \Prob[X_{2}> - X_{1}] \\
 &= \frac{2}{n} \sum_{i=1}^n (1 - F(-x_i)) - \Esp[x][(1 - F(-x))] \\
\end{align*}
Since under the null, the distribution is symmetric \(F(-x)=1-F(x)\):
\begin{align*}
 H^{(1)}_{\hat{U}} &= \frac{2}{n} \sum_{i=1}^n F(x_i) - \Esp[x][F(x)]
\end{align*}
We will use that for continuous distribution \(F(x)\) is uniformly
distribution and therefore has variance \(\frac{1}{12}\). So we can
compute the asymptotic variance as:
\begin{align*}
\Var[\hat{U}] &= \Var[H^{(1)}_{U}] = \frac{4}{n^2} \sum_{i=1}^n \Var\left[ F(x_i) - \Esp[x][F(x)] \right] = \frac{4}{n^2} n \frac{1}{12} = \frac{1}{3}
\end{align*}

\subsection{Two sample U-statistics}
\label{sec:org2f08c9f}

So far we have assumed that all our observations were iid. But in the
case of GPC, we study two populations (experimental arm and control
arm) so we can only assume to have two independent samples
\(x_1,x_2,\ldots,x_m\) and \(y_1,y_2,\ldots,y_n\) where the first one
contains iid realisations of a random variable \(X\) and the second
one contains iid realisations of a second variable \(Y\). We can now
define a two-sample U-statistic of order \(k_x\) and \(k_y\) as:
\begin{align*}
\hat{U} = \frac{1}{{m \choose k_x}{n \choose k_y}} \sum_{(\alpha_1,\ldots,\alpha_{k_x})\in \alpha} \sum_{(\beta_1,\ldots,\beta_{k_y})\in \beta} h(x_{\alpha_{1}},\ldots,x_{\alpha_{k_x}},y_{\beta_1},\ldots,y_{\beta_{k_y}})
\end{align*}
where \(\alpha\) (resp. \(\beta\)) is the set of all possible
 permutations between \(k_x\) (resp. \(k_y\)) intergers chosen from
 \(\{1,\ldots,m\}\) (resp.  \(\{1,\ldots,n\}\)) and the kernel
 \(h=h(x_1,\ldots,x_{k_x},y_1,\ldots,y_{k_y})\) is permutation symmetric in
 its first \(k_x\) arguments and its last \(k_y\) arguments
 separately. Once more it follows from the independence and iid
 assumptions that \(\hat{U}\) is an unbiased estimator of \(U =
 \Esp[h(X_1,\ldots,X_{k_x},Y_1,\ldots,Y_{k_y})]\) where \(X_1,\ldots,X_{k_x}\)
 (resp. \(Y_1,\ldots,Y_{k_y}\)) are the random variables associated to
 distinct random samples from \(X\) (resp. \(Y\)). The two-sample case
 is a specific case of the Generalized U-statistics introduced in
 section 2.2 in \cite{lee1990u}.

\bigskip

Many results for U-statistics extends to two sample U-statistics. For
instance the \Hajek projection of \(\hat{U}-U\) becomes:
\begin{align*}
H^{(1)} = \frac{k_x}{m} \sum_{i=1}^{m} \left( \Esp[h(x_1,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x_i] - U \right) + \frac{k_y}{n} \sum_{j=1}^{n} \left( \Esp[h(x_1,\ldots,x_{k_x},y_1,y_2,\ldots,y_{k_y}) \big| y_j] - U \right)
\end{align*}
Before stating any asymptotic results, we need to define what we now
mean by asymptotic (since we have two sample sizes \(m\) and
\(n\)). We now mean by asymptotic that we create an increasing
sequence of \(m\) and \(n\) indexed by \(v\) such that:
\begin{itemize}
\item \(m_v \cvD[][v \rightarrow \infty] \infty\)
\item \(n_v \cvD[][v \rightarrow \infty] \infty\)
\item there exist a \(p \in ]0;1[\) satisfying \(\frac{m}{n+m} \cvD[][v
  \rightarrow \infty] p\) and \(\frac{n}{n+m} \cvD[][v \rightarrow
  \infty] 1-p\).
\end{itemize}

Informally speaking, this means that \(m\) and \(n\) goes to infinity
  at the same speed. Let's denotes:
\begin{align*}
\Var[\Esp[h(x,x_2,\ldots,x_{k_x},y_1,\ldots,y_{k_y}) \big| x]] &= \sigma^2_{1,0} \\
\Var[\Esp[h(x_1,\ldots,x_{k_x},y,y_2,\ldots,y_{k_y}) \big| y]] &= \sigma^2_{0,1} 
\end{align*}
We then have the following result:

\bigskip

\textbf{Theorem} (adapted from \cite{lee1990u}, theorem 1 page 141)
 \\ Let \(\hat{U}\) be a U-statistic of order \(k_x\) and
 \(k_y\) with non-zero first component (i.e. \(\sigma^2_{1,0}>0\) and
 \(\sigma^2_{0,1}>0\)) in its H-decomposition. Then
 \((m+n)^{\frac{1}{2}} (\hat{U}-U)\) is asymptotically normal with
 mean zero and asymptotic variance \(p^{-1} k_x^2
 \sigma^2_{1,0}+(1-p)^{-1} k_y^2 \sigma^2_{0,1}\) which is the
 variance of the first component in the H-decomposition of
 \(\hat{U}\).

\bigskip

\textsc{Example 4}: Mann-Whitney statistic \\
If our parameter of interest is \(\Prob[X \leq Y]\) then the estimator:
\begin{align*}
\hat{U} = \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j]
\end{align*}
is a U-statistic of order \(k_x=1\) and \(k_y=1\) with kernel \(h(x,y)=\Ind[x \leq y]\)
We first compute the \Hajek projection of the signed rank statistic:
\begin{align*}
H_{\hat{U}}^{(1)} &= \frac{1}{m} \sum_{i=1}^m \left( \Esp\left[\Ind[x_i \leq y] \big| x_i\right] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Esp\left[\Ind[x \leq y_j] \big| y_j\right] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( \Prob[Y \geq x_i] - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( \Prob[X \leq y_j] - U \right) \\
&= \frac{1}{m} \sum_{i=1}^m \left( 1 - F_{-,y}(x_i) - U \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - U \right) \\
&= - \frac{1}{m} \sum_{i=1}^m \left( F_{-,y}(x_i) - \Esp_x[ F_{-,x}(x) ] \right)
+ \frac{1}{n} \sum_{j=1}^n \left( F_x(y_j) - \Esp_y[ F_y(y) ] \right) 
\end{align*}
where \(F_{-}\) is the left limit of \(F\), \(F_x\)(resp. \(F_y\))
denoting the cumulative distribution function of \(X\)
(resp. \(Y\)). For continuous distributions \(F_{-}=F\) and under the
null hypothesis that \(F_x=F_y\), we get that:
\begin{align*}
\Var[\hat{U}] = \Var[H_{\hat{U}}^{(1)}] = \frac{1}{m} \frac{1}{12} + \frac{1}{n} \frac{1}{12} = \frac{nm}{12(m+n)}
\end{align*}
If we are not under the null we end up with the formula:
\begin{align*}
\Var[\hat{U}] = \frac{1}{m^2} \sum_{i=1}^m \Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] + \frac{1}{n^2} \sum_{j=1}^n \Var\left[ \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right]
\end{align*}
Noticing that:
\begin{align*}
\Esp\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] = \Esp\left[ \Ind[x_i \leq y]\right] - U = 0 
\end{align*}
We can compute the variance as:
\begin{align*}
\Var\left[ \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right] &= \Esp\left[ \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right)^2\right]  \\
&= \int_{x} \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right)  \left(\int_y \left(\Ind[x \leq y] - U \right) dF_Y(y)\right) dF_{X}(x)\\
&= \int_{x} \left(\int_{y_1} \left(\Ind[x \leq y_1] - U \right) dF_Y(y_1)\right)  \left(\int_{y_2} \left(\Ind[x \leq y_2] - U \right) dF_Y(y_2)\right) dF_{X}(x)\\
&= \int_{x} \int_{y_1} \int_{y_2} \left(\Ind[x \leq y_1] - U \right) \left(\Ind[x \leq y_2] - U \right) dF_Y(y_1) dF_Y(y_2) dF_{X}(x)\\
&= \Esp\left[\left( \Ind[x \leq y_1] - U\right)\left(\Ind[x \leq y_2] - U\right)\right] \\
&= \Esp\left[\Ind[x \leq x_1]  \Ind[x \leq y_2] \right] - \Esp\left[\Ind[x \leq y_1]\right] U  - \Esp\left[\Ind[x \leq y_2]\right] U + U^2 \\
&= \Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2
\end{align*}

So the variance is:
\begin{align*}
\Var[\hat{U}] &= \frac{1}{m} \left(\Prob[x \leq y_1, x \leq y_2] - \Prob[x \leq y]^2 \right) + \frac{1}{n} \left(\Prob[x_1 \leq y, x_2 \leq y] - \Prob[x \leq y]^2 \right) \\
&= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n}
\end{align*}
In fact we could have a more precise formula by accounting for the
second term in the H-decomposition. \cite{lee1990u} (Theorem 2 page 38, formula 2)
give the general formal for the variance that becomes in the case of a two sample U statistic of degree 1:
\begin{align*}
\Var[\hat{U}] &= \frac{\sigma^2_{1,0}}{m} + \frac{\sigma^2_{0,1}}{n} + \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm} \\
&= \frac{1}{nm} \left((n-1)\sigma^2_{1,0} + (m-1)\sigma^2_{0,1} + \sigma^2_{1,1} \right) 
\end{align*}
where \(\sigma^2_{1,1} = \Prob[x \leq y](1-\Prob[x \leq y])\). Indeed the second
term of the H-decomposition would be the projection of \(\Ind[X \leq
Y]\) on \(X,Y\) where we substract components of the \Hajek
projection to get the orthogonality between \(H_{\hat{U}}^{(1)}\) and
\(H_{\hat{U}}^{(2)}\) (see formula 11 page 33 of \cite{lee1990u} for a
generic formula):
\begin{align*}
H_{\hat{U}}^{(2)} &= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \left(\Esp\left[ \Ind[x_i \leq y_j] \big| x_i,y_j\right] - U\right) - \left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - U\right) - \left(\Esp\left[ \Ind[x \leq y_j] \big| y_j\right] - U\right) \\
&= \frac{1}{mn} \sum_{i=1}^m \sum_{j=1}^n \Ind[x_i \leq y_j] - \Esp\left[ \Ind[x_i \leq y] \big| x_i\right] - \Esp\left[ \Ind[x \leq y_j] \big| y_j\right] + U
\end{align*}
and we retrieve the formula given page 39 of \cite{lee1990u} for the variance of the Mann-Whitney U-statistic:
\begin{align*}
\Var[H_{\hat{U}}^{(2)}] 
&= \frac{1}{(mn)^2} \sum_{i=1}^m \sum_{j=1}^n \Var\left[\Ind[x_i \leq y_j]\right] + \Var\left[\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] + \Var\left[\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
& \qquad \qquad \qquad -2 \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] -2 \Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
&= \frac{1}{(mn)^2} \sum_{i=1}^m \sum_{j=1}^n \Var\left[\Ind[x_i \leq y_j]\right] - \Var\left[\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] - \Var\left[\Esp\left[ \Ind[x \leq y_j] \big| y_j\right]\right] \\
&= \frac{\sigma^2_{1,1}-\sigma^2_{0,1}-\sigma^2_{1,0}}{nm}
\end{align*}
where the second line follows from:
\begin{align*}
\Cov\left[\Ind[x_i \leq y_j],\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]\right] &= \Esp\left[\left(\Ind[x_i \leq y_j] - U\right)\left(\Esp\left[ \Ind[x_i \leq y] \big| x_i\right]-U\right)\right] \\
&= \Esp\left[\Esp\left[\left(\Ind[x_i \leq y_j] - U\right)\Esp\left[ \Ind[x_i \leq y]-U \big| x_i\right]\Big| x_i \right] \right] 
&= \Esp\left[\Esp\left[ \Ind[x_i \leq y]-U \big| x_i\right]^2 \right] \\
&= \Var\left[\Esp\left[\Ind[x_i \leq y] \big| x_i\right] \right] 
\end{align*}
and the last line follows from:
\begin{align*}
\Var\left[\Ind[x_i \leq y_j]\right] &= \Esp\left[\Ind[x_i \leq y_j]^2\right] - \Esp\left[\Ind[x_i \leq y_j]\right]^2 
= \Esp\left[\Ind[x_i \leq y_j]\right] - \Esp\left[\Ind[x_i \leq y_j]\right]^2 \\
&= \Esp\left[\Ind[x_i \leq y_j]\right]\left(1 - \Esp\left[\Ind[x_i \leq y_j]\right]\right)
= \Prob[x \leq y]\left(1 - \Prob[x \leq y]\right)
\end{align*}

A final useful result is about the covariance between two 2-samples
U-statistics.
\begin{align*}
\Cov[H_{\hat{U_1}}^{(2)},H_{\hat{U_2}}^{(2)}] 
&= \frac{1}{(mn)^2} \sum_{i=1}^m \sum_{j=1}^n \Cov\left[U_{1,ij} - \Esp\left[ U_{1,ij} \big| i\right] - \Esp\left[ U_{1,ij} \big| j\right],
U_{2,ij} - \Esp\left[ U_{2,ij} \big| i\right] - \Esp\left[ U_{2,ij} \big| j\right] \right] \\
&= \frac{1}{(mn)^2} \sum_{i=1}^m \sum_{j=1}^n \Cov\left[U_{1,ij},U_{2,ij}\right] 
+ \Cov\left[\Esp\left[ U_{1,ij} \big| i\right],\Esp\left[ U_{2,ij} \big| i\right]\right]
+ \Cov\left[\Esp\left[ U_{1,ij} \big| j\right],\Esp\left[ U_{2,ij} \big| j\right]\right] \\
&\qquad \qquad-\Cov\left[U_{1,ij},\Esp\left[ U_{1,ij} \big| i\right]\right]-\Cov\left[U_{2,ij},\Esp\left[ U_{1,ij} \big| i\right]\right] \\
&\qquad \qquad-\Cov\left[U_{1,ij},\Esp\left[ U_{1,ij} \big| j\right]\right]-\Cov\left[U_{2,ij},\Esp\left[ U_{1,ij} \big| j\right]\right] \\
&= \frac{1}{(mn)^2} \sum_{i=1}^m \sum_{j=1}^n \Cov\left[U_{1,ij},U_{2,ij}\right] 
- \Cov\left[\Esp\left[ U_{1,ij} \big| i\right],\Esp\left[ U_{2,ij} \big| i\right]\right]
- \Cov\left[\Esp\left[ U_{1,ij} \big| j\right],\Esp\left[ U_{2,ij} \big| j\right]\right] \\
\end{align*}
So:
\begin{align*}
&\Cov[U_1,U_2] \\
&= \Cov[H_{\hat{U_1}}^{(1)},H_{\hat{U_2}}^{(1)}] + \Cov[H_{\hat{U_1}}^{(2)},H_{\hat{U_2}}^{(2)}]  \\
&= \frac{1}{(mn)^2} \sum_{i=1}^n \sum_{j=1}^m
  (m-1) \Cov\left[\Esp\left[ U_{1,ij} \big| i\right],\Esp\left[ U_{2,ij} \big| i\right]\right]
+ (n-1) \Cov\left[\Esp\left[ U_{1,ij} \big| j\right],\Esp\left[ U_{2,ij} \big| j\right]\right] 
+ \Cov\left[U_{1,ij},U_{2,ij}\right]
\end{align*}
where the last term can be computed using that:
\begin{align*}
\Cov\left[U_{1,ij},U_{2,ij}\right] 
&= \Esp[\left(U_{1,ij} U_{2,ij}\right)^2] - \Esp[U_{1,ij} U_{2,ij}]^2
\end{align*}

\clearpage 

\section{Information about the R session used for this document}
\label{sec:org4ad64b0}

\lstset{language=r,label= ,caption= ,captionpos=b,numbers=none}
\begin{lstlisting}
sessionInfo()
\end{lstlisting}

\begin{verbatim}
R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=Danish_Denmark.1252  LC_CTYPE=Danish_Denmark.1252    LC_MONETARY=Danish_Denmark.1252 LC_NUMERIC=C                   
[5] LC_TIME=Danish_Denmark.1252    

attached base packages:
 [1] stats4    parallel  tools     stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] lava_1.6.4          doParallel_1.0.14   iterators_1.0.10    foreach_1.4.4       BuyseTest_1.7       testthat_2.0.0      prodlim_2018.04.18 
 [8] spelling_1.2        roxygen2_6.1.0.9000 butils.base_1.2     Rcpp_1.0.0          data.table_1.11.8   usethis_1.4.0       devtools_2.0.1     

loaded via a namespace (and not attached):
 [1] compiler_3.5.1            prettyunits_1.0.2         base64enc_0.1-3           remotes_2.0.2             digest_0.6.17             pkgbuild_1.0.2           
 [7] pkgload_1.0.2             lattice_0.20-35           memoise_1.1.0             rlang_0.3.0.1             Matrix_1.2-14             cli_1.0.1                
[13] commonmark_1.6            RcppArmadillo_0.9.200.4.0 withr_2.1.2               stringr_1.3.1             xml2_1.2.0                desc_1.2.0               
[19] fs_1.2.6                  grid_3.5.1                rprojroot_1.3-2           glue_1.3.0                R6_2.3.0                  processx_3.2.0           
[25] survival_2.42-6           sessioninfo_1.1.1         callr_3.0.0               purrr_0.2.5               magrittr_1.5              codetools_0.2-15         
[31] backports_1.1.2           ps_1.1.0                  splines_3.5.1             assertthat_0.2.0          stringi_1.2.4             crayon_1.3.4
\end{verbatim}
\end{document}